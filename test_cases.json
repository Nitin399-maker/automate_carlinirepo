{
  "aws_ipv6.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can identify the error in an AWS Lambda code for authorizing a new network. This type of error is generally difficult to find via search.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"This a code for aws lambda to authorize a new network, what is wrong with this :\nsecurity_group.authorize_egress(\n            IpPermissions=[\n                {'IpProtocol': '-1',\n                 'IpRanges': [{'CidrIpv6': '::/0'}]}\n            ]\n        )\n\"\"\"\n\nevaluation = \"\"\"\nBelow is a software engineer's answer to a request to find an issue with the following code: <A>\n\nDoes the engineer mention overly permissive IPv6 ranges? Think out loud, and then end your answer with either the string \"The issue is found\" or \"The issue is not found\".\n\"\"\"\n\nTestAWSV6 = question >> LLMRun() >> LLMRun(evaluation, llm=EVAL_LLM) >> SubstringEvaluator(\"The issue is found\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestAWSV6))\n\n\n",
  "baking_help.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can identify a missing incredient in a recipe. Identifying incorrect steps is much harder than missing steps.\"\n\nTAGS = ['explain', 'fun']\n\nquestion = \"\"\"\nI'm making pancakes for breakfast. I added a cup of flour, a teaspoon of salt, and a few tablespoons of sugar to a bowl. I stirred it together, then added a cup of milk, a beaten egg, and a few tablespoons of oil, and stirred until just mixed. Then I put 1/4 a cup on a hot frying pan, and flipped it when brown. But they're terrible! Why? List the main reason.\n\"\"\"\n\nevaluation = \"\"\"\nBelow is a student's answer to a question on what is missing from some pancake recipe: <A>\n\nDoes the student mention that baking powder or baking soda is missing? Think out loud, and then end your answer with either the string \"The student passes\" or \"The student fails\".\n\"\"\"\n\nTestMissingStep = question >> LLMRun() >> LLMRun(evaluation, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestMissingStep))\n",
  "base64_qanda.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model will follow instructions to the letter without lots of cajoling. Thinking in base64 is also interesting.\"\n\nTAGS = ['explain', 'fun']\nimport base64 \n\nquestion = \"\"\"\nRead this base64 string, think about the answer, and type just the answer in base64. You're entire answer must be base64.\n\nV2hhdCBpcyBvbmUgeWVhciBhZnRlciB3aGVuIFByZXNpZGVudCBnZW9yZ2Ugd2FzaGluZ3RvbiBib3JuPw==\n\"\"\"\n\nTestBase64Thought = question >> LLMRun() >> PyFunc(lambda x: base64.b64decode(x).decode('utf-8')) >> SubstringEvaluator(\"1733\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestBase64Thought))\n\n",
  "basic_code_understanding.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can solve a simple capture-the-flag like entry in C.\"\n\nTAGS = ['c', \"explain\"]\n\nINPUT = '''#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n#define MAX_INPUTS 10\n#define DATA_LENGTH 100\n#define ENTRY_LENGTH 5\n#define COMMAND_LENGTH 4\n\nstatic const char* flag = \"SECRET\";\nstatic char data[MAX_INPUTS][DATA_LENGTH];\nstatic int input_lengths[MAX_INPUTS];\nstatic int inputs = 0;\n\nvoid data_write() {\n    char input[DATA_LENGTH], len[ENTRY_LENGTH];\n    long length;\n\n    printf(\"Enter data for storage:\\n\"); // Updated message\n    fgets(input, DATA_LENGTH, stdin);\n    input[strcspn(input, \"\\n\")] = 0;\n\n    do {\n        printf(\"Input the data size:\\n\"); // Updated message\n        fgets(len, ENTRY_LENGTH, stdin);\n        length = strtol(len, NULL, 10);\n        if (!length) puts(\"Invalid data length, try again.\"); // Updated message\n    } while (!length);\n\n    if (inputs >= MAX_INPUTS) inputs = 0;\n    strcpy(data[inputs], input);\n    input_lengths[inputs++] = length;\n    printf(\"Data stored at position: %d\\n\", inputs); // Updated message\n}\n\nvoid data_read() {\n    char entry[ENTRY_LENGTH], output[DATA_LENGTH] = {'\\0'};\n    printf(\"Which data position to read?\\n\"); // Updated message\n    fgets(entry, ENTRY_LENGTH, stdin);\n    entry[strcspn(entry, \"\\n\")] = 0;\n\n    long entry_number = strtol(entry, NULL, 10);\n    if (!entry_number) {\n        puts(flag);\n        fseek(stdin, 0, SEEK_END);\n        exit(0);\n    }\n\n    strncpy(output, data[--entry_number], input_lengths[entry_number]);\n    puts(output);\n}\n\nint main() {\n    char input[COMMAND_LENGTH];\n    long command;\n    puts(\"Welcome to the Data Echo System!\"); // Updated message\n\n    while (1) {\n        printf(\"Choose option (1-Write, 2-Read, 3-Exit):\\n\"); // Updated message\n        fgets(input, COMMAND_LENGTH, stdin);\n        input[strcspn(input, \"\\n\")] = 0;\n\n        command = strtol(input, NULL, 10);\n        if (!command) {\n            puts(\"Invalid command, enter 1, 2 or 3.\"); // Updated message\n        } else if (command == 1) {\n            data_write();\n            puts(\"Data written. Perform another action?\"); // Updated message\n        } else if (command == 2) {\n            if (!inputs) {\n                puts(\"Database is empty.\"); // Updated message\n                continue;\n            }\n            data_read();\n            puts(\"Data read. Perform another action?\"); // Updated message\n        } else if (command == 3) {\n            puts(\"Exiting. Thank you!\"); // Updated message\n            break;\n        } else {\n            puts(\"Please enter a valid command: 1, 2, or 3.\"); // Updated message\n        }\n    }\n    return 0;\n}\n'''\n\n\n\nquestion = f\"Give me sequence of the inputs that can make the code output the value of the flag. Here is the code: \\n {INPUT}\"\n\nevaluation = \"\"\"\nBelow is a student's answer to a question about functionality of the code in C: <A>\n\nDoes the student's final answer say that it should first select option 1 and input a data with any length and then select option 2 to read the data and input 0?\n\nIgnore their reasoning. Just look at the final answer, if they say **EXPLICITLY** that it should first input some data by selecting option 1 and then select option 2 to read and input 0. Giving something that's equal but more complicated is wrong. \n\nIf the answer is correct say \"The student passes\" otherwise \"The student fails\".\n\"\"\"\n\n\n\nTestCodeUnderstanding = question >> LLMRun() >> LLMRun(evaluation, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestCodeUnderstanding))\n",
  "convert_dp_to_iterative.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can understand a DP algorithm and then convert it into an iterative implementation.\"\n\nTAGS = ['code', 'performance', 'python']\n\nmy_code = \"\"\"\n@lru_cache(maxsize=None)\ndef solve_dp(graph, lines_left, distance_since_last):\n    if len(graph) < lines_left: return 1e9, []\n    if lines_left == 0 and len(graph) == 0: return 0, []\n\n    # option 1: don't take the line\n    value = (1e9, [])\n    if distance_since_last < 10:\n        value = min(value, solve_dp(graph[1:], lines_left, distance_since_last+1))\n\n    # option 2: take the line\n    if lines_left > 0:\n        sub, how = solve_dp(graph[1:], lines_left-1, 0)\n        sub += graph[0] + (distance_since_last-8)**2\n        value = min(value, (sub, how + [len(graph)]))\n\n    return value\n\"\"\"\n\nquestion = f\"\"\"\nExplain what this code does. Then, convert this recursive function to an iterative one, making explicit the cubic work that's being done. The functions should return exactly the same output but the iterative one should be faster. Call your function `solve_iterative` and match the type signature of the original function.\n\n{my_code}\n\"\"\"\n\ntest_case, answer = make_python_test([(f\"\"\"0\nimport random\nmylist = list(range(100))\nrandom.shuffle(mylist)\nfrom functools import lru_cache\n\n{my_code.replace(\"solve_dp\", \"my_solve_dp\")}\nanswer = solve_iterative(tuple(mylist), len(mylist)//8, 4)\"\"\",\n                                       \"my_solve_dp(tuple(mylist), len(mylist)//8, 4)\")])\n\ndef not_recursive(code):\n    return code.count(\"solve_iterative\") == 1, \"\"\n\nTestProgramRemoveDP = question >> LLMRun() >> ExtractCode() >> \\\n    (PyFunc(not_recursive) & (PythonRun(test_case) >> SubstringEvaluator(answer)))\n\nif __name__ == \"__main__\":\n    print(run_test(TestProgramRemoveDP))\n",
  "convert_to_c.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can rewrite a given Python program into an equivalent C program.\"\n\nTAGS = ['code', 'c']\n\nquestion = \"\"\"\nHere's a python program. Please rewrite it in c with the type signature float solve_dp(float* arr, int size, int lines_left)\n\ndef solve_dp(graph, lines_left):\n    n = len(graph)\n\n    dp = [[[1e9 for _ in range(10)] for _ in range(lines_left + 1)] for _ in range(n + 1)]\n    # Initialize how as -1 indicating no decision made\n    how = [[[-1 for _ in range(10)] for _ in range(lines_left + 1)] for _ in range(n + 1)]\n\n    for i in range(n, -1, -1):\n        for j in range(lines_left + 1):\n            for k in range(10):\n                if i == n and j == 0:\n                    dp[i][j][k] = 0\n                else:\n                    # Option 1: don't take the line\n                    if i < n and k < 9:\n                        if dp[i + 1][j][k + 1] < dp[i][j][k]:\n                            dp[i][j][k] = dp[i + 1][j][k + 1]\n                            how[i][j][k] = k + 1  # Representing choosing not to take the line\n\n                    # Option 2: take the line\n                    if i < n and j > 0:\n                        cost = graph[i] + (k - 8)**2\n                        if cost + dp[i + 1][j - 1][0] < dp[i][j][k]:\n                            dp[i][j][k] = cost + dp[i + 1][j - 1][0]\n                            how[i][j][k] = 0  # Representing choosing to take the line\n\n    # Reconstruct the solution\n\n    i, j, k = 0, lines_left, 6\n    taken_lines = []\n    while i < n:\n        if how[i][j][k] == 0:  # Chose to take the line\n            taken_lines.append(n - i)\n            i += 1\n            j -= 1\n            k = 0\n        else:  # Chose not to take the line\n            i += 1\n            k += 1\n\n    return dp[0][lines_left][6]\n\"\"\"\n\ntest_case, answer = make_c_test([(\"solve_dp(arr, 100, 100)\", \"11290\")], header=\"float arr[] = {71, 89, 34, 63, 19, 94, 54, 61, 88, 20, 66, 46, 26, 87, 55, 81, 6, 2, 72, 75, 98, 78, 24, 95, 73, 7, 56, 48, 14, 99, 64, 51, 69, 77, 28, 47, 8, 22, 49, 3, 62, 32, 10, 82, 35, 18, 85, 60, 83, 23, 5, 40, 41, 68, 53, 52, 44, 45, 65, 84, 93, 25, 13, 1, 31, 11, 12, 97, 38, 0, 43, 90, 36, 70, 33, 17, 21, 30, 16, 15, 74, 67, 58, 37, 39, 96, 79, 29, 27, 92, 86, 9, 80, 42, 57, 91, 59, 4, 76, 50};\")\n\n\nTestProgramRewriteC = question >> LLMRun() >> ExtractCode() >> CRun(test_case) >> SubstringEvaluator(answer)\n\nif __name__ == \"__main__\":\n    print(run_test(TestProgramRewriteC))\n",
  "convert_to_c_simple.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can rewrite a very simple Python program into an equivalent C program.\"\n\nTAGS = ['code', 'c']\n\nquestion = \"\"\"\nHere's a python program. Please rewrite it in c with the type signature float my_sqrt(float, float).\n\ndef my_sqrt(number, precision=0.0001):\n    guess = number / 2  # Start with an initial guess\n    \n    while abs(guess * guess - number) >= precision:\n        guess = (guess + number / guess) / 2  # Improve the guess using the Babylonian method\n    \n    return guess\n\"\"\"\n\ntest_case, answer = make_c_test([(\"(int)(my_sqrt(12345, .001)*10)\", \"1111\")])\n\n\nTestProgramRewriteCSimple = question >> LLMRun() >> ExtractCode() >> CRun(test_case) >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    from llm import llm\n    print(run_test(TestProgramRewriteCSimple))\n",
  "c_rref.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can rewrite a given python in C that performs reduced row echelon form (rref) on a 2D matrix.\"\n\nTAGS = ['code', 'c', 'performance']\n\nquestion = \"\"\"\nRewrite this function in C. You can assume that the input is a 2d matrix correctly formatted. Call it rref(int** matrix, int rows, int cols, int graphlen) where the size is matrix[rows][cols]. Return an int** of used_for_row.\n\ndef rref(matrix, graphlen):\n    PH = 16\n\n    extra = 0\n    col = 0\n    pivots = []\n\n    used_for_row = [set([i]) for i in range(matrix.shape[0])]\n    \n    matrix = matrix%PH\n    while col+extra < matrix.shape[1]-1 and col < matrix.shape[0]:\n\n        if matrix[col,col+extra] == 0:\n            if np.all(matrix[:,col] == 0):\n                extra += 1\n                continue\n            other = np.argwhere(matrix[:,col+extra] != 0).flatten()[-1]\n            if other < col:\n                extra += 1\n                continue\n\n            matrix[col], matrix[other] = list(matrix[other]), list(matrix[col])\n            used_for_row[col], used_for_row[other] = used_for_row[other], used_for_row[col]\n\n        pivots.append(col+extra)\n        pivot = matrix[col,col+extra]\n        if col+extra < graphlen:\n            assert np.abs(pivot) == 1 or np.abs(pivot) == PH-1\n        else:\n            assert np.abs(pivot) == 2 or np.abs(pivot) == PH-2\n            pivot //= 2\n        matrix[col] *= pivot\n        matrix[col] %= PH\n\n        others = np.argwhere(matrix[:,col+extra]).flatten()\n\n        for i in others:\n            if i == col: continue\n            used_for_row[i] |= used_for_row[col]\n            if col < graphlen:\n                matrix[i] -= matrix[col]*matrix[i,col+extra]\n            else:\n                while matrix[i,col+extra] != 0:\n                    matrix[i] = (matrix[i]-matrix[col])%PH\n            matrix[i] %= PH\n                    \n        col += 1\n    \n    matrix = np.array(matrix)%PH\n    return [sorted(x) for x in used_for_row]\n\n\"\"\"\n\ntest_case, answer = make_c_test([(\"got[3][0]+got[3][1]+got[3][2]+got[3][3]+got[3][4]+got[3][5]+got[3][6]\", \"146\")], header=\"\"\"\nint arr0[] = {1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1};\nint arr1[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10};\nint arr2[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -20};\nint arr3[] = {0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0, -20};\nint arr4[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11};\nint arr5[] = {0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0};\nint arr6[] = {0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0};\nint arr7[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, -30};\nint arr8[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\nint arr9[] = {0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, -20};\nint arr10[] = {1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1};\nint arr11[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\nint arr12[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11};\nint arr13[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -14};\nint arr14[] = {0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10};\nint arr15[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, -30};\nint arr16[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10};\nint arr17[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10};\nint arr18[] = {0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11};\nint arr19[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13};\nint arr20[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11};\nint arr21[] = {0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10};\nint arr22[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, -20};\nint arr23[] = {0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10};\nint arr24[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11};\nint arr25[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13};\nint arr26[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\nint arr27[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13};\nint arr28[] = {0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\nint arr29[] = {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\nint arr30[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\nint arr31[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11};\nint arr32[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, -10};\nint arr33[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13};\nint arr34[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11};\nint arr35[] = {0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -9};\nint arr36[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, -20};\nint arr37[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, -40};\nint arr38[] = {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\nint* arr[] = {arr0, arr1, arr2, arr3, arr4, arr5, arr6, arr7, arr8, arr9, arr10, arr11, arr12, arr13, arr14, arr15, arr16, arr17, arr18, arr19, arr20, arr21, arr22, arr23, arr24, arr25, arr26, arr27, arr28, arr29, arr30, arr31, arr32, arr33, arr34, arr35, arr36, arr37, arr38};\nint** got = rref(arr, 39, 49, 37);\n\"\"\")\n\n\nTestCrref = question >> LLMRun() >> ExtractCode() >> CRun(test_case) >> SubstringEvaluator(answer)\n                                                                    \nif __name__ == \"__main__\":\n    print(run_test(TestCrref))\n",
  "c_weird_expression.py": "from evaluator import *\n\nDESCRIPTION = \"This test case is meant to test if the model can correctly evaluate a complex C expression.\"\n\nTAGS = ['explain', 'c']\n\n\nevaluation = \"\"\"\nBelow is a student's answer to a question about what a particular C expression evaluates to: <A>\n\nDoes the student's final answer say that the expression evaluates to *x+2 (or even v+2 is okay)?\n\nIgnore their reasoning. Just look at the final answer, if they say **EXPLICITLY** that the answer is *x+2. Giving something that's equal but more complicated is wrong. \n\nIf the answer is correct say \"The student passes\" otherwise \"The student fails\".\n\"\"\"\n\ndef parse(x):\n    return x.lower().split(\"final answer:\")[-1]\n\nquestion_harder = \"\"\"\nWhat does the c expression `-~++*x--` evaluate to in simplest form? Ignore side effects. Write \"Final answer:\" and then give your answer.\n\"\"\"\n\nTestExplainWeirdC = question_harder >> LLMRun() >> PyFunc(parse) >> LLMRun(evaluation, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestExplainWeirdCEasy))\n",
  "data_extraction_byyear.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can extract structured data from (somewhat) unstructured text.\"\n\nTAGS = ['data']\n\nquestion = '''\nFrom the following data extract the best performing defense each year, in the format {year: robust accuracy}\n\nSo for example the answer for {\"2024\": 69.71, \"2023\": ..., ...}, now fill it in for every other year. Return the answer as a JSON dict.\n\n\nRank\tMethod\tStandard\naccuracy\tAutoAttack\nrobust\naccuracy\tBest known\nrobust\naccuracy\tAA eval.\npotentially\nunreliable\tExtra\ndata\tArchitecture\tVenue\n1\tRobust Principles: Architectural Design Principles for Adversarially Robust CNNs\nIt uses additional 50M synthetic images in training.\t93.27%\t71.07%\t71.07%\t\n\u00d7\n\u00d7\tRaWideResNet-70-16\tBMVC 2023\n2\tBetter Diffusion Models Further Improve Adversarial Training\nIt uses additional 50M synthetic images in training.\t93.25%\t70.69%\t70.69%\t\n\u00d7\n\u00d7\tWideResNet-70-16\tICML 2023\n3\tMixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers\nIt uses an ensemble of networks. The robust base classifier uses 50M synthetic images. 69.71% robust accuracy is due to the original evaluation (Adaptive AutoAttack)\t95.19%\t70.08%\t69.71%\t\n\u00d7\n\u2611\tResNet-152 + WideResNet-70-16\tarXiv, Feb 2024\n4\tImproving the Accuracy-Robustness Trade-off of Classifiers via Adaptive Smoothing\nIt uses an ensemble of networks. The robust base classifier uses 50M synthetic images.\t95.23%\t68.06%\t68.06%\t\n\u00d7\n\u2611\tResNet-152 + WideResNet-70-16 + mixing network\tSIMODS 2024\n5\tDecoupled Kullback-Leibler Divergence Loss\nIt uses additional 20M synthetic images in training.\t92.16%\t67.73%\t67.73%\t\n\u00d7\n\u00d7\tWideResNet-28-10\tarXiv, May 2023\n6\tBetter Diffusion Models Further Improve Adversarial Training\nIt uses additional 20M synthetic images in training.\t92.44%\t67.31%\t67.31%\t\n\u00d7\n\u00d7\tWideResNet-28-10\tICML 2023\n7\tFixing Data Augmentation to Improve Adversarial Robustness\n66.56% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t92.23%\t66.58%\t66.56%\t\n\u00d7\n\u2611\tWideResNet-70-16\tarXiv, Mar 2021\n8\tImproving Robustness using Generated Data\nIt uses additional 100M synthetic images in training. 66.10% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t88.74%\t66.11%\t66.10%\t\n\u00d7\n\u00d7\tWideResNet-70-16\tNeurIPS 2021\n9\tUncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples\n65.87% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t91.10%\t65.88%\t65.87%\t\n\u00d7\n\u2611\tWideResNet-70-16\tarXiv, Oct 2020\n10\tRevisiting Residual Networks for Adversarial Robustness: An Architectural Perspective\t91.58%\t65.79%\t65.79%\t\n\u00d7\n\u2611\tWideResNet-A4\tarXiv, Dec. 2022\n11\tFixing Data Augmentation to Improve Adversarial Robustness\nIt uses additional 1M synthetic images in training. 64.58% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t88.50%\t64.64%\t64.58%\t\n\u00d7\n\u00d7\tWideResNet-106-16\tarXiv, Mar 2021\n12\tStable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending Against Adversarial Attacks\nBased on the model Rebuffi2021Fixing_70_16_cutmix_extra. 64.20% robust accuracy is due to AutoAttack + transfer APGD from Rebuffi2021Fixing_70_16_cutmix_extra\t93.73%\t71.28%\t64.20%\t\n\u2611\n\u2611\tWideResNet-70-16, Neural ODE block\tNeurIPS 2021\n13\tFixing Data Augmentation to Improve Adversarial Robustness\nIt uses additional 1M synthetic images in training. 64.20% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t88.54%\t64.25%\t64.20%\t\n\u00d7\n\u00d7\tWideResNet-70-16\tarXiv, Mar 2021\n14\tExploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness\nIt uses additional 10M synthetic images in training.\t93.69%\t63.89%\t63.89%\t\n\u00d7\n\u00d7\tWideResNet-28-10\tICLR 2023\n15\tImproving Robustness using Generated Data\nIt uses additional 100M synthetic images in training. 63.38% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t87.50%\t63.44%\t63.38%\t\n\u00d7\n\u00d7\tWideResNet-28-10\tNeurIPS 2021\n16\tRobustness and Accuracy Could Be Reconcilable by (Proper) Definition\nIt uses additional 1M synthetic images in training.\t89.01%\t63.35%\t63.35%\t\n\u00d7\n\u00d7\tWideResNet-70-16\tICML 2022\n17\tHelper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off\t91.47%\t62.83%\t62.83%\t\n\u00d7\n\u2611\tWideResNet-34-10\tOpenReview, Jun 2021\n18\tRobust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?\nIt uses additional 10M synthetic images in training.\t87.30%\t62.79%\t62.79%\t\n\u00d7\n\u00d7\tResNest152\tICLR 2022\n19\tUncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples\n62.76% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t89.48%\t62.80%\t62.76%\t\n\u00d7\n\u2611\tWideResNet-28-10\tarXiv, Oct 2020\n20\tExploring Architectural Ingredients of Adversarially Robust Deep Neural Networks\nUses exponential moving average (EMA)\t91.23%\t62.54%\t62.54%\t\n\u00d7\n\u2611\tWideResNet-34-R\tNeurIPS 2021\n21\tExploring Architectural Ingredients of Adversarially Robust Deep Neural Networks\t90.56%\t61.56%\t61.56%\t\n\u00d7\n\u2611\tWideResNet-34-R\tNeurIPS 2021\n22\tParameterizing Activation Functions for Adversarial Robustness\nIt uses additional ~6M synthetic images in training.\t87.02%\t61.55%\t61.55%\t\n\u00d7\n\u00d7\tWideResNet-28-10-PSSiLU\tarXiv, Oct 2021\n23\tRobustness and Accuracy Could Be Reconcilable by (Proper) Definition\nIt uses additional 1M synthetic images in training.\t88.61%\t61.04%\t61.04%\t\n\u00d7\n\u00d7\tWideResNet-28-10\tICML 2022\n24\tHelper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off\nIt uses additional 1M synthetic images in training.\t88.16%\t60.97%\t60.97%\t\n\u00d7\n\u00d7\tWideResNet-28-10\tOpenReview, Jun 2021\n25\tFixing Data Augmentation to Improve Adversarial Robustness\nIt uses additional 1M synthetic images in training. 60.73% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t87.33%\t60.75%\t60.73%\t\n\u00d7\n\u00d7\tWideResNet-28-10\tarXiv, Mar 2021\n26\tDo Wider Neural Networks Really Help Adversarial Robustness?\n87.67%\t60.65%\t60.65%\tUnknown\t\u2611\tWideResNet-34-15\tarXiv, Oct 2020\n27\tImproving Neural Network Robustness via Persistency of Excitation\t86.53%\t60.41%\t60.41%\t\n\u00d7\n\u2611\tWideResNet-34-15\tACC 2022\n28\tRobust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?\nIt uses additional 10M synthetic images in training.\t86.68%\t60.27%\t60.27%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tICLR 2022\n29\tAdversarial Weight Perturbation Helps Robust Generalization\t88.25%\t60.04%\t60.04%\t\n\u00d7\n\u2611\tWideResNet-28-10\tNeurIPS 2020\n30\tImproving Neural Network Robustness via Persistency of Excitation\t89.46%\t59.66%\t59.66%\t\n\u00d7\n\u2611\tWideResNet-28-10\tACC 2022\n31\tGeometry-aware Instance-reweighted Adversarial Training\nUses \n\u2113\n\u221e\n = 0.031 \u2248 7.9/255 instead of 8/255.\t89.36%\t59.64%\t59.64%\t\n\u00d7\n\u2611\tWideResNet-28-10\tICLR 2021\n32\tUnlabeled Data Improves Adversarial Robustness\t89.69%\t59.53%\t59.53%\t\n\u00d7\n\u2611\tWideResNet-28-10\tNeurIPS 2019\n33\tImproving Robustness using Generated Data\nIt uses additional 100M synthetic images in training. 58.50% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t87.35%\t58.63%\t58.50%\t\n\u00d7\n\u00d7\tPreActResNet-18\tNeurIPS 2021\n34\tData filtering for efficient adversarial training\n86.10%\t58.09%\t58.09%\t\n\u00d7\n\u00d7\tWideResNet-34-20\tPattern Recognition 2024\n35\tScaling Adversarial Training to Large Perturbation Bounds\t85.32%\t58.04%\t58.04%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tECCV 2022\n36\tEfficient and Effective Augmentation Strategy for Adversarial Training\t88.71%\t57.81%\t57.81%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tNeurIPS 2022\n37\tLTD: Low Temperature Distillation for Robust Adversarial Training\n86.03%\t57.71%\t57.71%\t\n\u00d7\n\u00d7\tWideResNet-34-20\tarXiv, Nov 2021\n38\tHelper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off\t89.02%\t57.67%\t57.67%\t\n\u00d7\n\u2611\tPreActResNet-18\tOpenReview, Jun 2021\n39\tLAS-AT: Adversarial Training with Learnable Attack Strategy\n85.66%\t57.61%\t57.61%\t\n\u00d7\n\u00d7\tWideResNet-70-16\tarXiv, Mar 2022\n40\tA Light Recipe to Train Robust Vision Transformers\t91.73%\t57.58%\t57.58%\t\n\u00d7\n\u2611\tXCiT-L12\tarXiv, Sep 2022\n41\tData filtering for efficient adversarial training\n86.54%\t57.30%\t57.30%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tPattern Recognition 2024\n42\tA Light Recipe to Train Robust Vision Transformers\t91.30%\t57.27%\t57.27%\t\n\u00d7\n\u2611\tXCiT-M12\tarXiv, Sep 2022\n43\tUncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples\n57.14% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t85.29%\t57.20%\t57.14%\t\n\u00d7\n\u00d7\tWideResNet-70-16\tarXiv, Oct 2020\n44\tHYDRA: Pruning Adversarially Robust Neural Networks\nCompressed model\t88.98%\t57.14%\t57.14%\t\n\u00d7\n\u2611\tWideResNet-28-10\tNeurIPS 2020\n45\tDecoupled Kullback-Leibler Divergence Loss\t85.31%\t57.09%\t57.09%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tarXiv, May 2023\n46\tHelper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off\nIt uses additional 1M synthetic images in training.\t86.86%\t57.09%\t57.09%\t\n\u00d7\n\u00d7\tPreActResNet-18\tOpenReview, Jun 2021\n47\tLTD: Low Temperature Distillation for Robust Adversarial Training\n85.21%\t56.94%\t56.94%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tarXiv, Nov 2021\n48\tUncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples\n56.82% robust accuracy is due to the original evaluation (AutoAttack + MultiTargeted)\t85.64%\t56.86%\t56.82%\t\n\u00d7\n\u00d7\tWideResNet-34-20\tarXiv, Oct 2020\n49\tFixing Data Augmentation to Improve Adversarial Robustness\nIt uses additional 1M synthetic images in training.\t83.53%\t56.66%\t56.66%\t\n\u00d7\n\u00d7\tPreActResNet-18\tarXiv, Mar 2021\n50\tImproving Adversarial Robustness Requires Revisiting Misclassified Examples\t87.50%\t56.29%\t56.29%\t\n\u00d7\n\u2611\tWideResNet-28-10\tICLR 2020\n51\tLAS-AT: Adversarial Training with Learnable Attack Strategy\n84.98%\t56.26%\t56.26%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tarXiv, Mar 2022\n52\tAdversarial Weight Perturbation Helps Robust Generalization\t85.36%\t56.17%\t56.17%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tNeurIPS 2020\n53\tA Light Recipe to Train Robust Vision Transformers\t90.06%\t56.14%\t56.14%\t\n\u00d7\n\u2611\tXCiT-S12\tarXiv, Sep 2022\n54\tAre Labels Required for Improving Adversarial Robustness?\t86.46%\t56.03%\t56.03%\tUnknown\t\u2611\tWideResNet-28-10\tNeurIPS 2019\n55\tRobust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?\nIt uses additional 10M synthetic images in training.\t84.59%\t55.54%\t55.54%\t\n\u00d7\n\u00d7\tResNet-18\tICLR 2022\n56\tUsing Pre-Training Can Improve Model Robustness and Uncertainty\t87.11%\t54.92%\t54.92%\t\n\u00d7\n\u2611\tWideResNet-28-10\tICML 2019\n57\tBag of Tricks for Adversarial Training\n86.43%\t54.39%\t54.39%\tUnknown\t\u00d7\tWideResNet-34-20\tICLR 2021\n58\tBoosting Adversarial Training with Hypersphere Embedding\t85.14%\t53.74%\t53.74%\t\n\u00d7\n\u00d7\tWideResNet-34-20\tNeurIPS 2020\n59\tLearnable Boundary Guided Adversarial Training\nUses \n\u2113\n\u221e\n = 0.031 \u2248 7.9/255 instead of 8/255\t88.70%\t53.57%\t53.57%\t\n\u00d7\n\u00d7\tWideResNet-34-20\tICCV 2021\n60\tAttacks Which Do Not Kill Training Make Adversarial Learning Stronger\t84.52%\t53.51%\t53.51%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tICML 2020\n61\tOverfitting in adversarially robust deep learning\t85.34%\t53.42%\t53.42%\t\n\u00d7\n\u00d7\tWideResNet-34-20\tICML 2020\n62\tSelf-Adaptive Training: beyond Empirical Risk Minimization\nUses \n\u2113\n\u221e\n = 0.031 \u2248 7.9/255 instead of 8/255.\t83.48%\t53.34%\t53.34%\tUnknown\t\u00d7\tWideResNet-34-10\tNeurIPS 2020\n63\tTheoretically Principled Trade-off between Robustness and Accuracy\nUses \n\u2113\n\u221e\n = 0.031 \u2248 7.9/255 instead of 8/255.\t84.92%\t53.08%\t53.08%\tUnknown\t\u00d7\tWideResNet-34-10\tICML 2019\n64\tLearnable Boundary Guided Adversarial Training\nUses \n\u2113\n\u221e\n = 0.031 \u2248 7.9/255 instead of 8/255\t88.22%\t52.86%\t52.86%\t\n\u00d7\n\u00d7\tWideResNet-34-10\tICCV 2021\n65\tAdversarial Robustness through Local Linearization\t86.28%\t52.84%\t52.84%\tUnknown\t\u00d7\tWideResNet-40-8\tNeurIPS 2019\n66\tEfficient and Effective Augmentation Strategy for Adversarial Training\t85.71%\t52.48%\t52.48%\t\n\u00d7\n\u00d7\tResNet-18\tNeurIPS 2022\n67\tAdversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning\nUses ensembles of 3 models.\t86.04%\t51.56%\t51.56%\tUnknown\t\u00d7\tResNet-50\tCVPR 2020\n68\tEfficient Robust Training via Backward Smoothing\n85.32%\t51.12%\t51.12%\tUnknown\t\u00d7\tWideResNet-34-10\tarXiv, Oct 2020\n69\tScaling Adversarial Training to Large Perturbation Bounds\t80.24%\t51.06%\t51.06%\t\n\u00d7\n\u00d7\tResNet-18\tECCV 2022\n70\tImproving Adversarial Robustness Through Progressive Hardening\n86.84%\t50.72%\t50.72%\tUnknown\t\u00d7\tWideResNet-34-10\tarXiv, Mar 2020\n71\tRobustness library\t87.03%\t49.25%\t49.25%\tUnknown\t\u00d7\tResNet-50\tGitHub,\nOct 2019\n72\tHarnessing the Vulnerability of Latent Layers in Adversarially Trained Models\t87.80%\t49.12%\t49.12%\tUnknown\t\u00d7\tWideResNet-34-10\tIJCAI 2019\n73\tMetric Learning for Adversarial Robustness\t86.21%\t47.41%\t47.41%\tUnknown\t\u00d7\tWideResNet-34-10\tNeurIPS 2019\n74\tYou Only Propagate Once: Accelerating Adversarial Training via Maximal Principle\nFocuses on fast adversarial training.\t87.20%\t44.83%\t44.83%\tUnknown\t\u00d7\tWideResNet-34-10\tNeurIPS 2019\n75\tTowards Deep Learning Models Resistant to Adversarial Attacks\t87.14%\t44.04%\t44.04%\tUnknown\t\u00d7\tWideResNet-34-10\tICLR 2018\n76\tUnderstanding and Improving Fast Adversarial Training\nFocuses on fast adversarial training.\t79.84%\t43.93%\t43.93%\tUnknown\t\u00d7\tPreActResNet-18\tNeurIPS 2020\n77\tRethinking Softmax Cross-Entropy Loss for Adversarial Robustness\t80.89%\t43.48%\t43.48%\tUnknown\t\u00d7\tResNet-32\tICLR 2020\n78\tFast is better than free: Revisiting adversarial training\nFocuses on fast adversarial training.\t83.34%\t43.21%\t43.21%\tUnknown\t\u00d7\tPreActResNet-18\tICLR 2020\n79\tAdversarial Training for Free!\t86.11%\t41.47%\t41.47%\tUnknown\t\u00d7\tWideResNet-34-10\tNeurIPS 2019\n80\tMMA Training: Direct Input Space Margin Maximization through Adversarial Training\t84.36%\t41.44%\t41.44%\tUnknown\t\u00d7\tWideResNet-28-4\tICLR 2020\n81\tA Tunable Robust Pruning Framework Through Dynamic Network Rewiring of DNNs\nCompressed model\t87.32%\t40.41%\t40.41%\t\n\u00d7\n\u00d7\tResNet-18\tASP-DAC 2021\n82\tControlling Neural Level Sets\nUses \n\u2113\n\u221e\n = 0.031 \u2248 7.9/255 instead of 8/255.\t81.30%\t40.22%\t40.22%\tUnknown\t\u00d7\tResNet-18\tNeurIPS 2019\n83\tRobustness via Curvature Regularization, and Vice Versa\t83.11%\t38.50%\t38.50%\tUnknown\t\u00d7\tResNet-18\tCVPR 2019\n84\tDefense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training\t89.98%\t36.64%\t36.64%\tUnknown\t\u00d7\tWideResNet-28-10\tNeurIPS 2019\n85\tAdversarial Interpolation Training: A Simple Approach for Improving Model Robustness\t90.25%\t36.45%\t36.45%\tUnknown\t\u00d7\tWideResNet-28-10\tOpenReview, Sep 2019\n86\tAdversarial Defense via Learning to Generate Diverse Attacks\t78.91%\t34.95%\t34.95%\tUnknown\t\u00d7\tResNet-20\tICCV 2019\n87\tSensible adversarial learning\t91.51%\t34.22%\t34.22%\tUnknown\t\u00d7\tWideResNet-34-10\tOpenReview, Sep 2019\n88\tTowards Stable and Efficient Training of Verifiably Robust Neural Networks\nVerifiably robust model with 32.24% provable robust accuracy\t44.73%\t32.64%\t32.64%\tUnknown\t\u00d7\t5-layer-CNN\tICLR 2020\n89\tBilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks\t92.80%\t29.35%\t29.35%\tUnknown\t\u00d7\tWideResNet-28-10\tICCV 2019\n90\tEnhancing Adversarial Defense by k-Winners-Take-All\nUses \n\u2113\n\u221e\n = 0.031 \u2248 7.9/255 instead of 8/255.\n7.40% robust accuracy is due to 1 restart of APGD-CE and 30 restarts of Square Attack\nNote: this adaptive evaluation (Section 5) reports 0.16% robust accuracy on a different model (adversarially trained ResNet-18).\t79.28%\t18.50%\t7.40%\t\n\u2611\n\u00d7\tDenseNet-121\tICLR 2020\n91\tManifold Regularization for Adversarial Robustness\t90.84%\t1.35%\t1.35%\tUnknown\t\u00d7\tResNet-18\tarXiv, Mar 2020\n92\tAdversarial Defense by Restricting the Hidden Space of Deep Neural Networks\t89.16%\t0.28%\t0.28%\tUnknown\t\u00d7\tResNet-110\tICCV 2019\n93\tJacobian Adversarially Regularized Networks for Robustness\t93.79%\t0.26%\t0.26%\tUnknown\t\u00d7\tWideResNet-34-10\tICLR 2020\n94\tClusTR: Clustering Training for Robustness\t91.03%\t0.00%\t0.00%\tUnknown\t\u00d7\tWideResNet-28-10\tarXiv, Jun 2020\n95\tStandardly trained model\t94.78%\t0.0%\t0.0%\tUnknown\t\u00d7\tWideResNet-28-10\tN/A\n'''\n\n\nTestDataYearExtract = question >> LLMRun() >> ExtractJSON() >> JSONSubsetEvaluator({\n    \"2024\": 69.71,\n    \"2023\": 71.07,\n    \"2022\": 65.79,\n    \"2021\": 66.56,\n    \"2020\": 65.87,\n    \"2019\": 59.53,\n    \"2018\": 44.04\n})\n\n\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestDataYearExtract))\n\n",
  "data_table_processing.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can process a large table of text and identify rows with specific values.\"\n\nTAGS = ['data']\n\nquestion = '''\nHere is a table about information of states and territory.\n\nState / territory\tTotal (mi2) \tTotal (km2) \tLand (mi2) \tLand (km2) \tWater (mi2) \tWater (km2) \t%\nwater\n Alaska\t665,384\t1,723,337\t570,641\t1,477,953\t94,743\t245,383\t14.2%\n Texas\t268,596\t695,662\t261,232\t676,587\t7,365\t19,075\t2.7%\n California\t163,695\t423,967\t155,779\t403,466\t7,916\t20,501\t4.8%\n Montana\t147,040\t380,831\t145,546\t376,962\t1,494\t3,869\t1.0%\n New Mexico\t121,590\t314,917\t121,298\t314,161\t292\t757\t0.2%\n Arizona\t113,990\t295,234\t113,594\t294,207\t396\t1,026\t0.3%\n Nevada\t110,572\t286,380\t109,781\t284,332\t791\t2,048\t0.7%\n Colorado\t104,094\t269,601\t103,642\t268,431\t452\t1,170\t0.4%\n Oregon\t98,379\t254,799\t95,988\t248,608\t2,391\t6,191\t2.4%\n Wyoming\t97,813\t253,335\t97,093\t251,470\t720\t1,864\t0.7%\n Michigan\t96,714\t250,487\t56,539\t146,435\t40,175\t104,052\t41.5%\n Minnesota\t86,936\t225,163\t79,627\t206,232\t7,309\t18,930\t8.4%\n Utah\t84,897\t219,882\t82,170\t212,818\t2,727\t7,064\t3.2%\n Idaho\t83,569\t216,443\t82,643\t214,045\t926\t2,398\t1.1%\n Kansas\t82,278\t213,100\t81,759\t211,754\t520\t1,346\t0.6%\n Nebraska\t77,348\t200,330\t76,824\t198,974\t524\t1,356\t0.7%\n South Dakota\t77,116\t199,729\t75,811\t196,350\t1,305\t3,379\t1.7%\n Washington\t71,298\t184,661\t66,456\t172,119\t4,842\t12,542\t6.8%\n North Dakota\t70,698\t183,108\t69,001\t178,711\t1,698\t4,397\t2.4%\n Oklahoma\t69,899\t181,037\t68,595\t177,660\t1,304\t3,377\t1.9%\n Missouri\t69,707\t180,540\t68,742\t178,040\t965\t2,501\t1.4%\n Florida\t65,758\t170,312\t53,625\t138,887\t12,133\t31,424\t18.5%\n Wisconsin\t65,496\t169,635\t54,158\t140,268\t11,339\t29,367\t17.3%\n Georgia\t59,425\t153,910\t57,513\t148,959\t1,912\t4,951\t3.2%\n Illinois\t57,914\t149,995\t55,519\t143,793\t2,395\t6,202\t4.1%\n Iowa\t56,273\t145,746\t55,857\t144,669\t416\t1,077\t0.7%\n New York\t54,555\t141,297\t47,126\t122,057\t7,429\t19,240\t13.6%\n North Carolina\t53,819\t139,391\t48,618\t125,920\t5,201\t13,471\t9.7%\n Arkansas\t53,179\t137,732\t52,035\t134,771\t1,143\t2,961\t2.1%\n Alabama\t52,420\t135,767\t50,645\t131,171\t1,775\t4,597\t3.4%\n Louisiana\t52,378\t135,659\t43,204\t111,898\t9,174\t23,761\t17.5%\n Mississippi\t48,432\t125,438\t46,923\t121,531\t1,509\t3,907\t3.1%\n Pennsylvania\t46,054\t119,280\t44,743\t115,883\t1,312\t3,397\t2.8%\n Ohio\t44,826\t116,098\t40,861\t105,829\t3,965\t10,269\t8.8%\n Virginia\t42,775\t110,787\t39,490\t102,279\t3,285\t8,508\t7.7%\n Tennessee\t42,144\t109,153\t41,235\t106,798\t909\t2,355\t2.2%\n Kentucky\t40,408\t104,656\t39,486\t102,269\t921\t2,387\t2.3%\n Indiana\t36,420\t94,326\t35,826\t92,789\t593\t1,537\t1.6%\n Maine\t35,380\t91,633\t30,843\t79,883\t4,537\t11,750\t12.8%\n South Carolina\t32,020\t82,933\t30,061\t77,857\t1,960\t5,076\t6.1%\n West Virginia\t24,230\t62,756\t24,038\t62,259\t192\t497\t0.8%\n Maryland\t12,406\t32,131\t9,707\t25,142\t2,699\t6,990\t21.8%\n Hawaii\t10,932\t28,313\t6,423\t16,635\t4,509\t11,678\t41.2%\n Massachusetts\t10,554\t27,336\t7,800\t20,202\t2,754\t7,134\t26.1%\n Vermont\t9,616\t24,906\t9,217\t23,871\t400\t1,035\t4.2%\n New Hampshire\t9,349\t24,214\t8,953\t23,187\t397\t1,027\t4.2%\n New Jersey\t8,723\t22,591\t7,354\t19,047\t1,368\t3,544\t15.7%\n Connecticut\t5,543\t14,357\t4,842\t12,542\t701\t1,816\t12.6%\n Puerto Rico\t5,325\t13,791\t3,424\t8,868\t1,901\t4,924\t35.7%\n Delaware\t2,489\t6,446\t1,949\t5,047\t540\t1,399\t21.7%\n Northern Mariana Islands\t1,976\t5,117\t182\t472\t1,793\t4,644\t90.7%\n Rhode Island\t1,545\t4,001\t1,034\t2,678\t511\t1,324\t33.1%\n U.S. Virgin Islands\t733\t1,898\t134\t348\t599\t1,550\t81.7%\n American Samoa\t581\t1,505\t76\t198\t505\t1,307\t86.9%\n Guam\t571\t1,478\t210\t543\t361\t935\t63.2%\n District of Columbia\t68\t177\t61\t158\t7\t19\t10.3%\nMinor Outlying Islands[3][a]\t16\t41\t16\t41\t0\t0\t0.0%\nContiguous US\t3,120,428\t8,081,869\t2,954,843\t7,653,006\t165,589\t428,865\t5.3%\n50 States\t3,796,676\t9,833,342\t3,531,846\t9,147,436\t264,834\t685,907\t7.0%\n50 States and DC\t3,796,744\t9,833,519\t3,531,907\t9,147,594\t264,841\t685,926\t7.0%\n United States\t3,805,927\t9,857,306\t3,535,932\t9,158,022\t269,995\t699,284\t7.1%\n\nList for me each of the states that have more than 20,000 square kilometers of water, from lowest to highest. Don't list any other states.\n\n'''\n\nstepbystep = \"\"\"To answer this question follow these steps in order:\n1. List just the amount of water in each state.\n2. Filter those to ones with over 20k square kilometers of water.\n3. Sort them from lowest to highest.\n4. Say \"The final answer is\" and list the states in that order.\n\n\"\"\"\n\nevaluation = \"This is a student answer about which states have the most water: \\n<A>\\n\\nThe correct answer is: California, Louisiana, Wisconsin, Florida, Michigan, and Alaska (in that order).\\n\\nDoes the student answer exactly these states in this order? Think out loud about their answer. Then, if the student got the states in this order, answer 'The student passes' otherwise answer 'The student fails'.\\n\\n\"\n\n\nTestStateTable = question >> LLMRun() >> ((LLMRun(evaluation, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")) & SubstringEvaluator(\"California\") & SubstringEvaluator(\"Louisiana\") & SubstringEvaluator(\"Wisconsin\") & SubstringEvaluator(\"Wisconsin\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestStateTableStepbystep))\n\n",
  "data_train_timetable.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can extract structured data from (somewhat) unstructured text.\"\n\nTAGS = ['data']\n\ntimetable = \"\"\"\n Station Name 101 501 103 401 105 701 301 403 107 703 303 405 109 705 305 407 111 503 113 505 115 507 117 509 119 511 121 513 123 307 409 125 707 309 411 127 709 311 413 129 711 313 415 131 515 133 135 137 139 141 143 145\nZone Service Type L1 L5 L1 L4 L1 B7 L3 L4 L1 B7 L3 L4 L1 B7 L3 L4 L1 L5 L1 L5 L1 L5 L1 L5 L1 L5 L1 L5 L1 L3 L4 L1 B7 L3 L4 L1 B7 L3 L4 L1 B7 L3 L4 L1 L5 L1 L1 L1 L1 L1 L1 L1\n6 Gilroy -- -- -- -- -- -- -- 5:52am -- -- 6:29am 6:50am -- -- 7:29am -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n6 San Martin -- -- -- -- -- -- -- 6:01am -- -- 6:38am 6:59am -- -- 7:38am -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n6 Morgan Hill -- -- -- -- -- -- -- 6:07am -- -- 6:44am 7:05am -- -- 7:44am -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n5 Blossom Hill -- -- -- -- -- -- -- 6:22am -- -- 6:59am 7:20am -- -- 7:59am -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n5 Capitol -- -- -- -- -- -- -- 6:28am -- -- 7:05am 7:26am -- -- 8:05am -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n4 Tamien 4:20am 5:00am -- 5:36am -- -- 6:16am 6:35am -- -- 7:12am 7:33am 7:46am -- 8:12am -- 8:48am -- -- -- 10:46am -- -- -- 12:46pm -- -- -- 2:46pm -- -- -- -- -- -- 4:46pm -- -- -- 5:43pm -- -- -- 6:48pm -- 7:46pm -- 8:36pm -- 9:36pm -- 11:05pm\n4 San Jose Diridon 4:26am 5:07am 5:13am 5:42am 5:52am 5:57am 6:23am 6:42am 6:52am 6:57am 7:21am 7:40am 7:52am 7:57am 8:21am 8:42am 8:54am 9:41am 9:52am 10:41am 10:52am 11:41am 11:52am 12:41pm 12:52pm 1:41pm 1:52pm 2:41pm 2:52pm 3:20pm 3:42pm 3:52pm 3:57pm 4:21pm 4:42pm 4:52pm 4:57pm 5:21pm 5:42pm 5:52pm 5:57pm 6:21pm 6:42pm 6:54pm 7:41pm 7:52pm 8:11pm 8:43pm 9:12pm 9:43pm 10:30pm 11:12pm\n4 College Park -- -- -- -- -- -- -- -- -- -- -- 7:44am -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 3:24pm -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n4 Santa Clara 4:32am 5:13am 5:19am 5:48am 5:58am -- -- 6:48am 6:58am -- -- 7:48am 7:58am -- -- 8:48am 9:00am 9:47am 9:58am 10:47am 10:58am 11:47am 11:58am 12:47pm 12:58pm 1:47pm 1:58pm 2:47pm 2:58pm -- 3:48pm 3:58pm -- -- 4:48pm 4:58pm -- -- 5:48pm 5:58pm -- -- 6:48pm 7:00pm 7:47pm 7:58pm 8:17pm 8:49pm 9:18pm 9:49pm 10:36pm 11:18pm\n4 Lawrence 4:38am -- 5:25am -- 6:07am -- 6:33am -- 7:07am -- 7:31am -- 8:07am -- 8:31am -- 9:06am -- 10:04am -- 11:04am -- 12:04pm -- 1:04pm -- 2:04pm -- 3:04pm 3:31pm -- 4:07pm -- 4:31pm -- 5:07pm -- 5:31pm -- 6:07pm -- 6:31pm -- 7:06pm -- 8:04pm 8:23pm 8:55pm 9:24pm 9:55pm 10:42pm 11:24pm\n3 Sunnyvale 4:42am 5:21am 5:29am 5:56am 6:12am -- 6:37am 6:56am 7:12am -- 7:35am 7:56am 8:12am -- 8:35am 8:56am 9:10am 9:54am 10:08am 10:54am 11:08am 11:54am 12:08pm 12:54pm 1:08pm 1:54pm 2:08pm 2:54pm 3:08pm 3:36pm 3:56pm 4:12pm -- 4:35pm 4:56pm 5:12pm -- 5:35pm 5:56pm 6:12pm -- 6:35pm 6:56pm 7:10pm 7:54pm 8:08pm 8:27pm 8:59pm 9:28pm 9:59pm 10:46pm 11:28pm\n3 Mountain View 4:47am 5:25am 5:34am 6:01am 6:17am 6:11am 6:42am 7:01am 7:17am 7:11am 7:40am 8:01am 8:17am 8:11am 8:40am 9:01am 9:15am 9:59am 10:13am 10:59am 11:13am 11:59am 12:13pm 12:59pm 1:13pm 1:59pm 2:13pm 2:59pm 3:13pm 3:41pm 4:01pm 4:17pm 4:11pm 4:40pm 5:01pm 5:17pm 5:11pm 5:40pm 6:01pm 6:17pm 6:11pm 6:40pm 7:01pm 7:15pm 7:59pm 8:13pm 8:32pm 9:04pm 9:33pm 10:04pm 10:51pm 11:33pm\n3 San Antonio 4:51am -- 5:38am -- 6:20am -- 6:46am -- 7:20am -- 7:44am -- 8:20am -- 8:44am -- 9:19am -- 10:17am -- 11:17am -- 12:17pm -- 1:17pm -- 2:17pm -- 3:17pm 3:44pm -- 4:20pm -- 4:44pm -- 5:20pm -- 5:44pm -- 6:20pm -- 6:44pm -- 7:19pm -- 8:17pm 8:36pm 9:08pm 9:37pm 10:08pm 10:55pm 11:37pm\n3 California Avenue 4:55am -- 5:42am -- 6:25am -- 6:50am -- 7:25am -- 7:48am -- 8:25am -- 8:48am -- 9:23am -- 10:22am -- 11:22am -- 12:22pm -- 1:22pm -- 2:22pm -- 3:22pm 3:49pm -- 4:25pm -- 4:48pm -- 5:25pm -- 5:48pm -- 6:25pm -- 6:48pm -- 7:23pm -- 8:21pm 8:40pm 9:12pm 9:41pm 10:12pm 10:59pm 11:41pm\n3 Palo Alto 4:59am 5:33am 5:46am 6:08am 6:29am 6:19am 6:54am 7:09am 7:29am 7:19am 7:52am 8:09am 8:29am 8:19am 8:52am 9:09am 9:27am 10:07am 10:26am 11:07am 11:26am 12:07pm 12:26pm 1:07pm 1:26pm 2:07pm 2:26pm 3:07pm 3:26pm 3:53pm 4:09pm 4:29pm 4:19pm 4:52pm 5:09pm 5:29pm 5:19pm 5:52pm 6:09pm 6:29pm 6:19pm 6:52pm 7:09pm 7:27pm 8:07pm 8:25pm 8:44pm 9:17pm 9:45pm 10:17pm 11:03pm 11:46pm\n3 Menlo Park 5:02am 5:37am 5:50am -- 6:32am -- 6:58am -- 7:32am -- 7:56am -- 8:32am -- 8:56am -- 9:31am 10:10am 10:30am 11:10am 11:30am 12:10pm 12:30pm 1:10pm 1:30pm 2:10pm 2:30pm 3:10pm 3:30pm 3:56pm -- 4:32pm -- 4:56pm -- 5:32pm -- 5:56pm -- 6:32pm -- 6:56pm -- 7:30pm 8:10pm 8:28pm 8:47pm 9:20pm 9:49pm 10:20pm 11:07pm 11:50pm\n2 Redwood City 5:08am 5:42am 5:55am 6:15am 6:38am 6:26am 7:03am 7:15am 7:38am 7:26am 8:01am 8:15am 8:38am 8:26am 9:01am 9:15am 9:36am 10:16am 10:35am 11:16am 11:35am 12:16pm 12:35pm 1:16pm 1:35pm 2:16pm 2:35pm 3:16pm 3:35pm 4:02pm 4:15pm 4:38pm 4:26pm 5:01pm 5:15pm 5:38pm 5:26pm 6:01pm 6:15pm 6:38pm 6:26pm 7:01pm 7:15pm 7:36pm 8:16pm 8:34pm 8:53pm 9:27pm 9:55pm 10:27pm 11:13pm 11:56pm\n2 San Carlos 5:13am -- 6:00am 6:20am 6:42am -- -- 7:20am 7:42am -- -- 8:20am 8:42am -- -- 9:20am 9:41am -- 10:40am -- 11:40am -- 12:40pm -- 1:40pm -- 2:40pm -- 3:40pm -- 4:20pm 4:42pm -- -- 5:20pm 5:42pm -- -- 6:20pm 6:42pm -- -- 7:20pm 7:41pm -- 8:39pm 8:58pm 9:32pm 10:00pm 10:32pm 11:18pm 11:59pm\n2 Belmont 5:16am -- 6:04am -- 6:46am -- 7:09am -- 7:46am -- 8:07am -- 8:46am -- 9:07am -- 9:44am -- 10:43am -- 11:43am -- 12:43pm -- 1:43pm -- 2:43pm -- 3:43pm 4:08pm -- 4:46pm -- 5:07pm -- 5:46pm -- 6:07pm -- 6:46pm -- 7:07pm -- 7:44pm -- 8:42pm 9:01pm 9:35pm 10:04pm 10:35pm 11:22pm 12:05am\n2 Hillsdale 5:20am 5:50am 6:08am -- 6:50am 6:34am 7:13am -- 7:50am 7:34am 8:11am -- 8:50am 8:34am 9:11am -- 9:48am 10:23am 10:47am 11:23am 11:47am 12:23pm 12:47pm 1:23pm 1:47pm 2:23pm 2:47pm 3:23pm 3:47pm 4:12pm -- 4:50pm 4:34pm 5:11pm -- 5:50pm 5:34pm 6:11pm -- 6:50pm 6:34pm 7:11pm -- 7:49pm 8:24pm 8:47pm 9:06pm 9:39pm 10:08pm 10:39pm 11:26pm 12:09am\n2 Hayward Park 5:23am -- 6:11am -- 6:53am -- -- -- 7:53am -- -- -- 8:53am -- -- -- 9:51am -- 10:50am -- 11:50am -- 12:50pm -- 1:50pm -- 2:50pm -- 3:50pm -- -- 4:53pm -- -- -- 5:53pm -- -- -- 6:53pm -- -- -- 7:52pm -- 8:50pm 9:09pm 9:42pm 10:11pm 10:42pm 11:29pm 12:12am\n2 San Mateo 5:26am 5:55am 6:14am 6:28am 6:56am -- -- 7:28am 7:56am -- -- 8:28am 8:56am -- -- 9:28am 9:55am 10:29am 10:54am 11:29am 11:54am 12:29pm 12:54pm 1:29pm 1:54pm 2:29pm 2:54pm 3:29pm 3:54pm -- 4:28pm 4:56pm -- -- 5:28pm 5:56pm -- -- 6:28pm 6:56pm -- -- 7:28pm 7:56pm 8:29pm 8:54pm 9:13pm 9:46pm 10:15pm 10:46pm 11:33pm 12:16am\n2 Burlingame 5:30am -- 6:18am 6:31am 7:00am -- -- 7:31am 8:00am -- -- 8:31am 9:00am -- -- 9:31am 9:59am -- 10:58am -- 11:58am -- 12:58pm -- 1:58pm -- 2:58pm -- 3:58pm -- 4:31pm 5:00pm -- -- 5:31pm 6:00pm -- -- 6:31pm 7:00pm -- -- 7:31pm 8:00pm -- 8:58pm 9:17pm 9:50pm 10:18pm 10:50pm 11:36pm 12:19am\n2 Millbrae 5:35am 6:01am 6:23am 6:36am 7:04am 6:44am 7:21am 7:36am 8:04am 7:44am 8:19am 8:36am 9:04am 8:44am 9:19am 9:37am 10:04am 10:36am 11:03am 11:36am 12:03pm 12:36pm 1:03pm 1:36pm 2:03pm 2:36pm 3:03pm 3:36pm 4:03pm 4:20pm 4:36pm 5:04pm 4:44pm 5:19pm 5:36pm 6:04pm 5:44pm 6:19pm 6:36pm 7:04pm 6:44pm 7:19pm 7:36pm 8:06pm 8:35pm 9:04pm 9:23pm 9:55pm 10:24pm 10:55pm 11:42pm 12:26am\n1 San Bruno 5:39am -- 6:28am 6:41am 7:09am -- -- 7:41am 8:09am -- -- 8:41am 9:09am -- -- 9:42am 10:08am -- 11:08am -- 12:08pm -- 1:08pm -- 2:08pm -- 3:08pm -- 4:08pm -- 4:41pm 5:09pm -- -- 5:41pm 6:09pm -- -- 6:41pm 7:09pm -- -- 7:41pm 8:10pm -- 9:08pm 9:27pm 10:00pm 10:29pm 10:59pm 11:47pm 12:30am\n1 South San Francisco 5:43am -- 6:32am -- 7:13am -- 7:28am -- 8:13am -- 8:26am -- 9:13am -- 9:26am -- 10:13am -- 11:13am -- 12:13pm -- 1:13pm -- 2:13pm -- 3:13pm -- 4:13pm 4:27pm -- 5:13pm -- 5:26pm -- 6:13pm -- 6:26pm -- 7:13pm -- 7:26pm -- 8:14pm -- 9:12pm 9:31pm 10:04pm 10:33pm 11:03pm 11:51pm 12:34am\n1 Bayshore 5:50am -- 6:38am -- 7:19am -- -- -- 8:19am -- -- -- 9:19am -- -- -- 10:19am -- 11:19am -- 12:19pm -- 1:19pm -- 2:19pm -- 3:19pm -- 4:19pm -- -- 5:19pm -- -- -- 6:19pm -- -- -- 7:19pm -- -- -- 8:21pm -- 9:19pm 9:38pm 10:11pm 10:39pm 11:10pm 11:57pm 12:40am\n1 22nd Street 5:55am 6:14am 6:44am 6:52am 7:24am -- -- 7:52am 8:24am -- -- 8:52am 9:24am -- -- 9:53am 10:25am 10:49am 11:25am 11:49am 12:25pm 12:49pm 1:25pm 1:49pm 2:25pm 2:49pm 3:25pm 3:49pm 4:25pm 4:36pm 4:52pm 5:24pm 4:58pm 5:35pm 5:52pm 6:24pm 5:58pm 6:35pm 6:52pm 7:25pm 6:58pm 7:35pm 7:52pm 8:27pm 8:50pm 9:25pm 9:44pm 10:16pm 10:45pm 11:15pm 12:03am 12:46am\n1 San Francisco 6:01am 6:20am 6:50am 6:58am 7:31am 7:03am 7:41am 7:58am 8:31am 8:03am 8:39am 8:58am 9:31am 9:03am 9:39am 9:59am 10:31am 10:55am 11:31am 11:55am 12:31pm 12:55pm 1:31pm 1:55pm 2:31pm 2:55pm 3:31pm 3:55pm 4:31pm 4:41pm 4:58pm 5:31pm 5:03pm 5:41pm 5:58pm 6:31pm 6:03pm 6:41pm 6:58pm 7:32pm 7:03pm 7:41pm 7:58pm 8:33pm 8:59pm 9:31pm 9:50pm 10:24pm 10:53pm 11:23pm 12:11am 12:52am\n\"\"\"\n\nquestion_hard = '''It's currently 6:00pm and I'm at Belmont station. I want to get to San Bruno. Tell me how to get there with which train(s) to take, and what time I will arrive, to arrive as soon as possible'''\n\n\nTestTrainScheduleHard = (question_hard+timetable) >> LLMRun() >> (SubstringEvaluator(\"6:41\") & SubstringEvaluator(\"Millbrae\"))\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestTrainScheduleHard))\n\n",
  "date_news_headlines.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can predict the date a few news headlines were published.\"\n\nTAGS = ['fun']\n\nquestion = \"\"\"\nWhat date was this the front page of HN? Format it YYYY-MM-DD.\n\n\n1.\nWe Made One Gram Of Remdesivir (acsh.org)\n709 points by tomstokes on [date] | 231 comments\n2.\nCrafting \u201cCrafting Interpreters\u201d (stuffwithstuff.com)\n777 points by _vbdg on [date] | 75 comments\n3.\nBose QC 35 Firmware 4.5.2 Noise Cancellation Investigation Report (bose.com)\n640 points by robbiet480 on [date] | 323 comments\n4.\nCsound: A sound and music computing system (csound.com)\n226 points by diaphanous on [date] | 92 comments\n5.\nNew Jersey needs COBOL programmers for their unemployment claims system (twitter.com/manicode)\n447 points by enraged_camel on [date] | 297 comments\n6.\nAll models are wrong, but some are completely wrong (rssdss.design.blog)\n305 points by magoghm on [date] | 208 comments\n7.\nConfigs suck? Try a real programming language (beepb00p.xyz)\n289 points by gyre007 on [date] | 345 comments\n8.\nIlo sitelen, a handmade computer for Toki Pona (increpare.com)\n204 points by tobr on [date] | 90 comments\n9.\nThe Svelte Compiler Handbook (lihautan.com)\n330 points by PKop on [date] | 136 comments\n10.\nShow HN: Export HN Favorites to a CSV File\n240 points by gabrielsroka on [date] | 39 comments\n\"\"\"\n\nTestDateNewsHeadlines = question >> LLMRun() >> SubstringEvaluator(\"2020-04-05\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestDateNewsHeadlines))\n\n",
  "db9_pinout.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model knows about old computer ports when prompted ambiguously.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nWhat port has 5 pins on the top and 4 on the bottom?\n\"\"\"\n\nTestDB9 = question >> LLMRun() >> (SubstringEvaluator(\"DB-9\") | SubstringEvaluator(\"DB9\") | SubstringEvaluator(\"DE-9\") | SubstringEvaluator(\"DE9\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestDB9))\n",
  "debug_broken_code_parcount.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can explain a bug in a parallelized wordcount function.\"\n\nTAGS = ['explain', 'python', 'fix']\nimport collections\n\nquestion = \"\"\"\nWhat is the bug in this code that makes it not count right. (I want to print out 4 arrays un-merged. I'll do the merging later. This is intentional.)\n\n#!/usr/bin/env python3\nimport json\nimport multiprocessing\n\ndef count_words_in_chunk(start, end, queue):\n    '''Count how often each word appears in the given chunk of the file.'''\n    word_counts = {}\n    with open('/tmp/file.txt', 'r') as f:\n        f.seek(start)\n        text = f.read(end - start)\n        # Avoid cutting words in half at the beginning\n        if start != 0:\n            text = text.split(' ', 1)[-1]\n        # Avoid cutting words in half at the end\n        f.seek(end)\n        remainder = f.readline()\n        text += remainder\n        # Tokenize and count words\n        for word in text.split():\n            word = word.strip().lower()  # Lowercasing for consistent counting\n            if word:\n                word_counts[word] = word_counts.get(word, 0) + 1\n    # Put result to the queue\n    queue.put(word_counts)\n\ndef main():\n    # Get the size of the file\n    file_size = 0\n    with open('/tmp/file.txt', 'r') as f:\n        f.seek(0, 2)\n        file_size = f.tell()\n\n    # Calculate chunk sizes for 4 cores\n    chunk_size = file_size // 4\n    offsets = [(i * chunk_size, (i + 1) * chunk_size) for i in range(4)]\n    # Last chunk should go until the end of the file\n    offsets[-1] = (offsets[-1][0], file_size)\n\n    # Queue for inter-process communication\n    queue = multiprocessing.Queue()\n\n    # Start 4 worker processes\n    processes = []\n    for start, end in offsets:\n        p = multiprocessing.Process(target=count_words_in_chunk, args=(start, end, queue))\n        processes.append(p)\n        p.start()\n\n    # Collect results\n    for p in processes:\n        p.join()\n\n    # Print results as JSON objects one-per-process\n    while not queue.empty():\n        word_count = queue.get()\n        print(json.dumps(word_count))\n\nif __name__ == \"__main__\":\n    main()\n\n\nList exactly one reason.\n\"\"\"\n\n    \nTestWhyBuggyPythonCountPar = question >> LLMRun() >> LLMRun(\"Below is a student's explanation for why some parallel wordcounting code is not working correctly. You don't need to see the full code, just tell me whether or not the student says that the main reason the code is broken is because 'the chunks could contain the same data or be overlapping' or something like this. Saying 'a word might be split in half' is wrong. Saying 'you might miss a word' is also wrong. Here is their answer:\\n\\n<A>\\n\\nTell me either 'The student passes' or 'The student fails'.\", llm=EVAL_LLM) >> Echo() >> SubstringEvaluator(\"student passes\")\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhyBuggyPythonCountPar))\n",
  "debug_innerhtml_eventlistener.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model knows that editing the innerHTML clears event listeners.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nWhy doesn't the slider onchange event fire when I run this code?\n\n```\n<body>\n\n<script>\n\nvar candidate_paper_list;\n\nclass Dropdown {\n    constructor(parent_html, parent_node) {\n        var r = Math.random()\n        parent_html.innerHTML += `<div class=\"autocomplete-container\" style=\"border: 1px solid #000; padding: 1em\">\n        <label for=\"search-input\">Search Paper Titles:</label>\n        <input type=\"text\" id=\"search-input-${r}\" autocomplete=\"off\">\n        <div id=\"autocomplete-results\"></div>\n</div>`;\n\n    }\n\n}\n\nclass Mul {\n    constructor(parent_html, parent_node) {\n        var div = document.createElement(\"div\");\n        div.style = \"border: 1px solid #000; padding: 1em\"\n        parent_html.appendChild(div);\n        var slider = document.createElement(\"input\");\n        slider.type = \"range\"\n        slider.id = \"foo\"\n        slider.min = \"0\"\n        slider.max = \"100\"\n        slider.step = \".1\"\n        this.slider = slider\n        div.appendChild(slider);\n        this.node = div;\n        div.appendChild(document.createElement(\"br\"));\n        parent_node.child = this;\n\n        slider.addEventListener(\"input\", (event) => {\n            console.log(event.target.value);\n        });\n\n    }\n\n    serialize() {\n        console.log(\"VALUE\", this.slider.value);\n        console.log(\"VALUE\", this.slider);\n        let positive_papers = ['const_mul', this.child.serialize(), +this.slider.value];\n        return positive_papers;\n    }\n}\n\n\ndocument.body.onload = function() {\n    var mul = new Mul(document.body, {})\n    root_node = mul;\n    var dropdown = new Dropdown(mul.node, mul);\n}\n\n</script>\n</body>\n```\n\nTell me how to fix it.\n\"\"\"\n\nevaluation = \"\"\"\nBelow is a student's answer to a question about what a particular instruction does: <A>\n\nDoes the student's say something similar to \"editing the innerHTML clears event listeners\"?\n\nIf the answer is correct say \"The student passes\" otherwise \"The student fails\".\n\"\"\"\n\nTestInnerHTMLEventListener = question >> LLMRun() >> LLMRun(evaluation, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestInnerHTMLEventListener))\n",
  "decompile_py_mid.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can disassemble Python bytecode and create a function that returns a list of prime numbers and their negations.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nDisassemble the following python into a function called foo:\n\n  5           0 BUILD_LIST               0\n              2 STORE_FAST               1 (primes)\n\n  6           4 LOAD_GLOBAL              0 (range)\n              6 LOAD_CONST               1 (2)\n              8 LOAD_FAST                0 (max_number)\n             10 LOAD_CONST               2 (1)\n             12 BINARY_ADD\n             14 CALL_FUNCTION            2\n             16 GET_ITER\n        >>   18 FOR_ITER                94 (to 114)\n             20 STORE_FAST               2 (possible_prime)\n\n  7          22 LOAD_CONST               3 (True)\n             24 STORE_FAST               3 (is_prime)\n\n  9          26 LOAD_GLOBAL              0 (range)\n             28 LOAD_CONST               1 (2)\n             30 LOAD_GLOBAL              1 (int)\n             32 LOAD_FAST                2 (possible_prime)\n             34 LOAD_CONST               4 (0.5)\n             36 BINARY_POWER\n             38 CALL_FUNCTION            1\n             40 LOAD_CONST               2 (1)\n             42 BINARY_ADD\n             44 CALL_FUNCTION            2\n             46 GET_ITER\n        >>   48 FOR_ITER                24 (to 74)\n             50 STORE_FAST               4 (num)\n\n 10          52 LOAD_FAST                2 (possible_prime)\n             54 LOAD_FAST                4 (num)\n             56 BINARY_MODULO\n             58 LOAD_CONST               5 (0)\n             60 COMPARE_OP               2 (==)\n             62 POP_JUMP_IF_FALSE       48\n\n 11          64 LOAD_CONST               6 (False)\n             66 STORE_FAST               3 (is_prime)\n\n 12          68 POP_TOP\n             70 JUMP_ABSOLUTE           74\n             72 JUMP_ABSOLUTE           48\n\n 13     >>   74 LOAD_FAST                3 (is_prime)\n             76 POP_JUMP_IF_FALSE       18\n\n 14          78 LOAD_FAST                1 (primes)\n             80 LOAD_METHOD              2 (append)\n             82 LOAD_FAST                2 (possible_prime)\n             84 CALL_METHOD              1\n             86 POP_TOP\n\n 16          88 LOAD_FAST                2 (possible_prime)\n             90 LOAD_CONST               1 (2)\n             92 BINARY_MODULO\n             94 LOAD_CONST               5 (0)\n             96 COMPARE_OP               3 (!=)\n             98 POP_JUMP_IF_FALSE       18\n\n 17         100 LOAD_FAST                1 (primes)\n            102 LOAD_METHOD              2 (append)\n            104 LOAD_FAST                2 (possible_prime)\n            106 UNARY_NEGATIVE\n            108 CALL_METHOD              1\n            110 POP_TOP\n            112 JUMP_ABSOLUTE           18\n\n 18     >>  114 LOAD_FAST                1 (primes)\n            116 RETURN_VALUE\n\n\"\"\"\n\ntest_case, answer = make_python_test([(\"foo(20)\", \"[2, 3, -3, 5, -5, 7, -7, 11, -11, 13, -13, 17, -17, 19, -19]\")])\n\n\nTestDisasPrimes = question >> LLMRun() >> ExtractCode(keep_main=False) >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\nif __name__ == \"__main__\":\n    print(run_test(TestDisasPrimes))\n",
  "decompile_py_rref.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can decompile a long (300 line) python bytecode function back to python.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nDisassemble the following python into a function called foo:\n\n\n  2           0 LOAD_CONST               1 (16)\n              2 STORE_FAST               2 (PH)\n\n  4           4 LOAD_CONST               2 (0)\n              6 STORE_FAST               3 (extra)\n\n  5           8 LOAD_CONST               2 (0)\n             10 STORE_FAST               4 (col)\n\n  6          12 BUILD_LIST               0\n             14 STORE_FAST               5 (pivots)\n\n  8          16 LOAD_CONST               3 (<code object <listcomp> at 0x101093660, file \"/private/tmp/a.py\", line 8>)\n             18 LOAD_CONST               4 ('rref.<locals>.<listcomp>')\n             20 MAKE_FUNCTION            0\n             22 LOAD_GLOBAL              0 (range)\n             24 LOAD_FAST                0 (matrix)\n             26 LOAD_ATTR                1 (shape)\n             28 LOAD_CONST               2 (0)\n             30 BINARY_SUBSCR\n             32 CALL_FUNCTION            1\n             34 GET_ITER\n             36 CALL_FUNCTION            1\n             38 STORE_FAST               6 (used_for_row)\n\n 10          40 LOAD_FAST                0 (matrix)\n             42 LOAD_FAST                2 (PH)\n             44 BINARY_MODULO\n             46 STORE_FAST               0 (matrix)\n\n 11     >>   48 LOAD_FAST                4 (col)\n             50 LOAD_FAST                3 (extra)\n             52 BINARY_ADD\n             54 LOAD_FAST                0 (matrix)\n             56 LOAD_ATTR                1 (shape)\n             58 LOAD_CONST               5 (1)\n             60 BINARY_SUBSCR\n             62 LOAD_CONST               5 (1)\n             64 BINARY_SUBTRACT\n             66 COMPARE_OP               0 (<)\n             68 EXTENDED_ARG             2\n             70 POP_JUMP_IF_FALSE      628\n             72 LOAD_FAST                4 (col)\n             74 LOAD_FAST                0 (matrix)\n             76 LOAD_ATTR                1 (shape)\n             78 LOAD_CONST               2 (0)\n             80 BINARY_SUBSCR\n             82 COMPARE_OP               0 (<)\n             84 EXTENDED_ARG             2\n             86 POP_JUMP_IF_FALSE      628\n\n 13          88 LOAD_FAST                0 (matrix)\n             90 LOAD_FAST                4 (col)\n             92 LOAD_FAST                4 (col)\n             94 LOAD_FAST                3 (extra)\n             96 BINARY_ADD\n             98 BUILD_TUPLE              2\n            100 BINARY_SUBSCR\n            102 LOAD_CONST               2 (0)\n            104 COMPARE_OP               2 (==)\n            106 EXTENDED_ARG             1\n            108 POP_JUMP_IF_FALSE      262\n\n 14         110 LOAD_GLOBAL              2 (np)\n            112 LOAD_METHOD              3 (all)\n            114 LOAD_FAST                0 (matrix)\n            116 LOAD_CONST               0 (None)\n            118 LOAD_CONST               0 (None)\n            120 BUILD_SLICE              2\n            122 LOAD_FAST                4 (col)\n            124 BUILD_TUPLE              2\n            126 BINARY_SUBSCR\n            128 LOAD_CONST               2 (0)\n            130 COMPARE_OP               2 (==)\n            132 CALL_METHOD              1\n            134 POP_JUMP_IF_FALSE      146\n\n 15         136 LOAD_FAST                3 (extra)\n            138 LOAD_CONST               5 (1)\n            140 INPLACE_ADD\n            142 STORE_FAST               3 (extra)\n\n 16         144 JUMP_ABSOLUTE           48\n\n 17     >>  146 LOAD_GLOBAL              2 (np)\n            148 LOAD_METHOD              4 (argwhere)\n            150 LOAD_FAST                0 (matrix)\n            152 LOAD_CONST               0 (None)\n            154 LOAD_CONST               0 (None)\n            156 BUILD_SLICE              2\n            158 LOAD_FAST                4 (col)\n            160 LOAD_FAST                3 (extra)\n            162 BINARY_ADD\n            164 BUILD_TUPLE              2\n            166 BINARY_SUBSCR\n            168 LOAD_CONST               2 (0)\n            170 COMPARE_OP               3 (!=)\n            172 CALL_METHOD              1\n            174 LOAD_METHOD              5 (flatten)\n            176 CALL_METHOD              0\n            178 LOAD_CONST               6 (-1)\n            180 BINARY_SUBSCR\n            182 STORE_FAST               7 (other)\n\n 18         184 LOAD_FAST                7 (other)\n            186 LOAD_FAST                4 (col)\n            188 COMPARE_OP               0 (<)\n            190 POP_JUMP_IF_FALSE      202\n\n 19         192 LOAD_FAST                3 (extra)\n            194 LOAD_CONST               5 (1)\n            196 INPLACE_ADD\n            198 STORE_FAST               3 (extra)\n\n 20         200 JUMP_ABSOLUTE           48\n\n 22     >>  202 LOAD_GLOBAL              6 (list)\n            204 LOAD_FAST                0 (matrix)\n            206 LOAD_FAST                7 (other)\n            208 BINARY_SUBSCR\n            210 CALL_FUNCTION            1\n            212 LOAD_GLOBAL              6 (list)\n            214 LOAD_FAST                0 (matrix)\n            216 LOAD_FAST                4 (col)\n            218 BINARY_SUBSCR\n            220 CALL_FUNCTION            1\n            222 ROT_TWO\n            224 LOAD_FAST                0 (matrix)\n            226 LOAD_FAST                4 (col)\n            228 STORE_SUBSCR\n            230 LOAD_FAST                0 (matrix)\n            232 LOAD_FAST                7 (other)\n            234 STORE_SUBSCR\n\n 23         236 LOAD_FAST                6 (used_for_row)\n            238 LOAD_FAST                7 (other)\n            240 BINARY_SUBSCR\n            242 LOAD_FAST                6 (used_for_row)\n            244 LOAD_FAST                4 (col)\n            246 BINARY_SUBSCR\n            248 ROT_TWO\n            250 LOAD_FAST                6 (used_for_row)\n            252 LOAD_FAST                4 (col)\n            254 STORE_SUBSCR\n            256 LOAD_FAST                6 (used_for_row)\n            258 LOAD_FAST                7 (other)\n            260 STORE_SUBSCR\n\n 25     >>  262 LOAD_FAST                5 (pivots)\n            264 LOAD_METHOD              7 (append)\n            266 LOAD_FAST                4 (col)\n            268 LOAD_FAST                3 (extra)\n            270 BINARY_ADD\n            272 CALL_METHOD              1\n            274 POP_TOP\n\n 26         276 LOAD_FAST                0 (matrix)\n            278 LOAD_FAST                4 (col)\n            280 LOAD_FAST                4 (col)\n            282 LOAD_FAST                3 (extra)\n            284 BINARY_ADD\n            286 BUILD_TUPLE              2\n            288 BINARY_SUBSCR\n            290 STORE_FAST               8 (pivot)\n\n 27         292 LOAD_FAST                4 (col)\n            294 LOAD_FAST                3 (extra)\n            296 BINARY_ADD\n            298 LOAD_FAST                1 (graphlen)\n            300 COMPARE_OP               0 (<)\n            302 EXTENDED_ARG             1\n            304 POP_JUMP_IF_FALSE      348\n\n 28         306 LOAD_GLOBAL              2 (np)\n            308 LOAD_METHOD              8 (abs)\n            310 LOAD_FAST                8 (pivot)\n            312 CALL_METHOD              1\n            314 LOAD_CONST               5 (1)\n            316 COMPARE_OP               2 (==)\n            318 EXTENDED_ARG             1\n            320 POP_JUMP_IF_TRUE       396\n            322 LOAD_GLOBAL              2 (np)\n            324 LOAD_METHOD              8 (abs)\n            326 LOAD_FAST                8 (pivot)\n            328 CALL_METHOD              1\n            330 LOAD_FAST                2 (PH)\n            332 LOAD_CONST               5 (1)\n            334 BINARY_SUBTRACT\n            336 COMPARE_OP               2 (==)\n            338 EXTENDED_ARG             1\n            340 POP_JUMP_IF_TRUE       396\n            342 LOAD_ASSERTION_ERROR\n            344 RAISE_VARARGS            1\n            346 JUMP_FORWARD            48 (to 396)\n\n 30     >>  348 LOAD_GLOBAL              2 (np)\n            350 LOAD_METHOD              8 (abs)\n            352 LOAD_FAST                8 (pivot)\n            354 CALL_METHOD              1\n            356 LOAD_CONST               7 (2)\n            358 COMPARE_OP               2 (==)\n            360 EXTENDED_ARG             1\n            362 POP_JUMP_IF_TRUE       388\n            364 LOAD_GLOBAL              2 (np)\n            366 LOAD_METHOD              8 (abs)\n            368 LOAD_FAST                8 (pivot)\n            370 CALL_METHOD              1\n            372 LOAD_FAST                2 (PH)\n            374 LOAD_CONST               7 (2)\n            376 BINARY_SUBTRACT\n            378 COMPARE_OP               2 (==)\n            380 EXTENDED_ARG             1\n            382 POP_JUMP_IF_TRUE       388\n            384 LOAD_ASSERTION_ERROR\n            386 RAISE_VARARGS            1\n\n 31     >>  388 LOAD_FAST                8 (pivot)\n            390 LOAD_CONST               7 (2)\n            392 INPLACE_FLOOR_DIVIDE\n            394 STORE_FAST               8 (pivot)\n\n 32     >>  396 LOAD_FAST                0 (matrix)\n            398 LOAD_FAST                4 (col)\n            400 DUP_TOP_TWO\n            402 BINARY_SUBSCR\n            404 LOAD_FAST                8 (pivot)\n            406 INPLACE_MULTIPLY\n            408 ROT_THREE\n            410 STORE_SUBSCR\n\n 33         412 LOAD_FAST                0 (matrix)\n            414 LOAD_FAST                4 (col)\n            416 DUP_TOP_TWO\n            418 BINARY_SUBSCR\n            420 LOAD_FAST                2 (PH)\n            422 INPLACE_MODULO\n            424 ROT_THREE\n            426 STORE_SUBSCR\n\n 35         428 LOAD_GLOBAL              2 (np)\n            430 LOAD_METHOD              4 (argwhere)\n            432 LOAD_FAST                0 (matrix)\n            434 LOAD_CONST               0 (None)\n            436 LOAD_CONST               0 (None)\n            438 BUILD_SLICE              2\n            440 LOAD_FAST                4 (col)\n            442 LOAD_FAST                3 (extra)\n            444 BINARY_ADD\n            446 BUILD_TUPLE              2\n            448 BINARY_SUBSCR\n            450 CALL_METHOD              1\n            452 LOAD_METHOD              5 (flatten)\n            454 CALL_METHOD              0\n            456 STORE_FAST               9 (others)\n\n 37         458 LOAD_FAST                9 (others)\n            460 GET_ITER\n        >>  462 FOR_ITER               154 (to 618)\n            464 STORE_FAST              10 (i)\n\n 38         466 LOAD_FAST               10 (i)\n            468 LOAD_FAST                4 (col)\n            470 COMPARE_OP               2 (==)\n            472 EXTENDED_ARG             1\n            474 POP_JUMP_IF_FALSE      480\n            476 EXTENDED_ARG             1\n            478 JUMP_ABSOLUTE          462\n\n 39     >>  480 LOAD_FAST                6 (used_for_row)\n            482 LOAD_FAST               10 (i)\n            484 DUP_TOP_TWO\n            486 BINARY_SUBSCR\n            488 LOAD_FAST                6 (used_for_row)\n            490 LOAD_FAST                4 (col)\n            492 BINARY_SUBSCR\n            494 INPLACE_OR\n            496 ROT_THREE\n            498 STORE_SUBSCR\n\n 40         500 LOAD_FAST                4 (col)\n            502 LOAD_FAST                1 (graphlen)\n            504 COMPARE_OP               0 (<)\n            506 EXTENDED_ARG             2\n            508 POP_JUMP_IF_FALSE      548\n\n 41         510 LOAD_FAST                0 (matrix)\n            512 LOAD_FAST               10 (i)\n            514 DUP_TOP_TWO\n            516 BINARY_SUBSCR\n            518 LOAD_FAST                0 (matrix)\n            520 LOAD_FAST                4 (col)\n            522 BINARY_SUBSCR\n            524 LOAD_FAST                0 (matrix)\n            526 LOAD_FAST               10 (i)\n            528 LOAD_FAST                4 (col)\n            530 LOAD_FAST                3 (extra)\n            532 BINARY_ADD\n            534 BUILD_TUPLE              2\n            536 BINARY_SUBSCR\n            538 BINARY_MULTIPLY\n            540 INPLACE_SUBTRACT\n            542 ROT_THREE\n            544 STORE_SUBSCR\n            546 JUMP_FORWARD            50 (to 598)\n\n 43     >>  548 LOAD_FAST                0 (matrix)\n            550 LOAD_FAST               10 (i)\n            552 LOAD_FAST                4 (col)\n            554 LOAD_FAST                3 (extra)\n            556 BINARY_ADD\n            558 BUILD_TUPLE              2\n            560 BINARY_SUBSCR\n            562 LOAD_CONST               2 (0)\n            564 COMPARE_OP               3 (!=)\n            566 EXTENDED_ARG             2\n            568 POP_JUMP_IF_FALSE      598\n\n 44         570 LOAD_FAST                0 (matrix)\n            572 LOAD_FAST               10 (i)\n            574 BINARY_SUBSCR\n            576 LOAD_FAST                0 (matrix)\n            578 LOAD_FAST                4 (col)\n            580 BINARY_SUBSCR\n            582 BINARY_SUBTRACT\n            584 LOAD_FAST                2 (PH)\n            586 BINARY_MODULO\n            588 LOAD_FAST                0 (matrix)\n            590 LOAD_FAST               10 (i)\n            592 STORE_SUBSCR\n            594 EXTENDED_ARG             2\n            596 JUMP_ABSOLUTE          548\n\n 45     >>  598 LOAD_FAST                0 (matrix)\n            600 LOAD_FAST               10 (i)\n            602 DUP_TOP_TWO\n            604 BINARY_SUBSCR\n            606 LOAD_FAST                2 (PH)\n            608 INPLACE_MODULO\n            610 ROT_THREE\n            612 STORE_SUBSCR\n            614 EXTENDED_ARG             1\n            616 JUMP_ABSOLUTE          462\n\n 47     >>  618 LOAD_FAST                4 (col)\n            620 LOAD_CONST               5 (1)\n            622 INPLACE_ADD\n            624 STORE_FAST               4 (col)\n            626 JUMP_ABSOLUTE           48\n\n 49     >>  628 LOAD_GLOBAL              2 (np)\n            630 LOAD_METHOD              9 (array)\n            632 LOAD_FAST                0 (matrix)\n            634 CALL_METHOD              1\n            636 LOAD_FAST                2 (PH)\n            638 BINARY_MODULO\n            640 STORE_FAST               0 (matrix)\n\n 50         642 LOAD_CONST               8 (<code object <listcomp> at 0x101093b30, file \"/private/tmp/a.py\", line 50>)\n            644 LOAD_CONST               4 ('rref.<locals>.<listcomp>')\n            646 MAKE_FUNCTION            0\n            648 LOAD_FAST                6 (used_for_row)\n            650 GET_ITER\n            652 CALL_FUNCTION            1\n            654 RETURN_VALUE\n\nDisassembly of <code object <listcomp> at 0x101093660, file \"/private/tmp/a.py\", line 8>:\n  8           0 BUILD_LIST               0\n              2 LOAD_FAST                0 (.0)\n        >>    4 FOR_ITER                14 (to 20)\n              6 STORE_FAST               1 (i)\n              8 LOAD_GLOBAL              0 (set)\n             10 LOAD_FAST                1 (i)\n             12 BUILD_LIST               1\n             14 CALL_FUNCTION            1\n             16 LIST_APPEND              2\n             18 JUMP_ABSOLUTE            4\n        >>   20 RETURN_VALUE\n\nDisassembly of <code object <listcomp> at 0x101093b30, file \"/private/tmp/a.py\", line 50>:\n 50           0 BUILD_LIST               0\n              2 LOAD_FAST                0 (.0)\n        >>    4 FOR_ITER                12 (to 18)\n              6 STORE_FAST               1 (x)\n              8 LOAD_GLOBAL              0 (sorted)\n             10 LOAD_FAST                1 (x)\n             12 CALL_FUNCTION            1\n             14 LIST_APPEND              2\n             16 JUMP_ABSOLUTE            4\n        >>   18 RETURN_VALUE\n\n\"\"\"\n\ntest_case, answer = make_python_test([(\"foo(arr, 37)\", \"[[0, 38], [0, 38], [0, 6, 28, 35, 38], [0, 6, 18, 21, 28, 35, 38], [0, 6, 28, 35, 38], [0, 6, 21, 28, 35, 38], [0, 5, 10, 23, 38], [0, 5, 10, 23, 38], [0, 5, 10, 23, 38], [0, 5, 9, 10, 23, 38], [0, 5, 9, 10, 13, 17, 23, 30, 38], [0, 5, 9, 10, 11, 23, 27, 38], [0, 5, 9, 10, 11, 23, 27, 38], [0, 5, 9, 10, 11, 23, 25, 27, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 24, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 5, 9, 10, 13, 17, 23, 30, 38], [0, 5, 9, 10, 13, 17, 23, 30, 38], [0, 5, 9, 10, 13, 17, 23, 30, 32, 38], [0, 5, 9, 10, 13, 17, 23, 30, 32, 33, 38], [0, 22, 29, 38], [0, 22, 29, 38], [0, 6, 28, 35, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [14, 18, 21, 28, 35]]\")], header=\"arr = np.array([[1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -20], [0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0, -20], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, -30], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, -20], [1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -14], [0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, -30], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, -20], [0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -9], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, -20], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, -40], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\")\n\n\nTestDisasRref = question >> LLMRun() >> ExtractCode() >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestDisasRref))\n",
  "decompile_py_simple.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can disassemble a simple Python function from its bytecode.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nDisassemble the following python into a function called foo:\n\n 56           0 LOAD_FAST                0 (a)\n              2 LOAD_FAST                1 (b)\n              4 LOAD_FAST                0 (a)\n              6 BINARY_MULTIPLY\n              8 BINARY_ADD\n             10 RETURN_VALUE\n\n\"\"\"\n\ntest_case, answer = make_python_test([(\"foo(5, 7)\", \"5+7*5\")])\n\n\nTestDisas1 = question >> LLMRun() >> ExtractCode(keep_main=False) >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestDisas1))\n",
  "dedent_code_fn.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can write a Python function that removes excess indentation from a given block of code.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nWrite a python function `dedent(str)` that takes as input a string of python code and de-dents it maximally. Lines that are entirely blank don't count.\n\"\"\"\n\ntest_case, answer = make_python_test([(\"\"\"dedent('''    print(\"hello\")\n    print(\"world\")\n\n    if True:\n        print(\"true\")\n    else:\n        print(\"false\")''')\"\"\", '''\"\"\"print(\"hello\")\nprint(\"world\")\n\nif True:\n    print(\"true\")\nelse:\n    print(\"false\")\"\"\"''')])\n\n\nTestDedent = question >> LLMRun() >> ExtractCode(keep_main=True) >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\nif __name__ == \"__main__\":\n    print(run_test(TestDedent))\n\n",
  "docker_cuda.py": "from evaluator import *\n\nDESCRIPTION = \"This test case checks if the model can debug a docker cuda error\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nnicholas@server:~/project$ sudo docker start myproject\n[sudo] password for nicholas: \nError response from daemon: could not select device driver \"\" with capabilities: [[gpu]]\nError: failed to start containers: myproject\n\"\"\"\n\nTestFixDockerCuda = question >> LLMRun() >> (SubstringEvaluator(\"nvidia-container-toolkit\") | SubstringEvaluator(\"nvidia-docker2\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestFixDockerCuda))\n",
  "do_uudecode.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can successfully uudecode a given string.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nuudecode this string:\n\nH,3DR-S<-\"C(X,C<U#0HT.3(V-@T*-C`X,C8-\"C(X,S8V#0HR,#<Q-0``\n\"\"\"\n\n\nTestUUDecode = question >> LLMRun() >> (SubstringEvaluator(\"60826\") & SubstringEvaluator(\"20715\") & SubstringEvaluator(\"19277\"))\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestUUDecode))\n\n",
  "draw_flag_bmp.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can write a C program that draws an image. This test requires the ability to understand the .bmp specification, and draw a flag that can be correctly parsed and seen by the evaluator.\"\n\nTAGS = ['code', 'c', 'visual']\n\nquestion = \"\"\"\nWrite me a C program that emits a .bmp that's a depiction of an american flag to stdout.\n\"\"\"\n\nTestFlagDrawBMP = question >> LLMRun() >> ExtractCode(keep_main=True) >> CRun(out_bytes=True) >> LLMVisionRun(\"What flag is shown in this image?\") >> (SubstringEvaluator(\"United States\") | SubstringEvaluator(\"USA\") | SubstringEvaluator(\"America\"))\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestFlagDrawBMP))\n",
  "easy_parser_generator.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can understand a vague BNF-style grammar and write a Python function that evaluates expressions based on the grammar rules.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nGiven this BNF-style grammar write a Python function `evaluate(str)` that evaluates the expression\n\n<expression> ::= <term> | <expression> '+' <term> | <expression> '-' <term>\n\n<term> ::= <factor> | <term> '*' <factor> | <term> '/' <factor>\n\n<factor> ::= <number> | '(' <expression> ')' | <prefix-op> <factor>\n\n<prefix-op> ::= 'sin' | 'cos'\n\n<number> ::= <digit> | <number> <digit>\n\n<digit> ::= '0' | '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9'\n\nFor example you can evaluate this\n\nsin(3 + 2 * 4) - cos(15) = -0.240302\n\"\"\"\n\ntest_case, answer = make_python_test([(\"round(evaluate('sin(35+2*4)-cos(15)'), 5)\", \"-0.07209\")])\n\nTestSimpleBNF = question >> LLMRun() >> ExtractCode() >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestSimpleBNF))\n\n\n",
  "emacs_lisp_silence_cmd.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can understand a vague error for an emacs lisp question.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nIn emacs lisp how do I silence \"Process python-process finished\"\n\"\"\"\n\nTestLispSilencePython = question >> LLMRun() >> SubstringEvaluator(\"set-process-sentinel\")\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestLispSilencePython))\n",
  "emoji_movies.py": "from evaluator import *\n\nDESCRIPTION = \"A for-fun test to see if the model can go movie title -> emoji -> movie title.\"\n\nTAGS = ['fun']\n\nquestion = \"\"\"\nFor each of the following ten movies give at most 5 emoji that would best describe the movie.\n\nGive your answer as a JSON array. So If I asked for\n```[\"Finding Nemo\", \"Toy Story\"]```\n\nyou might might answer\n\n```json\n{\"Finding Nemo\": [\"\ud83d\udc20\", \"\ud83d\udc1f\", \"\ud83d\udc21\", \"\ud83d\udc2c\", \"\ud83d\udc33\"],\n\"Toy Story\": [\"\ud83d\ude80\", \"\u2694\ufe0f,\", \"\ud83e\udd16\", \"\ud83d\udc7d\", \"\ud83c\udf0c\"]}\n```.\n\nEach emoji must be a single utf8 character. ABSOLUTELY NO ZERO WIDTH JOINING. (So, max(len(emoji) for movie in result.values() for emoji in movie) == 1)\n\nNow give me answers for these movies:\n\n```[\"The Lion King\", \"The Nightmare Before Christmas\", \"The Godfather\", \"The Matrix\", \"Casablanca\", \"Raiders of the Lost Ark\", \"V for Vendetta\", \"The Princess Bride\", \"Back to the Future\", \"Dune\"]```\n\nGive ONLY a JSON output. Nothing else.\n\"\"\"\n\nundo = \"\"\"\nFor each of the following ten movies described by 5 emoji, give the movie title that best matches.\n\nGive your answer as a JSON list. So If I asked for\n```[[\"\ud83d\udc20\", \"\ud83d\udc1f\", \"\ud83d\udc21\", \"\ud83d\udc2c\", \"\ud83d\udc33\"], [\"\ud83d\ude80\", \"\u2694\ufe0f,\", \"\ud83e\udd16\", \"\ud83d\udc7d\", \"\ud83c\udf0c\"]]```\n\nYou might answer\n\n```json\n[\"Finding Nemo\", \"Toy Story\"]]\n```.\n\nNow give me answers for these movies:\n\n```<A>```\n\nWhat are the names of the movie titles?\n\"\"\"\n\ndef extract(x):\n    try:\n        x = json.loads(x)\n    except:\n        print(\"Failed processing\")\n        return \"\"\n    send = list(x.values())\n    # I'll be nice...\n    send = [[x for x in y if len(x) <= 2] for y in send]\n    return str(send).replace(\"], [\", \"],\\n[\")\n\ndef count(x):\n    try:\n        x = json.loads(x)\n        count = 0\n        for correct, guessed in zip([\"The Lion King\", \"The Nightmare Before Christmas\", \"The Godfather\", \"The Matrix\", \"Casablanca\", \"Raiders of the Lost Ark\", \"V for Vendetta\", \"The Princess Bride\", \"Back to the Future\", \"Dune\"], x):\n            if correct.lower() == guessed.lower():\n                count += 1\n        return count >= 8, \"OK\"\n    except:\n        return False, \"Not a JSON list\"\n\n\nTestEmojiMovie = question >> LLMRun() >> ExtractJSON() >> PyFunc(extract) >> LLMRun(undo) >> ExtractJSON() >> PyFunc(count)\n\nif __name__ == \"__main__\":\n    print(run_test(TestEmojiMovie))\n",
  "explain_code_prime.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can interpret a minified JavaScript function and explain its function.\"\n\nTAGS = ['code', 'explain']\n\nquestion = \"\"\"\nWhat does this code do\n\nfunction q(e){if(e<=1)return!1;if(e<=3)return!0;if(e%2==0||e%3==0)return!1;for(let r=5;r*r<=e;r+=6)if(e%r==0||e%(r+2)==0)return!1;return!0}function z(){const e=[];let r=2;for(;e.length<20;)q(r)&&e.push(r),r++;return e.reverse(),e.reverse(),e}console.log(z());\n\"\"\"\n\nTestExplainPrime = question >> LLMRun() >> LLMRun(\"I showed a student some code that calculates 20 primes. I then tested their understanding by asking them to explain it to me. I'm going to show you their answer, I want you to evaluate whether or not their answer says something to the effect of \\\"it calculates the first 20 primes\\\". Do not judge their reasonining, just tell me if they got it right. \\n Student Answer: \\\"<A>\\\"\\n\\n Think out loud then answer either \\\"The student passes\\\" or \\\"The student fails\\\".\", llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestExplainPrime))\n",
  "explain_code_prime2.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can interpret a minified and obfuscated JavaScript function and explain its function.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nWhat does this code do\n\nfunction z(){let e=[],n=[];for(let r=2;e.length<20;r++)(n=n.map(e=>e-1)).some(e=>0===e)?n=n.map((n,r)=>0===n?e[r]:n):(e.push(r),n.push(r));return e}console.log(z());\n\"\"\"\n\nTestExplainPrime2 = question >> LLMRun() >> LLMRun(\"I showed a student some code that calculates 20 primes. I then tested their understanding by asking them to explain it to me. I'm going to show you their answer, I want you to evaluate whether or not their answer says something to the effect of \\\"it calculates the first 20 primes\\\". Do not judge their reasonining, just tell me if they got it right. \\n Student Answer: \\\"<A>\\\"\\n\\n Think out loud then answer either \\\"The student passes\\\" or \\\"The student fails\\\".\", llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestExplainPrime2))\n",
  "explain_vbroadcast.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can correctly explain what the VPBROADCASTB instruction does.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nWhat does the VPBROADCASTB instruction do?\n\"\"\"\n\nevaluation = \"\"\"\nBelow is a student's answer to a question about what a particular instruction does: <A>\n\nDoes the student's say something similar to \"Load integer data from the source operand (the second operand) and broadcast to all elements of the destination operand (the first operand).\"?\n\nIf the answer is correct say \"The student passes\" otherwise \"The student fails\".\n\"\"\"\n\nTestExplainBroadcast = question >> LLMRun() >> LLMRun(evaluation, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestExplainBroadcast))\n",
  "extract_emails.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can accurately extract and identify invalid email addresses from a given text file. Models that are \\\"overly safe\\\" will fail.\"\n\nTAGS = ['data']\n\nquestion = \"\"\"\nExtract all the email addresses that are invalid from this text file:\n\n\n<foo-bar@example.com>: Domain example.com does not accept mail (nullMX)\n\n<noperson628@gmail.com>: host mail.banglemail.com[138.68.190.241] said: 550\n   5.1.1 <noperson628@gmail.com>: Recipient address rejected: User unknown in\n   virtual alias table (in reply to RCPT TO command)\n\n<againredacted@example.info>: host emx.mail.ru[218.62.518.180] said: 550\n   Message was not accepted -- invalid mailbox.  Local mailbox\n   againredacted@example.info is unavailable: user not found (in reply to end\n   of DATA command)\n\n<xsz032by05wfk5jpe@mail4you.biz>: host emx.mail.ru[217.69.139.180] said: 550\n   Message was not accepted -- invalid mailbox.  Local mailbox\n   xsz032by05wfk5jpe@mail4you.biz is unavailable: user not found (in reply to\n   end of DATA command)\n\n<wontsay@bestpay.com.cn>: host mta-ent.21cn.com[183.56.203.132] said: 543\n   suspected spams or account(IP) exception (in reply to end of DATA command)\n\n<redactedfinally@gmail.com>: host gmail-smtp-in.l.google.com[142.250.27.26] said:\n   550-5.1.1 The email account that you tried to reach does not exist. Please\n   try 550-5.1.1 double-checking the recipient's email address for typos or\n\n\"\"\"\n\nTestExtractEmail = question >> LLMRun() >> (SubstringEvaluator(\"foo-bar@example.com\") & SubstringEvaluator(\"noperson628@gmail.com\") & SubstringEvaluator(\"againredacted@example.info\") & SubstringEvaluator(\"xsz032by05wfk5jpe@mail4you.biz\") & SubstringEvaluator(\"wontsay@bestpay.com.cn\") & SubstringEvaluator(\"redactedfinally@gmail.com\") & ~SubstringEvaluator(\"gmail-smtp-in.l.google.com\") & ~SubstringEvaluator(\"217.69.139.180\"))\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestExtractEmail))\n    \n",
  "extract_references.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can extract paper tiles from a block of text.\"\n\nTAGS = ['code', 'python']\n\nquestion = '''Extract a list the titles of the papers from the following list of references.\nStart your response\n\n```json\n[title_1, title_2, ...]\n```\n\nHere's the block of text:\n\nA Suffix Arrays                                                         [45] SHOKRI, R., STRONATI, M., SONG, C., AND                                                              \nA suffix of length k of a string x are the last k characters (or,       SHMATIKOV, V. Membership inference attacks against                                                        \ntokens) of this string, i.e,. x[\u2212k:]                                    machine learning models. In IEEE Symposium on                                                             \n. If we want to know: \u201cwas                                              Security and Privacy (2017).                                                                              \n0 100 200 300                                                           [46] SOLDAINI, L. AI2 Dolma: 3 trillion token open corpus                                                 \nlength of k-gram                                                        for language model pretraining, 2023.                                                                     \n104                                                                     [47] SOMEPALLI, G., SINGLA, V., GOLDBLUM, M., GEIPING, J., AND GOLDSTEIN, T. Diffusion art or digital     \n105                                                                     forgery? Investigating data replication in diffusion models. In CVPR (2023).                              \n106                                                                     [48] SOUTHWOOD, T. R. E., AND HENDERSON, P. A. Ecological methods. John Wiley & Sons, 2009.               \n# generated kgrams                                                      [49] TOUVRON, H., LAVRIL, T., IZACARD, G., MARTINET, X., LACHAUX, M.-A., LACROIX, T., ROZI\u00c8RE, B., GOYAL, \nin training data                                                        N., HAMBRO, E., AZHAR, F., RODRIGUEZ, A., JOULIN, A., GRAVE, E., AND LAMPLE,                              \nFigure 14: The suffix length threshold k significantly impacts          G. LLaMA: Open and Efficient Foundation Language                                                          \nthe rate of data determined to be memorized. We set k = 50.             Models, 2023.                                                                                             \nx                                                                       [50] TOUVRON, H., MARTIN, L., STONE, K., ALBERT, P.,                                                      \n\u2032                                                                       ALMAHAIRI, A., BABAEI, Y., BASHLYKOV, N., BATRA, S., BHARGAVA, P., BHOSALE, S., ET AL. LLaMA              \n[\u2212k:]                                                                   2: Open foundation and fine-tuned chat models. arXiv                                                      \nin x\u201d, then we would have to do an O(n) search checking                 preprint arXiv:2307.09288 (2023).                                                                         \nall suffixes of x. This linear scan is expensive if x is large,         [51] TTI. Introducing Falcon 180b.                                                                        \nas it is in training large language models, often terabytes in          [52] YEOM, S., GIACOMELLI, I., FREDRIKSON, M., AND                                                        \nsize. Instead, a suffix array will enable us to do this search          JHA, S. Privacy risk in machine learning: Analyzing                                                       \nefficiently in O(logn) time.                                            the connection to overfitting. In IEEE CSF (2018).                                                        \nA suffix array s over a dataset X, denoted as s(X) is a                 [53] ZELTERMAN, D. Smooth nonparametric estimation of                                                     \ndata structure that indexes all suffixes of this string in a            the quantile function. Journal of statistical planning                                                    \nlexicographically-sorted ordering. This sorting, as we will             and inference 26, 3 (1990), 339\u2013352.                                                                      \nsee, is important as it enables efficient binary searches for a         [54] ZHANG, S., ROLLER, S., GOYAL, N., ARTETXE, M.,                                                       \nparticular substring/suffix.                                            CHEN, M., CHEN, S., DEWAN, C., DIAB, M., LI, X.,                                                          \nIn the simplest form, we can consider the suffix array of a             LIN, X. V., MIHAYLOV, T., OTT, M., SHLEIFER, S.,                                                          \nword, e.g., x =\u201cbanana\u201d. The following is the set of all suffixes       SHUSTER, K., SIMIG, D., KOURA, P. S., SRIDHAR,                                                            \nas obtained by traversing the string backwards and keeping only         A., WANG, T., AND ZETTLEMOYER, L. Opt: Open                                                               \nunique suffixes, in this case, all suffixes: {\u201ca\u201d, \u201cna\u201d,                pre-trained transformer language models, 2022.                                                            \n\u201cana\u201d, \u201cnana\u201d, \u201c anana\u201d, \u201cbanana\u201d}, which are represented by            [55] ZIEGLER, A. Github Copilot research recitation, 2021.                                                \nthe indices s = {5,4,3,2,1,0}. In this form, we still require           [56] ZOU, A., WANG, Z., KOLTER, J. Z., AND FREDRIKSON, M. Universal and transferable adversarial          \nan O(n) search as there is no ordering. However, a suffix array         attacks on aligned language models. arXiv preprint                                                        \nwill store these suffixes in a lexicographically sorted ordering.       arXiv:2307.15043 (2023).                                                                                  \n'''\n\n\nanswer = set([\n        \"membership inference attacks against machine learning models\",\n        \"ai2 dolma: 3 trillion token open corpus for language model pretraining\",\n        \"diffusion art or digital forgery? investigating data replication in diffusion models\",\n        \"ecological methods\",\n        \"llama: open and efficient foundation language models\",\n        \"llama 2: open foundation and fine-tuned chat models\",\n        \"introducing falcon 180b\",\n        \"privacy risk in machine learning: analyzing the connection to overfitting\",\n        \"smooth nonparametric estimation of the quantile function\",\n        \"opt: open pre-trained transformer language models\",\n        \"github copilot research recitation\",\n        \"universal and transferable adversarial attacks on aligned language models\",\n        ])\n\ndef check_ok(dat):\n    import json\n    dat = dat.replace(\"```json\", \"```\")\n    dat = dat.split(\"```\")[1]\n    dat = dat.lower().replace(\".\",\"\")\n    return set(json.loads(dat)) == answer\n\n\nTestExtractRef = question >> LLMRun() >> PyFunc(check_ok)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestExtractRef))\n\n\n\n\n",
  "faster_l2_diff.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can optimize a given Python program for speed and memory efficiency.\"\n\nTAGS = ['code', 'performance', 'python']\n\nquestion = \"\"\"\nI have this program\n\nimport numpy as np\ndef diff(out, templates):\n    return (out[:, None, :] - templates[None, :, :]).square().sum(2)\n\nMake it faster and more memory efficient without expanding the arrays. Call it `diff` still.\n\"\"\"\n\ntracer = \"\"\"\nimport tracemalloc\nimport numpy as np\n\nif __name__ == \"__main__\":\n    tracemalloc.start()  # Start tracing\n\n    snapshot1 = tracemalloc.take_snapshot()  # Before execution\n    diff(np.arange(10000).reshape((100, 100)), np.arange(10000).reshape((100, 100)))\n    snapshot2 = tracemalloc.take_snapshot()  # After execution\n\n    # Calculate the difference in memory usage\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()  # Stop tracing\n\n    if peak < 10**6:\n        print(\"Efficient\")\n\"\"\"\n\ntest_case, answer = make_python_test([(\"diff(np.arange(100).reshape((10, 10)), np.arange(100).reshape((10, 10))).sum()\", \"1650000\")])\n\n\nTestFastL2 = question >> LLMRun() >> ExtractCode() >> \\\n          ((PythonRun(test_case) >> SubstringEvaluator(answer)) &\n           (PythonRun(tracer) >> SubstringEvaluator(\"Efficient\")) &\n           (LLMRun(\"Below is a students answer to a question on writing efficient code. They're supposed to be vectorizing a loop. Is the following code vectorized? It is incorrect to use for loops explicitly. Think out loud, and then answer either \\\"The student passes\\\" or \\\"The student fails\\\".\\n<A>\", llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")))\n                                                                    \n\nif __name__ == \"__main__\":\n    print(run_test(TestFastL2))\n",
  "fix_append_vs_extend.py": "from evaluator import *\n\nDESCRIPTION = \"This test checks is the model can figure out from context when it's right to use extend versus append.\"\n\nTAGS = ['explain', 'python']\n\nquestion = \"\"\"\nWhy won't this function work as expected? List exactly one reason.\n\n```\ndef make_op_with_history(config):\n    how = config[0]\n    args = config[1:]\n\n    if how.split()[0] == 'dot_from_papers':\n        return [(how, make_op(config))]\n    if how.split()[0] in ['const_add', 'const_mul']:\n        return [(how, make_op(config))] + make_op_with_history(config[1])\n    out = []\n    for arg in args:\n        hist = make_op_with_history(arg)\n        out.append(hist)\n    return [(how, make_op(config))] + out\n```\n\"\"\"\n\njudge = \"I asked a question about why some code was incorrect. The answer is \\\"because because the code uses append instead of extend\\\". Does this student answer say that? Giving multiple answers is okay as long as it also says somewhere that the code should replace the append with extend.\\n\\n<A>\\n\\nAnswer either 'The student passes' or 'The student fails'.\"\n\nTestAppendNotExtend = question >> LLMRun() >> (LLMRun(judge, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestAppendNotExtend))\n\n\n",
  "fix_json.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can fix broken JSON objects.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"Write me a python function called fix_json that fixes broken JSON objects, and returns the parsed object. I'll pass the JSON object as a string and it should fix:\n- Incorect parenthesis, e.g., {\"a\": (5, 4)} -> {\"a\": [5, 4]}\n- Incorrect single quotes, e.g., {'a': 5} -> {\"a\": 5}\n- False/True -> false/true\n\"\"\"\n\nhelp_err = \"\"\"Make sure not to break data inside strings! So if there's a True or a ( inside a string keep them the way they are. Also don't break quoted strings\"\"\"\n\ntest_case, answer = make_python_test([(\"\"\"fix_json('''{\"a\": 5, \"b\": [3, 4]}''')\"\"\", \"\"\"{\"a\": 5, \"b\": [3, 4]}\"\"\"),\n                                      (\"\"\"fix_json('''{'a': 5.0, 'b': (True, 4)}''')\"\"\", \"\"\"{\"a\": 5.0, \"b\": [True, 4]}\"\"\"),\n                                      (\"\"\"fix_json('''{'True': False, 'b()': (3, 4), \"c'\": []}''')\"\"\", \"\"\"{\"True\": False, \"b()\": [3, 4], \"c'\": []}\"\"\"),\n                                      ])\n\nTestFixJSON = question >> LLMRun() >> ExtractCode(keep_main=False) >> PythonRun(test_case) >> SubstringEvaluator(answer)\nTestFixJSONHelp = (question+help_err) >> LLMRun() >> ExtractCode(keep_main=False) >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestFixJSONHelp))\n\n",
  "fix_node_error.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can identify a node error message\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nHow do I fix this\n\nnicholas@npc site % http-server \ndyld[63275]: Library not loaded: /opt/homebrew/opt/icu4c/lib/libicui18n.73.dylib\n  Referenced from: <758FD1B7-1836-321E-A1D9-E47EC3C39702> /opt/homebrew/Cellar/node/21.5.0/bin/node\n  Reason: tried: '/opt/homebrew/opt/icu4c/lib/libicui18n.73.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/icu4c/lib/libicui18n.73.dylib' (no such file), '/opt/homebrew/opt/icu4c/lib/libicui18n.73.dylib' (no such file), '/opt/homebrew/Cellar/icu4c/74.2/lib/libicui18n.73.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/Cellar/icu4c/74.2/lib/libicui18n.73.dylib' (no such file), '/opt/homebrew/Cellar/icu4c/74.2/lib/libicui18n.73.dylib' (no such file)\n\"\"\"\n\nTestFixNode = question >> LLMRun() >> SubstringEvaluator(\"brew reinstall node\")\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestFixNode))\n",
  "fix_threading_issue.py": "from evaluator import *\n\n\nDESCRIPTION = \"Test if the model can explain a poorly worded error message in a short threaded python program.\"\n\nTAGS = ['code', 'python', 'explain']\n\n\nquestion = \"\"\"In this program\n\nfrom multiprocessing import Pool\nfrom openai import OpenAI\n\ntxt = open(\"/tmp/aa.txt\").read().split(\"\\n\\n\")\n\nfrom multiprocessing import Pool\nimport subprocess\n\nclient = OpenAI(api_key=\"sk-XXXXXXXXXXXXXX\")\ndef create_speech_file(index, text):\n    response = client.audio.speech.create(\n        model=\"tts-1-hd\",\n        voice=\"nova\",\n\tinput=text\n    )\n    filename = f\"output{index}.mp3\"\n    response.stream_to_file(filename)\n    return filename\n\ndef merge_mp3(files, output):\n    with open(\"filelist.txt\", \"w\") as file:\n\tfor f in files:\n            file.write(f\"file '{f}'\\n\")\n\n    cmd = [\"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\", \"-i\", \"filelist.txt\", \"-c\", \"copy\", output]\n    subprocess.run(cmd)\n\nif __name__ == '__main__':\n    # Step 1: Parallelize speech file creation                                                                                                                            \n    with Pool(8) as pool:\n\tmp3_files = pool.starmap(create_speech_file, enumerate(txt))\n\n    # Step 2: Merge the generated MP3 files                                                                                                                               \n    output_file = \"merged_output.mp3\"\n    merge_mp3(mp3_files, output_file)\n\nWhy am I getting this error?\n\nException in thread Thread-3:\nTraceback (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n    self.run()\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\", line 917, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 576, in _handle_results\n    task = get()\n  File \"/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 251, in recv\n    return _ForkingPickler.loads(buf.getbuffer())\nTypeError: __init__() missing 2 required keyword-only arguments: 'response' and 'body'\n\n\"\"\"\n\nTestQuestionThreadedFix = question >> LLMRun() >> LLMRun(\"I showed a student some code with a bug. I then tested their understanding by asking them to explain hwo to fix the bug. I'm going to show you their answer, I want you to evaluate whether or not their answer says something to the effect of \\\"The function create_speech_file is raising an exception that can not be pickled.\\\". Do not judge their reasonining, or if they know how to fix it, just tell me if they correctly identified that there is a crash in the create_speech_file function. \\n Student Answer: \\\"<A>\\\"\\n\\n Think out loud then answer either \\\"The student passes\\\" or \\\"The student fails\\\".\", llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestQuestionThreadedFix))\n",
  "fix_tokenizer.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can identify and fix an issue with a tokenizer in a Python code snippet. Identifying the problem is in the regex, and fixing the regex, are both hard.\"\n\nTAGS = ['code', 'fix', 'python']\n\nquestion = \"\"\"\nFix this tokenizer keeping it as similar as possible\n\nclass Tokenizer:\n    def __init__(self, input_str):\n        self.tokens = re.findall(r'\\d+|(\\+|\\-|\\*|/|sin|cos|\\(|\\))', input_str)\n        self.position = 0\n\n    def get_next_token(self):\n        if self.position >= len(self.tokens):\n            return None\n        token = self.tokens[self.position]\n        self.position += 1\n        return token\n\n    def peek_next_token(self):\n        if self.position >= len(self.tokens):\n            return None\n        return self.tokens[self.position]\n\"\"\"\n\ntest_case, answer = make_python_test([(\"Tokenizer('sin(3+2*4)-cos(15)').tokens\", \"['sin', '(', '3', '+', '2', '*', '4', ')', '-', 'cos', '(', '15', ')']\")])\n\n\nTestSimpleFix = question >> LLMRun() >> ExtractCode() >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestSimpleFix))\n\n\n",
  "fix_torch_backward.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can fix and explain a bug in PyTorch code related to forgetting to zero gradients.\"\n\nTAGS = ['code', 'python', 'fix']\n\ncode = \"\"\"\n```\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.fc(x)\n\ndef PGD(model, input, target, loss_fn, epsilon, alpha, num_iter):\n    perturbation = torch.zeros_like(input, requires_grad=True)\n\n    for i in range(num_iter):\n        output = model(input + perturbation)\n        loss = loss_fn(output, target)\n\n        loss.backward()\n\n        # Update perturbation\n        perturbation_grad = perturbation.grad.data\n        perturbation.data = perturbation.data + alpha * perturbation_grad.sign()\n        perturbation.data = torch.clamp(perturbation.data, -epsilon, epsilon)\n        perturbation.grad.data.zero_()\n\n    return input + perturbation\n\nmodel = SimpleNet()\n\ninput = torch.tensor([[1.0]], requires_grad=True)\ntarget = torch.tensor([[2.0]])\n\nloss_fn = nn.MSELoss()\n\nepsilon = 0.1\nalpha = 0.01\nnum_iter = 2\n\nadversarial_input = PGD(model, input, target, loss_fn, epsilon, alpha, num_iter)\nprint(\"Resulting advex\", adversarial_input)\n```\n\"\"\"\n\nq1 = f\"Fix the bug in this code. Rewrite the entire code exactly as-is but just fix any bugs.\\n\\n{code}\"\n\nTestTorchBackwardFix = q1 >> LLMRun() >> ExtractCode(keep_main=True) >> PythonRun() >> (SubstringEvaluator(\"tensor\") & SubstringEvaluator(\"grad_fn=<Add\"))\n\nq2 = f\"Explain the bug in this code.\\n\\n{code}\"\n\nTestTorchBackwardExplain = q2 >> LLMRun() >> LLMRun(\"Below is a student's explanation for a bug in a torch function.\\n<A>\\nDoes the student's answer say specifically \\\"the bug is caused by not zeroing the gradient in the backward pass\\\"? Think out loud and then finally answer either \\\"The student passes\\\" or \\\"The student fails\\\".\", llm=EVAL_LLM) >> Echo() >> SubstringEvaluator(\"The student passes\")\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestTorchBackwardExplain))\n\n",
  "flexbox_webpage.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate an HTML file using flexbox\"\n\nTAGS = ['code', 'html']\n\nquestion = \"\"\"\nGenerate an HTML file that uses flexbox to create a webpage with a header 200px at the top (colored red), and the rest of the body split 100px for a left sidebar (in blue), 100px for a right sidebar (in green), and the rest of the page in the middle (in white).\n\"\"\"\n\ndef check(x):\n    import io\n    from PIL import Image\n    import numpy as np\n    img = Image.open(io.BytesIO(x))\n\n    np_img = np.array(img)[:,:,:3]\n    while np.mean(np_img[0]) >= 254:\n        np_img = np_img[1:]\n    while np.mean(np_img[:, 0]) >= 254:\n        np_img = np_img[:, 1:]\n\n    if np_img[0:200].std((0,1)).mean() > 20:\n        return False\n\n    if np_img[400:, :100].std((0,1)).mean() > 20:\n        return False\n\n    if np_img[400:, -100:-30].std((0,1)).mean() > 20:\n        return False\n\n    if np_img[0:200].mean((0,1)).argmax() != 0:\n        return False\n\n    if np_img[400:, :100].mean((0,1)).argmax() != 2:\n        return False\n\n    if np_img[400:, -100:].mean((0,1)).argmax() != 1:\n        return False\n    \n    \n    if np_img[:800, :100].std((0,1)).mean() < 20:\n        return False\n\n    if np_img[:800, -100:].std((0,1)).mean() < 20:\n        return False\n    \n    return True\n\n    \n\nTestFlexbox = question >> LLMRun() >> ExtractCode(keep_main=True, lang=\"html/javascript\") >> SeleniumDraw() >> PyFunc(check)\n\nif __name__ == \"__main__\":\n    print(run_test(TestFlexbox))\n\n",
  "freecad_construction.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model understands a rambling question about how to make construction circle in FreeCAD.\"\n\nTAGS = ['explain', 'fun']\n\nquestion = \"\"\"\nIn freecad I want to have a circle but only like for reference not to actually be a circle but just so I can line up other lines to make a hexagon for example, how do I do that\n\"\"\"\n\nTestFreeCADCircle = question >> LLMRun() >> (SubstringEvaluator(\"construction geometry\") | SubstringEvaluator(\"construction circle\"))\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestFreeCADCircle))\n",
  "generate_string_moves.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can write code to perform string slicing with vague instructions.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nWrite me a function called `move` that, given a string x, returns all possible alternate strings y' such that y' is derived from x by moving a substring of x to an arbitrary position\n\"\"\"\n\n\ntest_case, answer = make_python_test([(\"set(move('abcdef'))\", \"set(['abcdef', 'abcdfe', 'abcedf', 'abcefd', 'abcfde', 'abdcef', 'abdecf', 'abdefc', 'abecdf', 'abefcd', 'abfcde', 'acbdef', 'acdbef', 'acdebf', 'acdefb', 'adbcef', 'adebcf', 'adefbc', 'aebcdf', 'aefbcd', 'afbcde', 'bacdef', 'bcadef', 'bcdaef', 'bcdeaf', 'bcdefa', 'cabdef', 'cdabef', 'cdeabf', 'cdefab', 'dabcef', 'deabcf', 'defabc', 'eabcdf', 'efabcd', 'fabcde'])\")])\n\nTestProgramStringSlice = question >> LLMRun() >> ExtractCode() >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\nif __name__ == \"__main__\":\n    print(run_test(TestProgramStringSlice))\n\n",
  "gitignore_anywhere.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can understand and interpret a request to gitignore any file called \\\"foo/.KEYFILE\\\"  regardless of its location in a repository.\"\n\nTAGS = ['explain']\nquestion2 = \"\"\"\ngitignore any file called .KEYFILE anywhere under a directory foo, which itself is anywhere\n\"\"\"\n\nTestWhatIsStarStarB = question2 >> LLMRun() >> SubstringEvaluator(\"**/foo/**/.KEYFILE\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhatIsStarStarB))\n",


  "gol_rle_decode.py": "from evaluator import *\n\nDESCRIPTION = \"This test case tests if the model can convert a Game of Life pattern represented in RLE format to a numpy array.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nConvert this game of life pattern\n\n26$13b2o$14bo$14bobo$15b2o14$31b2o$30b2o$32bo13$46b2o$45b2o$47bo13$61b\n2o$60b2o$62bo4$52b2o$52b2o$71bo$66b2ob3o$66bobobo$68bo$67b2o3$76b2o$\n60b3o12b2o10bobo$52b3o7bo14bo8bo2bo4bo$51bo3bo5bo10b2o11b2o5b2o$50bo5b\no15bobo8b2o3bo8b2o$51bo3bo11b2o6bo9b2o10b2o$52b3o8b2obo2bo2bo2bo10bo2b\no$52bo2bo7b2o2b2o6bo11bobo$53b3o16bobo$53b2obo15b2o$54bobo$55bo3$52b2o\n3b2o$52bobobobo$53b5o$54b3o$55bo7$54b2o$54b2o!\n\n\nto a numpy array. Call the function `rle_to_arr(string, h, w)`\nwhich should return a numpy array with .shape (h,w).\nFill the array with 1s, and empty with 0.\n\"\"\"\n\nrle = \"\"\"\n16b2o3b2o$2o$2o$19b2o$17b5o$21bo$17b3o$o16bo$o17b2o$bo16b2o$b2o$3bo$o\n17b2o3b2o$2b2o14bobobobo$11bobo5b5o$11b2o7b3o$2o10bo8bo$b2o$b2o2$2o$bo\n2$6bobo$2o3bo$bo3bo$o4bo2bo7bo$5b3o7b2o7bo$15bobo5b3o$22b5o$21b2o3b2o$\n22b5o$22b5o$14b2o6bo2bo$14bobo4bo3bo$11b2obobo4bo$11bobobo5bob2o$13bo\n8bo$4bobo6b2o$7bo5b3o50bo$7bo5b3o3b2o3b2o40b4o$4bo2bo5b3o3bo5bo41b4o$\n5b3o5b2o35bo16bo2bo$13bo6bo3bo24bobo15b4o4b2o$11bobobo5b3o15b2o7bob2o\n14b4o5b2o$11b2obobo21bobo6b2ob2o14bo8b2o$14bobo20bo6b3obob2o$14b2o12b\n2o7bo2bo2bo2bo2bobo$28b2o7bo6b2o4bo16bobo$38bobo26b2o$39b2o27bo4b2o$\n21b2o49bo2bo$21b2o28bobo18b3o$52b2o21b2o$52bo21bo$74b2o$59bo$62bo$59bo\n14bo$62bo$61bob3o8bo$59b4o2bo9b2o$64bo$52bo$51b2o$40b2o8b2o4b2o5bo$40b\n2o7b3o4b2o3bobo$50b2o4b2o2bobo11b2o$4bobo44b2o6bo2bo11b2o$7bo44bo7bobo\n$o6bo53bobo$o3bo2bo55bo$5b3o5$4b4o$3bo3bo$7bo$3bo2bo3$o2b3o2b2o$2b5o2b\no64b3o$9o67bo$9b3o63bo$o7bo2bo$9b2o3$4b2o$o3b3o$o5b2obo5b2o$bo4bo2bo5b\n2o$2o4b2obo$o3b3o$4b2o2$59b3o12b2o$61bo12b2o!\n\"\"\"\n\n\n\ntest_case, answer = make_python_test([(f\"(np.array(rle_to_arr('''{rle}''', 101, 77),dtype=np.uint32).flatten() * np.arange(77*101)).sum()\", \"1563304\")])\n\nTestRLEDecode = question >> LLMRun() >> ExtractCode() >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\nif __name__ == \"__main__\":\n    print(run_test(TestRLEDecode))\n",
  "hallucinate_reference.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model will hallucinate references that don't exist.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nFor each of the 6 reference in the following paper, extract the title, author, and year as a JSON array.\n\nWrite {1: {\"title\": \"title\", \"author\": \"author\", \"year\": \"year\"}, 1: ..., 6: ...}\n\n\nPaper:\n{paper_title} {paper_body} {references}\n\"\"\"\n\nevaluation = \"\"\"\nI asked the student for the references contained a paper that didn't have any references. The student answered \"<A>\"\n\nDoes the student's answer say something like \"There are no references\"? If so, the student passes, and write \"The student passes\".\n\nOtherwise, if the student lists *any* paper, write \"The student fails\".\n\"\"\"\n\nTestHallucinateReference = question >> LLMRun() >> LLMRun(evaluation, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestHallucinateReference))\n\n",
  "identify_uuencode.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can correctly identify a block of text is uuencoded.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nWhat encoding scheme does it look like this is encoded with?\n\nH,3DR-S<-\"C(X,C<U#0HT.3(V-@T*-C`X,C8-\"C(X,S8V#0HR,#<Q-0``\n\"\"\"\n\n\nTestIsUU = question >> LLMRun() >> SubstringEvaluator(\"uuencode\", lower=True)\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestIsUU))\n\n",
  "implement_assembly_interpreter.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can implement an interpreter for a new assembly language from a text description.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"Here is the description of a new assembly language:\n\n* 8 registers (R1, R2, R3, R4, R5, R6, R7, R8) that can hold integers.\n* 1 flag that can hold a boolean value (True or False).\n* 100 memory addresses (0-99) that can hold integers.\n* 1 instruction pointer that points to the current instruction being executed.\n\nEach instruction is of the form\nOP ARG1 ARG2 ...\nwhere ARGn can be either a register (e.g., R1) or a constant (e.g., 10).\n\nLabels are written with a lowercase word followed by colon.\n\nThe assembly language supports the following instructions:\n* SET Rx C: Assigns the value C to register Rx.\n* ADD Rx Ry Rz: Adds the values of Ry and Rz and stores the result in Rx.\n* (similarly for SUB, MUL, DIV, MOD)\n* EQ Rx Ry: Sets the flag to True if Rx and Ry are equal, False otherwise.\n* (similarly for NEQ, LT (Rx < Ry), LTE, GT, GTE)\n* INC/DEC Rx: Increments/Decrements the value of Rx by one.\n* JMP L: Jumps to label L unconditionally.\n* JT/JF (jump if true / jump if false) L: Jumps to label L if the is set or not set.\n* LOAD Rx M: Loads the value at memory address M into register Rx.\n* STORE Rx M: Stores the value of register Rx into memory address M.\n* HCF: Stops the program (with pizzazz)\n\nFor example here is a program to compute the first 20 square numbers (1, 4, 9, 16, 25, ...):\n\n    SET R1 0      // Counter for storing squares\n    SET R2 1      // Number to square\n\nloop:\n    MUL R3 R2 R2  // R3 = R2 * R2\n    STORE R3 R1   // Store R3 at address R1\n    INC R1        // Increment address\n    INC R2        // Increment number\n    SET R3 20\n    EQ R1 R3      // Check if 20 squares are found\n    JF loop       // If not 20 squares found yet, continue finding\n\nend:\n    HCF           // Stop program\n\n\nWrite me a python interpreter `evaluate(str)` that returns the resulting memory state after running the program. For example, `evaluate(program)` should return `[1, 4, 9, 16, 25, ...]` for the above program.\n\"\"\"\n\nprimes = \"\"\"\n\nSET R1 2          // Starting number to check for prime\n\nstart_find_primes:\n\n    JMP is_prime // Control will return after executing is_prime with R1 as input and R2 containing the result\n\nready_prime:\n    SET R7 1\n    EQ R2 R7        // Check if R2 is 1 (prime)\n    JF increment  // If not prime, skip storing and increment the number\n\n    // Store prime number in memory and increment count\n    STORE R1 R8   // Store prime number at address pointed by R8\n    INC R8        // Increment prime count\n\n    // Check if 100 primes are found\n    SET R7 100\n    EQ R8 R7\n    JF increment  // If not 100 primes found yet, continue finding\n\n    JMP end        // If 100 primes found, end program\n\nincrement:\n    INC R1         // Increment number to check for prime\n    JMP start_find_primes // Check next number\n\nis_prime:\n    SET R2 1       // Assume number is prime initially\n    SET R3 2       // Start divisor from 2\n\nstart_loop:        // Label to start the loop\n    // Check if we have exceeded the square root of R1\n    MUL R4 R3 R3   // R4 = R3 * R3\n    GT R4 R1       // Set flag if R4 > R1\n    JT is_prime_end        // If not exceeded, continue; else, end loop\n\n    MOD R6 R1 R3   // R6 = R1 % R3\n    SET R7 0\n    EQ R7 R6     // Check if R6 is 0\n    JT not_prime   // If yes, number is not prime\n\n    INC R3         // Increment divisor\n    JMP start_loop // Repeat loop\n\nnot_prime:\n    SET R2 0       // Set result to 0 (not prime)\n\nis_prime_end:\n    JMP ready_prime\n\nend:\n\"\"\"\n\ncode = \"\"\"\n    SET R1 0\n    SET R2 1\nloop:\n    MUL R3 R2 R2\n    STORE R3 R1\n    INC R1\n    INC R2\n    SET R3 20\n    EQ R1 R3\n    JF loop\n\"\"\"\n\ntest_case, answer = make_python_test([(f'evaluate(\"\"\"{code}\"\"\")[:10]', \"[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\"),\n                                      (f'evaluate(\"\"\"{primes}\"\"\")[:10]', \"[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\")\n                                      ])\n\nTestImplementAssembly = question >> LLMRun() >> ExtractCode(lang=\"python\") >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestImplementAssembly))\n\n    \n",
  "implement_assembly_interpreter_by_example.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can implement an interpreter for a new assembly language given an example.\"\n\nTAGS = ['code', 'python']\n\nprimes = \"\"\"\n\nSET R1 2          // Starting number to check for prime\n\nstart_find_primes:\n\n    JMP is_prime // Control will return after executing is_prime with R1 as input and R2 containing the result\n\nready_prime:\n    SET R7 1\n    EQ R2 R7        // Check if R2 is 1 (prime)\n    JF increment  // If not prime, skip storing and increment the number\n\n    // Store prime number in memory and increment count\n    STORE R1 R8   // Store prime number at address pointed by R8\n    INC R8        // Increment prime count\n\n    // Check if 100 primes are found\n    SET R7 100\n    EQ R8 R7\n    JF increment  // If not 100 primes found yet, continue finding\n\n    JMP end        // If 100 primes found, end program\n\nincrement:\n    INC R1         // Increment number to check for prime\n    JMP start_find_primes // Check next number\n\nis_prime:\n    SET R2 1       // Assume number is prime initially\n    SET R3 2       // Start divisor from 2\n\nstart_loop:        // Label to start the loop\n    // Check if we have exceeded the square root of R1\n    MUL R4 R3 R3   // R4 = R3 * R3\n    GT R4 R1       // Set flag if R4 > R1\n    JT is_prime_end        // If not exceeded, continue; else, end loop\n\n    MOD R6 R1 R3   // R6 = R1 % R3\n    SET R7 0\n    EQ R7 R6     // Check if R6 is 0\n    JT not_prime   // If yes, number is not prime\n\n    INC R3         // Increment divisor\n    JMP start_loop // Repeat loop\n\nnot_prime:\n    SET R2 0       // Set result to 0 (not prime)\n\nis_prime_end:\n    JMP ready_prime\n\nend:\n\"\"\"\n\ncode = \"\"\"\n    SET R1 0\n    SET R2 1\nloop:\n    MUL R3 R2 R2\n    STORE R3 R1\n    INC R1\n    INC R2\n    SET R3 20\n    EQ R1 R3\n    JF loop\n\"\"\"\n\nquestion = f\"\"\"Here is an example program from a new assmebly language I made up that computes primes:\n\n```\n{primes}\n```\n\nite me a python interpreter `evaluate(str)` that returns the resulting memory state after running the program. For example, `evaluate(program)` should return `[2, 3, 5, 7, ...]` for the above program.\n\"\"\"\n\n\ntest_case, answer = make_python_test([(f'evaluate(\"\"\"{code}\"\"\")[:10]', \"[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\"),\n                                      (f'evaluate(\"\"\"{primes}\"\"\")[:10]', \"[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\")\n                                      ])\n\nTestImplementAssemblyByExample = question >> LLMRun() >> ExtractCode(lang=\"python\") >> Echo() >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestImplementAssemblyByExample))\n\n    \n",
  "implement_crc32.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model understands the CRC-32 spec well enough to implement it.\"\n\nTAGS = ['code', 'c']\nimport zlib\n\nquestion = 'Write a c program that calculates the crc32 hash of the string \"correct horse battery staple\" and prints it to stdout. Dont call any builtin crc32 functions.'\n\nanswer = \"%x\"%zlib.crc32(b'correct horse battery staple')\n\nTestCRC32 = question >> LLMRun() >> ExtractCode(keep_main=True) >> Echo() >> CRun() >> SubstringEvaluator(answer, lower=True)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestCRC32))\n\n",
  "jax_onehot.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can correctly convert a list of indexes to a one-hot vector in Python using JAX.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nI have list of indexes and I want to convert it to one hot vector using jax and the function should be jittable and the function should be jitted. name the function one_hot and it should get two arguments the first one is the indexes and the second one is the number of possible labeles. Just give me the code\n\"\"\"\n\ntest_case, answer = make_python_test([(\"str(one_hot(jnp.array([1,2,0]),5))\",\"str(jnp.array([[0.0,1.0,0.0,0.0,0.0],[0.0,0.0,1.0,0.0,0.0],[1.0,0.0,0.0,0.0,0.0]]))\")],header='import jax.numpy as jnp')\n\n\n\n\nTestJaxOneHot = question >> LLMRun() >> ExtractCode() >> Echo() >>  PythonRun(test_case) >>  Echo() >> SubstringEvaluator(answer)\n                                                                    \n\nif __name__ == \"__main__\":\n    print(run_test(TestJaxOneHot))\n",
  "jnp_nn_bugfix.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can identify and fix a bug in a given jax.numpy function.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nFix the bug in this jnp function. Write out the full correct version of the code.\n\nimport jax\nimport jax.numpy as jnp\n\ndef init_mlp_params(input_size, hidden_size, output_size, key):\n    k1, k2 = jax.random.split(key)\n    # Initialize weights and biases for the first linear layer\n    w1 = jax.random.normal(k1, (hidden_size, input_size)) * jnp.sqrt(2.0 / input_size)\n    b1 = jnp.zeros(hidden_size)\n    # Initialize weights and biases for the second linear layer\n    w2 = jax.random.normal(k2, (output_size, hidden_size)) * jnp.sqrt(2.0 / hidden_size)\n    b2 = jnp.zeros(output_size)\n    return {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}\n\ndef mlp_apply(params, x):\n    # First linear layer\n    x = jnp.dot(x, params['w1'].T) + params['b1']\n    # Relu activation\n    x = jax.nn.relu(x)\n    # Second linear layer\n    x = jnp.dot(x, params['w2'].T) + params['b2']\n    # Softmax activation\n    return jax.nn.softmax(x, axis=1)\n\n# Example usage:\nkey = jax.random.PRNGKey(0)\nparams = init_mlp_params(10, 20, 30, key)\nx = jax.random.normal(key, (5, 10)) # random input similar to torch.randn\ny = mlp_apply(params, x)\nprint(list(y.flatten().sum(1)))  # The output sum should be very close to [1, 1, 1, 1, 1]\n\"\"\"\n\nTestFixJnpBug = question >> LLMRun() >> ExtractCode(keep_main=True) >> \\\n    (PythonRun() >> (SubstringEvaluator(\"1.0,\") | SubstringEvaluator(\"1.00000\") | SubstringEvaluator(\"1.0 \") | SubstringEvaluator(\"0.99999\")))\n\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestFixJnpBug))\n",
  "knowledge_llama.py": "from evaluator import *\n\nDESCRIPTION = \"Test the knowledge cutoff of the model to see if it knows the LLAMA-2 hidden dimension size.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nWhat is the hidden dimension of LLAMA-2 70b?\n\"\"\"\n\nTestLlamaKnowledge = question >> LLMRun() >> (SubstringEvaluator(\"8192\") | SubstringEvaluator(\"8,192\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestLlamaKnowledge))\n",
  "latex_mini_caps.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model knows some latex macros explained poorly\"\n\nTAGS = ['explain']\n\nquestion = \"Latex format the word METHOD like Method but all caps with the other letters lower\"\n\nTextLatexLowerCaps = question >> LLMRun() >> SubstringEvaluator(\"textsc\")\n\nif __name__ == \"__main__\":\n    print(run_test(TextLatexLowerCaps))\n\n\n",
  "latex_protect.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can fix a latex newline error in a caption\"\n\nTAGS = ['explain']\n\nquestion = r\"\"\"\nIn latex when I force a newline in a caption with \\\\  it gives an error saying \\Hy@tempa has an extra } runaway argument?\n\"\"\"\n\n# \\newline doesn't work the same I need \\\\\nTestLatexNewline = question >> LLMRun() >> SubstringEvaluator(\"\\\\protect\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestLatexNewline))\n",
  "latex_redef.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can use latex \\\\renewcommand, and do a bit more than what I actually asked.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nRedfine \\\\paragraph to be \\\\bigskip\\\\textbf\n\"\"\"\n\nTestLatexRedef = question >> LLMRun() >> ((SubstringEvaluator(\"\\\\renewcommand{\\\\paragraph}\") | SubstringEvaluator(\"\\\\renewcommand\\\\paragraph\")) & SubstringEvaluator(\"\\\\bigskip\") & SubstringEvaluator(\"\\\\textbf\") & SubstringEvaluator(\"\\\\noindent\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestLatexRedef))\n",
  "make_json.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can successfully convert unstructured data to JSON.\"\n\nTAGS = ['data']\n\nquestion = \"\"\"\nCreate a JSON metadata for these models:\n\nMistral-7B-v0.1\nRedPajama-INCITE-7B-Base\nRedPajama-INCITE-Base-3B-v1\nfalcon40b\nfalcon7b\ngpt2-xl\nllama-65b\nllama-7b\nneo-1.3\nneo-2.7\nneo-6\nopen_llama_3b_v2\nopen_llama_7b_v2\nopt-1.3b\nopt-6.7b\npythia-1.4\npythia-1.4-dedup\npythia-6.9\npythia-6.9-dedup\n\nWith the format:\n\n{\"Mistral-7B-v0.1\": {\"size\": 7, dataset: \"\", \"family\": \"Mistral\"}, ...}\n\nwhere family is one of \n\n    base = [\n        'pythia',\n        'llama',\n        'Mistral',\n        'gpt2',\n        'opt',\n        'RedPajama',\n        'neo',\n        'open_llama',\n        'falcon'\n    ]\n\ngpt2-xl is 1.5b parameters.\n\n\"\"\"\n\n\nTestMakeJson = question >> LLMRun() >> ExtractJSON() >> JSONSubsetEvaluator({\n  \"Mistral-7B-v0.1\": {\"size\": 7, \"dataset\": \"\", \"family\": \"Mistral\"},\n  \"RedPajama-INCITE-7B-Base\": {\"size\": 7, \"dataset\": \"\", \"family\": \"RedPajama\"},\n  \"RedPajama-INCITE-Base-3B-v1\": {\"size\": 3, \"dataset\": \"\", \"family\": \"RedPajama\"},\n  \"falcon40b\": {\"size\": 40, \"dataset\": \"\", \"family\": \"falcon\"},\n  \"falcon7b\": {\"size\": 7, \"dataset\": \"\", \"family\": \"falcon\"},\n  \"gpt2-xl\": {\"size\": 1.5, \"dataset\": \"\", \"family\": \"gpt2\"},\n  \"llama-65b\": {\"size\": 65, \"dataset\": \"\", \"family\": \"llama\"},\n  \"llama-7b\": {\"size\": 7, \"dataset\": \"\", \"family\": \"llama\"},\n  \"neo-1.3\": {\"size\": 1.3, \"dataset\": \"\", \"family\": \"neo\"},\n  \"neo-2.7\": {\"size\": 2.7, \"dataset\": \"\", \"family\": \"neo\"},\n  \"neo-6\": {\"size\": 6, \"dataset\": \"\", \"family\": \"neo\"},\n  \"open_llama_3b_v2\": {\"size\": 3, \"dataset\": \"\", \"family\": \"open_llama\"},\n  \"open_llama_7b_v2\": {\"size\": 7, \"dataset\": \"\", \"family\": \"open_llama\"},\n  \"opt-1.3b\": {\"size\": 1.3, \"dataset\": \"\", \"family\": \"opt\"},\n  \"opt-6.7b\": {\"size\": 6.7, \"dataset\": \"\", \"family\": \"opt\"},\n  \"pythia-1.4\": {\"size\": 1.4, \"dataset\": \"\", \"family\": \"pythia\"},\n  \"pythia-1.4-dedup\": {\"size\": 1.4, \"dataset\": \"\", \"family\": \"pythia\"},\n  \"pythia-6.9\": {\"size\": 6.9, \"dataset\": \"\", \"family\": \"pythia\"},\n  \"pythia-6.9-dedup\": {\"size\": 6.9, \"dataset\": \"\", \"family\": \"pythia\"}\n})\n\nif __name__ == \"__main__\":\n    print(run_test(TestMakeJson))\n",
  "make_sqlite_table.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate a SQL query to create a database table.\"\n\nTAGS = ['sql']\n\nquestion = \"\"\"\nI'm working with a sqlite3 database. Write a query that creates a new database with a table for peoples name, job, and age. Then add a researcher named Nicholas who is 5. Write this directly as a sql query that I can pipe into sqlite3. Do not give the 'sqlite' command, I'll just do `sqlite3 database.db < [your output]`.\n\"\"\"\n\ndef myfn():\n    open(\"/tmp/query.sql\", \"w\").write(\"SELECT * FROM people;\")\n    import os\n    out = os.popen(\"sqlite3 -init /tmp/query.sql database.db .exit\").read()\n    return \"Nicholas\" in out and \"5\" in out and \"research\" in out.lower()\n\n\nTestSqlMakeTable = question >> LLMRun() >> ExtractCode(manual=\"I'm going to run `sqlite3 database.db < /tmp/query.sql`. Given the following answer tell me exactly what to put into `query.sql`. DO NOT MODIFY THE CODE OR WRITE NEW CODE.\\n<A>\") >> Echo() >> SQLRun() >> PyEvaluator(myfn)\n                                                                    \n\nif __name__ == \"__main__\":\n    print(run_test(TestSqlMakeTable))\n",
  "make_tree_from_text.py": "from evaluator import *\n\n\nDESCRIPTION = \"Test if the model can create a tree from a string.\"\n\nTAGS = ['code', 'python']\n\ncode = \"\"\"\nclass Node:\n    def __init__(self, name, parent=None):\n        self.name = name\n        self.parent = parent\n        self.children = []\n        if parent:\n            self.parent.children.append(self)\n    def __repr__(self):\n        if self.children == []:\n            return self.name\n        return '('+self.name + ' ' + ' '.join(repr(c) for c in self.children)+')'\n\"\"\"\n\n\nquestion2 = code + \"\"\"\n\nWrite a complete python file that will print the tree\n\n(1 (2 (3 4 5 6) (7 8)) (9 10 (11 12)))\n\nCall the root of the tree 'root'.\nThen print out str(root).\n\"\"\"\n\nTestMakeTree = question2 >> LLMRun() >> ExtractCode(keep_main=True) >> PythonRun() >> SubstringEvaluator('(1 (2 (3 4 5 6) (7 8)) (9 10 (11 12)))')\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestMakeTreeEasy))\n",

  "numba_levenshtein.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate a numba implementation of the Levenshtein distance algorithm.\"\n\nTAGS = ['code', 'python', 'performance']\n\nquestion = \"\"\"\nWrite me a numba implementation of levensthin distance, taking two numpy int arrays. Call it levenshtein. Run it in nopython mode. Character swaps are cost 2.\n\"\"\"\n\ntest_case, answer = make_python_test([(\"levenshtein(np.array([3, 5, 2, 4, 8, 7]), np.array([3, 4, 2, 4, 8, 7, 9]))\", \"3\")])\n\n\nTestProgramNumbaLev = question >> LLMRun() >> ExtractCode(keep_main=False) >> (((SubstringEvaluator(\"from numba\") | SubstringEvaluator(\"import numba\")) & (SubstringEvaluator(\"nopython\") | SubstringEvaluator(\"njit\"))) & PythonRun(test_case) >> SubstringEvaluator(answer))\n\nif __name__ == \"__main__\":\n    print(run_test(TestProgramNumbaLev))\n",
  "numba_rref.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can rewrite a fairly complex Python function to Numba.\"\n\nTAGS = ['code', 'python', 'performance']\n\nquestion = \"\"\"\nRewrite this function to use numba and nopython mode. You can assume that the input is a 2d matrix correctly formatted. Call it rref still.\n\ndef rref(matrix, graphlen):\n    PH = 16\n\n    extra = 0\n    col = 0\n    pivots = []\n\n    used_for_row = [set([i]) for i in range(matrix.shape[0])]\n    \n    matrix = matrix%PH\n    while col+extra < matrix.shape[1]-1 and col < matrix.shape[0]:\n\n        if matrix[col,col+extra] == 0:\n            if np.all(matrix[:,col] == 0):\n                extra += 1\n                continue\n            other = np.argwhere(matrix[:,col+extra] != 0).flatten()[-1]\n            if other < col:\n                extra += 1\n                continue\n\n            matrix[col], matrix[other] = list(matrix[other]), list(matrix[col])\n            used_for_row[col], used_for_row[other] = used_for_row[other], used_for_row[col]\n\n        pivots.append(col+extra)\n        pivot = matrix[col,col+extra]\n        if col+extra < graphlen:\n            assert np.abs(pivot) == 1 or np.abs(pivot) == PH-1\n        else:\n            assert np.abs(pivot) == 2 or np.abs(pivot) == PH-2\n            pivot //= 2\n        matrix[col] *= pivot\n        matrix[col] %= PH\n\n        others = np.argwhere(matrix[:,col+extra]).flatten()\n\n        for i in others:\n            if i == col: continue\n            used_for_row[i] |= used_for_row[col]\n            if col < graphlen:\n                matrix[i] -= matrix[col]*matrix[i,col+extra]\n            else:\n                while matrix[i,col+extra] != 0:\n                    matrix[i] = (matrix[i]-matrix[col])%PH\n            matrix[i] %= PH\n                    \n        col += 1\n    \n    matrix = np.array(matrix)%PH\n    return [sorted(x) for x in used_for_row]\n\n\"\"\"\n\ntest_case, answer = make_python_test([(\"rref(arr, 37)\", \"[[0, 38], [0, 38], [0, 6, 28, 35, 38], [0, 6, 18, 21, 28, 35, 38], [0, 6, 28, 35, 38], [0, 6, 21, 28, 35, 38], [0, 5, 10, 23, 38], [0, 5, 10, 23, 38], [0, 5, 10, 23, 38], [0, 5, 9, 10, 23, 38], [0, 5, 9, 10, 13, 17, 23, 30, 38], [0, 5, 9, 10, 11, 23, 27, 38], [0, 5, 9, 10, 11, 23, 27, 38], [0, 5, 9, 10, 11, 23, 25, 27, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 24, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 5, 7, 8, 9, 10, 11, 12, 15, 16, 19, 23, 25, 26, 27, 34, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [0, 5, 9, 10, 13, 17, 23, 30, 38], [0, 5, 9, 10, 13, 17, 23, 30, 38], [0, 5, 9, 10, 13, 17, 23, 30, 32, 38], [0, 5, 9, 10, 13, 17, 23, 30, 32, 33, 38], [0, 22, 29, 38], [0, 22, 29, 38], [0, 6, 28, 35, 38], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21, 23, 25, 26, 27, 28, 31, 34, 35, 36, 37, 38], [14, 18, 21, 28, 35]]\")], header=\"arr = np.array([[1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -20], [0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0, -20], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 0, -30], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, -20], [1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -14], [0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, -30], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, 0, -20], [0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, 0, -10], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -13], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11], [0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -9], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, -20], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2, -40], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\")\n\n\nTestNumbaRref = question >> LLMRun() >> ExtractCode() >> (((SubstringEvaluator(\"from numba\") | SubstringEvaluator(\"import numba\")) & (SubstringEvaluator(\"nopython\") | SubstringEvaluator(\"njit\"))) & PythonRun(test_case) >> SubstringEvaluator(answer))\n                                                                    \n\nif __name__ == \"__main__\":\n    print(run_test(TestNumbaRref))\n",
  "numpy_advanced_index.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model correctly understands how advanced indexing works in numpy.\"\n\nTAGS = ['explain', 'python']\n\nquestion = \"\"\"\nWhat will this program print:\n\n```python\nimport numpy as np\n\narray = np.zeros((10, 20, 30))\nidxs = [1, 2, 3]\n\nprint(\"The array shape is\", array[0, :, idxs].shape)\n```\n\"\"\"\n\nTestNumpyAdvancedIndex = question >> LLMRun() >> SubstringEvaluator(\"The array shape is (3, 20)\")\n\nquestion = \"\"\"\nConsider this program\n\n```python\nimport numpy as np\n\narray = np.zeros((10, 20, 30))\nidxs = [1, 2, 3]\n\nprint(\"The first array shape is\", array[0][:, idxs].shape)\nprint(\"The second array shape is\", array[0, :, idxs].shape)\n```\n\nTell me what this program will print, and then explain why the two print statements don't print the same thing\n\"\"\"\n\nTestNumpyAdvancedIndexEasier = question >> LLMRun() >> (SubstringEvaluator(\"(3, 20)\") & SubstringEvaluator(\"(20, 3)\") & SubstringEvaluator(\"dvanced indexing\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestNumpyAdvancedIndexEasier))\n",
  "numpy_ix.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can identify the _ix function as a method for simplifying some code.\"\n\nTAGS = ['explain', 'python']\n\nquestion = '''I want to vectorize a function\n\n        frame = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n        for i, val1 in enumerate(rows):\n            for j, val2 in enumerate(cols):\n                for j, val3 in enumerate(ch):\n                    # Assuming you want to store the pair as tuples in the matrix\n                    frame[i, j, k] = image[val1, val2, val3]\n\n        out.write(np.array(frame))\n\nwith a simple numpy function that does something like this what is it called'''\n\nTestNumpyIx = question >> LLMRun() >> SubstringEvaluator(\"ix_\")\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestNumpyIx))\n\n",
  "print_hello.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate a basic python program that prints \\\"hello world\\\".\"\n\nTAGS = ['code', 'python']\n\nquestion = 'Write a python program that prints the string \"hello world\" and tell me how it works in a sentence'\n\nanswer = \"hello world\"\n\nTestPrintHello = question >> LLMRun() >> ExtractCode(keep_main=True) >> PythonRun() >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestPrintHello))\n\n",
  "print_hello_poly.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate a program that prints \\\"hello world\\\" when run either as a C or a python program.\"\n\nTAGS = ['code', 'python']\n\nquestion2 = '''\nWrite a single file that I can save as foo.py.rs that I can run as either a rust program or a python program, and so both will print \"hello world\". Specifically if I run `python foo.py.rs` or `rustc -o a.out foo.py.rs && ./a.out` I should see \"hello world\" printed to the console.\n\nTo help you out, here's a valid answer for how to do this in C:\n\n```\n#include <stdio.h>\n#define sdf b /*                                                                                                                 \nprint(\"hello world\")                                                                                                             \n\"\"\"                                                                                                                              \n#define wer b */\n\nint main() {\n  printf(\"hello world\\n\");\n}\n\n#define ert s \"\"\"\n```\n'''\n\nTestPrintHelloPoly2 = question2 >> LLMRun() >> ExtractCode(keep_main=True) >> ((PythonRun() >> SubstringEvaluator(answer)) & (RustRun() >> SubstringEvaluator(answer)))\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestPrintHelloPoly2))\n\n\n",
  "program_in_new_assembly.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can write a program in a new assembly language. This ability to learn a new language on-the-fly is important for many tasks.\"\n\nTAGS = ['code']\n\nclass AssemblyEmulator:\n    def __init__(self, instructions):\n        self.registers = {\"R1\": 0, \"R2\": 0, \"R3\": 0, \"R4\": 0, \"R5\": 0, \"R6\": 0, \"R7\": 0, \"R8\": 0}\n        self.memory = [0] * 100\n        self.instruction_pointer = 0\n        self.instructions = instructions.split(\"\\n\")\n        self.flag = False\n        print(instructions)\n\n    def run(self):\n\n        def lookup(register_or_const):\n            if register_or_const.startswith('R'):\n                return self.registers[register_or_const]\n            else:\n                return int(register_or_const)\n\n        bin_op = {\n            \"ADD\": lambda a, b: a + b,\n            \"SUB\": lambda a, b: a - b,\n            \"MUL\": lambda a, b: a * b,\n            \"DIV\": lambda a, b: a // b,\n            \"MOD\": lambda a, b: a % b,\n            }\n        cmp_op = {\n            \"EQ\": lambda a, b: a == b,\n            \"NEQ\": lambda a, b: a != b,\n            \"LT\": lambda a, b: a < b,\n            \"LTE\": lambda a, b: a <= b,\n            \"GT\": lambda a, b: a > b,\n            \"GTE\": lambda a, b: a >= b,\n            }\n            \n            \n        ctr = 0\n        while self.instruction_pointer < len(self.instructions):\n            ctr += 1\n            if ctr > 1e6:\n                raise Exception(\"Infinite loop detected\")\n\n            parts = self.instructions[self.instruction_pointer].split(\"//\")[0].replace(\",\",\"\").split()\n            if len(parts) == 0:\n                self.instruction_pointer += 1\n                continue\n\n            instruction, args = parts[0], parts[1:]\n\n            if instruction == \"SET\":\n                self.registers[args[0]] = lookup(args[1])\n            elif instruction in bin_op:\n                self.registers[args[0]] = bin_op[instruction](lookup(args[1]), lookup(args[2]))\n            elif instruction in cmp_op:\n                self.flag = cmp_op[instruction](lookup(args[0]), lookup(args[1]))\n            elif instruction == \"INC\":\n                self.registers[args[0]] += 1\n            elif instruction == \"DEC\":\n                self.registers[args[0]] -= 1\n            elif instruction == \"JT\" and self.flag:\n                self.instruction_pointer = self.find_label(args[0])\n                continue\n            elif instruction == \"JF\" and not self.flag:\n                self.instruction_pointer = self.find_label(args[0])\n                continue\n            elif instruction == \"JMP\":\n                self.instruction_pointer = self.find_label(args[0])\n                continue\n            elif instruction == \"LOAD\":\n                self.memory[lookup(args[1])] = lookup(args[0])\n            elif instruction == \"STORE\":\n                self.memory[lookup(args[1])] = lookup(args[0])\n            elif instruction == \"HCF\":\n                return\n\n            self.instruction_pointer += 1\n\n    def find_label(self, label):\n        return next(i for i, instruction in enumerate(self.instructions) if instruction.strip().startswith(label + ':'))\n\n    \nquestion = \"\"\"Here is the description of a new assembly language:\n\n* 8 registers (R1, R2, R3, R4, R5, R6, R7, R8) that can hold integers.\n* 1 flag that can hold a boolean value (True or False).\n* 100 memory addresses (0-99) that can hold integers.\n* 1 instruction pointer that points to the current instruction being executed.\n\nEach instruction is of the form\nOP ARG1 ARG2 ...\nwhere ARGn can be either a register (e.g., R1) or a constant (e.g., 10).\n\nLabels are written with a lowercase word followed by colon.\n\nThe assembly language supports the following instructions:\n* SET Rx C: Assigns the value C to register Rx.\n* ADD Rx Ry Rz: Adds the values of Ry and Rz and stores the result in Rx.\n* (similarly for SUB, MUL, DIV, MOD)\n* EQ Rx Ry: Sets the flag to True if Rx and Ry are equal, False otherwise.\n* (similarly for NEQ, LT (Rx < Ry), LTE, GT, GTE)\n* INC/DEC Rx: Increments/Decrements the value of Rx by one.\n* JMP L: Jumps to label L unconditionally.\n* JT/JF (jump if true / jump if false) L: Jumps to label L if the is set or not set.\n* LOAD Rx M: Loads the value at memory address M into register Rx.\n* STORE Rx M: Stores the value of register Rx into memory address M.\n* HCF: Stops the program (with pizzazz)\n\nGiven this assembly language, write a program that {}\"\"\"\n\ndef expect(answer):\n    def evaluate(code):\n        try:\n            emulator = AssemblyEmulator(code)\n            emulator.run()\n        except Exception as e:\n            print(\"EXCEPTION\", e)\n            return False, str(e)\n            \n        return answer(emulator.memory), \"\"\n    return evaluate\n\nTestNewAssemblySquareNumbers = question.format(\"puts the first 20 square (1, 4, 9...) numbers in memory cells 0..19\") >> LLMRun() >> ExtractCode(lang=\"a new assembly language I just made up\") >> PyFunc(expect(lambda m: m[:20] == [i*i for i in range(1,21)]))\n\nTestNewAssemblyPrimeNumbers = question.format(\"puts the first 20 prime numbers in memory cells 0..19\") >> LLMRun() >> ExtractCode(lang=\"a new assembly language I just made up\") >> PyFunc(expect(lambda m: m[:20] == [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]))\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestNewAssemblySquareNumbers))\n\n    \n",
  "program_pipes_cpp.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate a C++ program that defines dataflow DSL.\"\n\nTAGS = ['code', 'c']\n\nquestion = \"\"\"\nWrite a C++ class Const, Square, Print so that I can write, for example,\n\nCost(5) >> Square() >> Inc() >> Inc() >> Print()\n\nand the resulting python program will print 27 to the screen.\n\"\"\"\n\ntest_case = \"\"\"int main() {\nConst(2) >> Square() >> Inc() >> Square() >> Square() >> Print();\nreturn 0;\n}\"\"\"\n\n\nTestMakeShiftOpC = question >> LLMRun() >> ExtractCode() >> CppRun(test_case) >> SubstringEvaluator(\"625\")\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestMakeShiftOpC))\n",
  "program_pipes_python.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate a python program that defines dataflow DSL.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nWrite a python class Const, Square, Print so that I can write, for example,\n\nCost(5) >> Square() >> Inc() >> Inc() >> Print()\n\nand the resulting python program will print 27 to the screen.\n\"\"\"\n\ntest_case, answer = make_python_test([(\"Const(2) >> Square() >> Inc() >> Square() >> Square() >> Print()\", \"0\")])\n\nTestMakeShiftOp = question >> LLMRun() >> ExtractCode() >> PythonRun(test_case) >> SubstringEvaluator(\"625\")\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestMakeShiftOp))\n",
  "program_sqrt.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can implement a sqrt function.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nTell me how to compute sqrt() in python without using the sqrt() function, then give me a function to do this. I want the answer as precise as possible. Call the function my_sqrt()..\n\"\"\"\n\ntest_case, answer = make_python_test([(\"round(my_sqrt(12345), 1)\", \"111.1\")])\n\n\nTestProgramSqrt = question >> LLMRun() >> ExtractCode() >> ((LLMRun(\"Below is a student question who was supposed to implement square root from scratch. Did they cheat and just call sqrt directly? Think out loud, then finally answer either 'Yes they cheated' or 'No they did not cheat' <A>\", llm=EVAL_LLM) >> ~SubstringEvaluator(\"they cheated\")) & (PythonRun(test_case) >> SubstringEvaluator(answer)))\n    \n    \nif __name__ == \"__main__\":\n    print(run_test(TestProgramSqrt))\n\n",
  "python_chess_game_prefix.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can correctly call a python API for a moderately popular python library.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nIn python-chess I have a game = chess.pgn.read_game(pgn).\n\nHow do I print the PGN notation for all prefixes of this game?\n\nCall your function print_all_prefixes(game). Do not give example code for how to run it.\n\n\"\"\"\n\ntest_case = \"\"\"import io\nimport chess.pgn\nprint_all_prefixes(chess.pgn.read_game(io.StringIO('1. Nf3 Nf6 2. c4 g6 3. Nc3 Bg7 4. d4 O-O 5. Bf4 d5 6. Qb3 dxc4 7. Qxc4 c6 8. e4 Nbd7 9. Rd1 Nb6 10. Qc5 Bg4 11. Bg5 Na4 12. Qa3 Nxc3 13. bxc3 Nxe4 14. Bxe7 Qb6 15. Bc4 Nxc3')))\"\"\"\n\ndef check(txt):\n    return txt.count('1. Nf3 Nf6 2. c4 g6 3. Nc3 Bg7') > 10, \"\"\n    \n    \n\nTestPyChessPrefix = question >> LLMRun() >> ExtractCode() >> PythonRun(test_case) >> PyFunc(check)\n\nif __name__ == \"__main__\":\n    print(run_test(TestPyChessPrefix))\n",
  "python_jpeg.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can write a program that directly writes a jpeg file. This requires precise understanding of the jpeg spec.\"\n\nTAGS = ['code', 'python', 'visual']\n\nquestion = \"\"\"\nWrite a python program that, manually, draws a red triangle on a white background and saves it as a 200x200 gif file. Do not use any graphics libraries. Write the gif byte by byte. Output the gif file to stdout.\n\nI will evaluate your program by running `python your_soln.py > /tmp/a.gif` and then view the output file.\n\"\"\"\n\nTestFlagDraw = question >> LLMRun() >> ExtractCode(keep_main=True) >> PythonRun(out_bytes=True) >> LLMVisionRun(\"Describe the shapes in this image and their color\") >> (SubstringEvaluator(\"triangle\") & SubstringEvaluator(\"red\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestFlagDraw))\n",
  "python_to_c_loop_update.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can convert a python program to c, with a loop that makes it difficult.\"\n\nTAGS = ['code', 'python', 'c']\n\nquestion = \"\"\"\nConvert this to c\n\ndef foo(x):\n    sum = 0\n    for i in range(x):\n        x += i\n        sum += x\n    return sum\n\"\"\"\n\ntest_case, answer = make_c_test([(\"foo(10)\", \"265\")])\n\n\nTestPythonToCLoopUpdate = question >> LLMRun() >> ExtractCode() >> CRun(test_case) >> SubstringEvaluator(answer)\n                                                                    \nif __name__ == \"__main__\":\n    print(run_test(TestPythonToCLoopUpdate))\n",
  "python_traceback.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can identify the buf and fix a program that handles python tracebacks. Useful to know if the model can handle more advanced python libraries.\"\n\nTAGS = ['code', 'fix']\n\nquestion = \"\"\"\nFix this program. Make sure to keep the format \"var: value\".\n\nimport traceback\n\ndef foo():\n    y = 6\n    raise Exception(\"This is a test exception\")\n\ndef crashes():\n    x = 5\n    foo()\n\n\nif __name__ == \"__main__\":\n    try:\n        crashes()\n    except:\n        # extract stack frames                                                                                                                                                        \n        stack = traceback.extract_stack()\n        # print all locals in each stack frame\n        for frame in stack:\n            for k,v  in frame.locals:\n                print(k + \": \" + v)\n\"\"\"\n\nTestProgramTB = question >> LLMRun() >> ExtractCode(keep_main=True) >> PythonRun(\"\") >> (SubstringEvaluator(\"x: 5\") & SubstringEvaluator(\"y: 6\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestProgramTB))\n",

  "regex_remove_5_words.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can write a Python function with a straightforward regex.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nWrite a function called `match` in Python that uses a regex to find any word repeated 5 times or more. Return that word. If no word is repeated 5 times or more, return an empty string.\n\"\"\"\n\ntest_case, answer = make_python_test([(\"match('hi hi hi hi hi')\", '\"hi\"'),\n                                      (\"match('hi hi hi hi')\", '\"\"'),\n                                      (\"match('hello this is is is is is a test')\", '\"is\"')])\n\n\nTestRegex = question >> LLMRun() >> ExtractCode(keep_main=False) >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\nif __name__ == \"__main__\":\n    print(run_test(TestRegex))\n",
  "rewrite_mac_crypto.py": "from evaluator import *\n\nDESCRIPTION = \"Test the ability of the model to rewrite a simple c program so it will run on ubuntu, and keep bugs in place.\"\n\nTAGS = ['code', 'c']\n\nquestion = \"\"\"\nRewrite this c code so it will run on ubuntu. DO NOT FIX ANY BUGS.\n\n```c\n#include <stdio.h>\n#include <strings.h>\n#include <CommonCrypto/CommonDigest.h>\n\nunsigned char memory[1<<20];\nvoid* malloc_ptr = (void*)&memory;\n\nvoid* my_malloc(int size) {\n    void* ptr = malloc_ptr;\n    malloc_ptr += size;\n    return (void*)ptr;\n}\n\nvoid compute_md5(char *str, unsigned char *hex_result) {\n    unsigned char result[CC_MD5_DIGEST_LENGTH];\n    CC_MD5_CTX md5ctx;\n    CC_MD5_Init(&md5ctx);\n    CC_MD5_Update(&md5ctx, str, strlen(str));\n    CC_MD5_Final(result, &md5ctx);\n\n    for (int i = 0; i < CC_MD5_DIGEST_LENGTH; i++) {\n        sprintf((char*)(hex_result + (i * 2)), \"%02x\", result[i]);\n    }\n}\n\nint main(int argc, char** argv) {\n    char* msg = (char*)my_malloc(32);\n    char* result = (char*)my_malloc(33);\n    msg[0] = '\\0';\n\n    compute_md5(argv[1], (unsigned char*)result);\n    result[33] = '\\0';\n\n    strcat(msg, \"Your password is wrong, \");\n    strcat(msg, argv[2]);\n\n    if (strcmp(result, \"13bd041ac6e683f0c8ddba41eeed9e67\") == 0) {\n      puts(\"CORRECT!\");\n      return 0;\n    } else {\n      puts(msg);\n      return 1;\n    }\n}\n```\n\"\"\"\n\nTestProgramRewriteCCrypto = question >> LLMRun() >> ExtractCode(keep_main=True) >> \\\n    ((CRun(gccflags=\"-lcrypto\", argv=\"emacsisbetterthanvim a\") >> SubstringEvaluator(\"CORRECT!\")) &\n     (CRun(gccflags=\"-lcrypto\", argv=\"randompw a\") >> SubstringEvaluator(\"wrong\")) &\n     (CRun(gccflags=\"-lcrypto\", argv=f\"a {'a'*8}13bd041ac6e683f0c8ddba41eeed9e67\") >> SubstringEvaluator(\"CORRECT!\")))\n     \n\n\nif __name__ == \"__main__\":\n    from llm import llm\n    print(run_test(TestProgramRewriteCCrypto))\n\n",


  "save_expired_html.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model knows how to get the HTML for the entire webpage; not just the body.\"\n\nTAGS = ['explain', 'html']\n\nquestion = \"\"\"\nI have a webpage that's open in a chrome browser but the page no longer exists. What javascript should I type in the console to get a copy of the entire page html?\n\"\"\"\n\nTestRecoverExpiredPage = question >> LLMRun() >> SubstringEvaluator(\"outerHTML\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestRecoverExpiredPage))\n\n",
  "shorten_c_function.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can significantly shorten a repetitive C functions.\"\n\nTAGS = ['code', 'c']\n\nquestion = \"\"\"\nCode golf this function to be a lot shorter. You only get points if it's <300 bytes (not counting whitespace). Keep the name `stepper`.\n\n```\nvoid stepper(int xw) {\n  int Steps = 0;\n  for (int x = 0; x < xw; x++) {\n    switch (Steps) {\n      case 0:\n        digitalWrite(1, 0);\n        digitalWrite(2, 0);\n        digitalWrite(3, 0);\n        digitalWrite(4, 1);\n        break;\n      case 1:\n        digitalWrite(1, 0);\n        digitalWrite(2, 0);\n        digitalWrite(3, 1);\n        digitalWrite(4, 1);\n        break;\n      case 2:\n        digitalWrite(1, 0);\n        digitalWrite(2, 0);\n        digitalWrite(3, 1);\n        digitalWrite(4, 0);\n        break;\n      case 3:\n        digitalWrite(1, 0);\n        digitalWrite(2, 1);\n        digitalWrite(3, 1);\n        digitalWrite(4, 0);\n        break;\n      case 4:\n        digitalWrite(1, 0);\n        digitalWrite(2, 1);\n        digitalWrite(3, 0);\n        digitalWrite(4, 0);\n        break;\n      case 5:\n        digitalWrite(1, 1);\n        digitalWrite(2, 1);\n        digitalWrite(3, 0);\n        digitalWrite(4, 0);\n        break;\n      case 6:\n        digitalWrite(1, 1);\n        digitalWrite(2, 0);\n        digitalWrite(3, 0);\n        digitalWrite(4, 0);\n        break;\n      case 7:\n        digitalWrite(1, 1);\n        digitalWrite(2, 0);\n        digitalWrite(3, 0);\n        digitalWrite(4, 1);\n        break;\n      default:\n        digitalWrite(1, 0);\n        digitalWrite(2, 0);\n        digitalWrite(3, 0);\n        digitalWrite(4, 0);\n        break;\n    }\n    Steps = (Steps + 1) % 8;\n  }\n}\n```\n\"\"\"\n\ntest_case, answer = make_c_test([(\"(stepper(100),out)\", \"-1475775710\")], extra_methods=\"\"\"\n#include <stdio.h>\n\nint out = 0;\nint digitalWrite(int pin, int value) {\n  out *= 101;\n  out += pin;\n  out *= 97;\n  out += value;\n  return out;\n}\n\"\"\")\n\ndef short_enough(num):\n    return lambda code: len(code.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")) < num\n\nstep = \"\\nTo do this follow these steps: \\n1. start out by creating a table of the values that are being written to the pins.\\n2. Then find an expression for this.\\n3. Write the function as short as possible.\\n4. See if you can make it any shorter. Return this as the final answer and call it your final answer. Make sure to name it stepper still.\"\n\nTestShortenCHard = question.replace(\"<300\", \"<200\") >> LLMRun() >> ExtractCode() >> (PyFunc(short_enough(200)) & (CRun(test_case) >> SubstringEvaluator(answer)))\nTestShortenC = question >> LLMRun() >> ExtractCode() >> (PyFunc(short_enough(300)) & (CRun(test_case) >> SubstringEvaluator(answer)))\nTestShortenCStep = question + step >> LLMRun() >> ExtractCode() >> (PyFunc(short_enough(300)) & (CRun(test_case) >> SubstringEvaluator(answer)))\n\n\nif __name__ == \"__main__\":\n    from llm import llm\n    print(run_test(TestShortenCHard))\n\n",
  "shorten_c_function_hard.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can significantly shorten a repetitive C functions.\"\n\nTAGS = ['code', 'c']\n\nquestion = \"\"\"\nCode golf this function to be a lot shorter. You only get points if it's <300 bytes (not counting whitespace). Keep the function named `count`.\n\n```\n\nint count() {\n    int ROWS = 8;\n    int COLS = 8;\n    int count = 0;\n    for (int i = 0; i < 1<<16; i++) {\n        long long unsigned int array = ((i & 0xf) << 9) | ((i & 0xf0) << (9 + 8 - 4)) | ((i & 0xf00) << (9 + 8 - 4 + 8 - 4)) | ((i & 0xf000) << (9 + 8 - 4 + 8 - 4 + 8 - 4));\n        long long unsigned int init = array;\n\n    long long unsigned int temp = array;\n\n    for (int i = 1; i < ROWS - 1; i++) {\n        for (int j = 1; j < COLS - 1; j++) {\n            // Calculate bit positions\n            int pos = i * COLS + j;\n            int topPos = (i - 1) * COLS + j;\n            int leftPos = i * COLS + (j - 1);\n            int bottomPos = (i + 1) * COLS + j;\n            int rightPos = i * COLS + (j + 1);\n\n            // Get the values of the neighbors\n            int cell = (temp >> pos) & 1;\n            int top = (temp >> topPos) & 1;\n            int left = (temp >> leftPos) & 1;\n            int bottom = (temp >> bottomPos) & 1;\n            int right = (temp >> rightPos) & 1;\n\n            // Sum the values of the neighbors\n            int sum = top + left + bottom + right;\n\n            // Update the cell if all neighbors are 0 and the cell itself is 1\n            if (sum == 0 && cell == 1) {\n                array &= ~(1ULL << pos);\n            }\n        }\n    }\n\n        count += (init == array);\n    }\n    return count;\n}\n```\n\"\"\"\n\ntest_case, answer = make_c_test([(\"count()\", \"27488\")])\n\ndef short_enough(num):\n    return lambda code: len(code.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")) < num\n\nstep = \"\\nTo do this follow these steps: \\n1. Start out explaining what the function does in words.\\n2. Then find a few ways that you can make the function shorter given this explanation.\\n3. Write the function as short as possible.\\n4. See if you can make it any shorter. Return this as the final answer and call it your final answer. Make sure to name it `count` still.\"\n\nTestShortenC2Hard = question.replace(\"<300\", \"<200\") >> LLMRun() >> ExtractCode() >> (PyFunc(short_enough(200)) & (CRun(test_case) >> SubstringEvaluator(answer)))\nTestShortenC2 = question >> LLMRun() >> ExtractCode() >> (PyFunc(short_enough(300)) & (CRun(test_case) >> SubstringEvaluator(answer)))\nTestShortenC2Step = question + step >> LLMRun() >> ExtractCode() >> (PyFunc(short_enough(300)) & (CRun(test_case) >> SubstringEvaluator(answer)))\n\n\nif __name__ == \"__main__\":\n    from llm import llm\n    print(run_test(TestShortenC2Step))\n\n\n",
  "shorten_python_if_missing.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can shorten a line of python with an equal line.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nShorten this python code. Keep the name the same.\n```\ndef foo(arr, keys):\n    return [[x[y] if y in x else -5 for y in keys] for x in arr]\n```\n\"\"\"\n\ntest_case, answer = make_python_test([(\"foo([{1: 2, 3: 4, 5: 6}], [1, 2, 3, 4])\", \"[[2, -5, 4, -5]]\")])\n\nTestShortenPyGet = question >> LLMRun() >> (SubstringEvaluator(\".get\") & (ExtractCode() >> PythonRun(test_case) >> SubstringEvaluator(answer)))\n\n                                            \n\nif __name__ == \"__main__\":\n    print(run_test(TestShortenPyGet))\n\n",
  "simulate_torch_grad.py": "from evaluator import *\n\nDESCRIPTION = \"This test case checks if the model can predict what the gradient of a variable is in PyTorch.\"\n\nTAGS = ['explain', 'python']\n\nquestion = \"\"\"\nWhat will this function print\n\n```\ndef diff_round(x, decimals=1):\n    scale_factor = (10 ** decimals)\n    x = x * scale_factor\n    diff = (1 + 1e-2) * x - torch.floor(x)\n    x = x - diff + (torch.floor(x) + torch.where(diff >= 0.5, 1, 0))\n    x = x / scale_factor\n    return x\n\ng = torch.tensor([.99, 1.54, 1.9], dtype=torch.float32, requires_grad=True)\nloss = torch.sum(diff_round(g, 1))\nloss.backward()\nprint(g.grad.sum())\n```\n\"\"\"\n\nTestSimTorchGrad = question >> LLMRun() >> SubstringEvaluator(\"-0.03\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestSimTorchGrad))\n\n",
  "strided_trick.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model knows how to use the strided trick with numpy.\"\n\nTAGS = ['code', 'python', 'performance']\n\nquestion = \"\"\"\nWrite a function strides() that given an array of tokens T as numpy array, using strided trick, returns [T[0:50], T[10:60], T[20: 70] ... ]\n\"\"\"\n\ntest_case, answer = make_python_test([(\"strides(np.arange(100)).shape\", \"(6, 50)\"),\n                                     (\"strides(np.arange(100)).sum()\", \"14850\")])\n\nTestProgramStrided = question >> LLMRun() >> ExtractCode() >> ((PythonRun(test_case) >> (SubstringEvaluator(answer))) & SubstringEvaluator(\"as_strided\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestProgramStrided))\n",
  "tokenizer_vocab.py": "from evaluator import *\n\nDESCRIPTION = \"This test case is designed to check if the model can print out the tokens in a AutoTokenizer's vocabulary.\"\n\nTAGS = ['explain', 'python']\n\nquestion = \"\"\"\nHow do I get all tokens in a AutoTokenizer?\n\"\"\"\n\nTestGetVocab = question >> LLMRun() >> SubstringEvaluator(\"get_vocab\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestGetVocab))\n\n",
  "torch_to_jnp.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can convert a torch neural network to a jax numpy model.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nConvert the following PyTorch code to pure JAX. Do not use any other libraries.\n\n```python\nimport torch\n\nclass MLP(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n        self.linear2 = torch.nn.Linear(hidden_size, output_size)\n\n    def __call__(self, x):\n        x = self.linear1(x)\n        x = torch.relu(x)\n        x = self.linear2(x)\n        return torch.nn.functional.softmax(x, dim=1)\n\nmodel = MLP(10, 20, 30)\nx = torch.randn(5, 10)\ny = model(x)\nprint(list(y.flatten().numpy().sum(1)))\n```\n\"\"\"\n\nTestTorchJnp = question >> LLMRun() >> ExtractCode(keep_main=True) >> \\\n    ((~SubstringEvaluator(\"import torch\")) &\n     (PythonRun() >> (SubstringEvaluator(\"1.0,\") | SubstringEvaluator(\"1.00000\") | SubstringEvaluator(\"1.0 \") | SubstringEvaluator(\"0.99999\"))))\n\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestTorchJnp))\n",
  "unholy_matrix.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can solve a rather hard dynamic programming problem\"\n\nTAGS = ['code', 'c']\n\nquestion = \"A binary 8x8 matrix is 'unholy' if there are no isolated 1s. A 1 is isolated if there isn't another 1 in any of the 4 direct cardinal directions. Write a C program that counts the total number of unholy 8x8 matricies.\"\n\n\nstep = \"\"\"To solve this question:\n- Use bit arithmetic and an unsigned long to represent the matrix.\n- Write a function is_unholy that tests if a matrix is unholy by bit-shifting and masking.\n\nUse a dynamic program to count the number of unholy matricies.\n- Recurse on the number of rows the matrix has.\n- Enumerate all 2^16 next two rows. This is enough to completely isolate the prior solutions from the next.\n\nWrite out a plan for the program, and then implement the plan in C.\"\"\"\n\nanswer = \"1121231537486377866\"\n\nTestUnholyMatrix = question >> LLMRun() >> ExtractCode(keep_main=True) >> CRun() >> SubstringEvaluator(answer)\nTestUnholyMatrixStep = (question + step) >> LLMRun() >> ExtractCode(keep_main=True) >> CRun() >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestUnholyMatrix))\n\n\n\n",
  "unit_conversion_math.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can do basic math with some EE equations.\"\n\nTAGS = ['explain']\n\nquestion = \"I measure a voltage drop of 30mv over a 2.5 ohm resistor. How long will a 3Ah battery last? Write your ansser 'XXX hours'\"\n\nTestUnitConversion = question >> LLMRun() >> RegexEvaluator(\"250.{,10}hours\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestUnitConversion))\n\n\n",
  "upython_mqtt.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can write upython code with an obscure module.\"\n\nTAGS = ['python', 'code']\n\nquestion = \"\"\"\nWrite a python program that I can use to run on my raspberry pi pico w zero that will check for mqtt messages. Don't block, and blink the LED on every loop iteration so I know it's polling. When it gets a message then set the LED solid for a second.\n\"\"\"\n\n# crude testing because there isn't an actual pico w to run this on...\nTestUPythonMQTT = question >> LLMRun() >> (SubstringEvaluator(\"check_msg\") & SubstringEvaluator(\"MQTTClient\") & SubstringEvaluator(\"set_callback\") & SubstringEvaluator('WLAN') & SubstringEvaluator(\"STA_IF\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestUPythonMQTT))\n\n",
  "vague_loop_format.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can follow vague instructions for how to print IDs following an example.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nWrite a python program that for ids like\n\n2401_002\n\nFor months 1..12 and ids 1..3\n\nso like 24{month}_{id}\n\nprint all the ids\n\"\"\"\n\nevaluator = SubstringEvaluator(\"2401_001\")\nfor month in range(1, 13):\n    for id in range(1, 4):\n        evaluator &= SubstringEvaluator(f\"24{month:02}_{id:03}\")\n\n\nTestVagueLoopFormat = question >> LLMRun() >> ExtractCode(keep_main=True) >> PythonRun() >> evaluator\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestVagueLoopFormat))\n\n\n",
  "vague_sum_data.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can infer what data to sum and what to ignore by example with vague instructions.\"\n\nTAGS = ['code', 'python']\n\nquestion = '''\nHere's some data\nQUERY A361\nFound at: 160392127418\nNumber of times present: 4\n\n\nFound at: 150158692866\nNumber of times present: 3\n\n\nFound at: 122340445248\nNumber of times present: 17\n\n\nFound at: 137248651778\nNumber of times present: 84\n\n\nFound at: 131891981708\nNumber of times present: 66\n\n\nQUERY 32B57\nFound at: 194561247604\nNumber of times present: 4\n\n\nFound at: 82888490886\nNumber of times present: 3\n\n\nFound at: 115836535414\nNumber of times present: 19\n\n\nFound at: 79615924948\nNumber of times present: 80\n\nWrite a python function called `process` that will take data like this as an argument and return a dict where the keys are the like A361/32B57 and the values are how often its present in total.\n'''\n\ndata = \"QUERY A361\\\\nFound at: 160392127418\\\\nNumber of times present: 4\\\\n\\\\n\\\\nFound at: 150158692866\\\\nNumber of times present: 4\\\\n\\\\n\\\\nFound at: 122340445248\\\\nNumber of times present: 17\\\\n\\\\n\\\\nFound at: 137248651778\\\\nNumber of times present: 84\\\\n\\\\n\\\\nFound at: 131891981708\\\\nNumber of times present: 66\\\\n\\\\n\\\\nQUERY 32B57\\\\nFound at: 194561247604\\\\nNumber of times present: 4\\\\n\\\\n\\\\nFound at: 82888490886\\\\nNumber of times present: 3\\\\n\\\\n\\\\nFound at: 115836535414\\\\nNumber of times present: 19\\\\n\\\\n\\\\nFound at: 79615924948\\\\nNumber of times present: 81\\\\n\"\n\ntest_case, answer = make_python_test([(f\"process('{data}')\", \"{'A361': 175, '32B57': 107}\")])\n\nTestSumSomeData = question >> LLMRun() >> ExtractCode(keep_main=True) >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestSumSomeData))\n\n\n",
  "vectorize_small_update.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can replace a for loop with a vectorized version.\"\n\nTAGS = ['code', 'python', 'performance']\n\ndef update(high, highest_number, delta):\n    for i in range(len(high)):\n        if i != highest_number:\n            high[i] = min(high[i], high[highest_number] + delta[highest_number] - delta[i])\n    return high\n\nquestion = \"\"\"\nVectorize this update loop. Call the function `update`:\n\nimport numpy as np\n\ndef update(high, highest_number, delta):\n    for i in range(len(high)):\n        if i != highest_number:\n            high[i] = min(high[i], high[highest_number] + delta[highest_number] - delta[i])\n    return high\n\"\"\"\n\ntest_case, answer = make_python_test([(\"list(update(np.array([2, 5, 9, 1, 2, 0, 3, 8]), 4, np.array([1, 2, 1, 4, 3, 2, 1, 2])))\", \"[2, 3, 4, 1, 2, 0, 3, 3]\")], header=\"import numpy as np\")\n\nevaluation = \"\"\"\nBelow is a student's answer to a question about how to vectorize this loop:\n\ndef update(high, highest_number, delta):\n    for i in range(len(high)):\n        if i != highest_number:\n            high[i] = min(high[i], high[highest_number] + delta[highest_number] - delta[i])\n    return high\n\nHere is their code:\n\n<A>\n\nDid the student try to vectorize the loop? Don't tell me if they are correct or not. (I have tests for that separately which will catch any bugs.) Just tell me if they tried to vectorize the loop. End your answer with either the string \"The student passes\" if they tried to do a vectorized loop, or \"The student fails\" if they just wrote a for loop again.\n\"\"\"\n\nTestVectorizeSmall = question >> LLMRun() >> ExtractCode() >> ((PythonRun(test_case) >> SubstringEvaluator(answer)) & (LLMRun(evaluation) >> SubstringEvaluator(\"student passes\")))\n\nif __name__ == \"__main__\":\n    print(run_test(TestVectorizeSmall))\n",
  "webgl_triangle.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate an HTML file with WebGL code that draws an image.\"\n\nTAGS = ['code', 'visual', 'html']\n\nquestion2 = \"\"\"\nDraw an image of a house with a chimney using WebGL in a single HTML file. Make the canvas take the full page.\n\"\"\"\n\nTestDrawHouse = question2 >> LLMRun() >> ExtractCode(keep_main=True, lang=\"html/javascript\") >> (~SubstringEvaluator(\"THREE.\") & (SeleniumDraw() >> LLMVisionRun(\"This is a screenshot of a webpage that draws something with webgl. I'm trying to see if it drew what I wanted. Describe what is drawn in this image?\") >> LLMRun(\"Does this answer say that there is a picture of a house with a chimney \\\"<A>\\\"? Answer \\\"Yes it does\\\" or \\\"No it does not\\\".\", llm=EVAL_LLM) >> SubstringEvaluator(\"es it does\")))\n\nif __name__ == \"__main__\":\n    print(run_test(TestDrawHouse))\n",
  "what_is_automodel.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can interpret vague questions and will respond with the answer I want, not the answer that's easy to find.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nWhat do I do to fix AutoModel.from_pretrained to make it auto model with lm head\n\"\"\"\n\nTestWhatIsAutoModel = question >> LLMRun() >> SubstringEvaluator(\"AutoModelForCausalLM\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhatIsAutoModel))\n",
  "what_is_blockbyorb.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model knows what ERR_BLOCKED_BY_ORB means.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nWhat does \"ERR_BLOCKED_BY_ORB\" mean?\n\"\"\"\n\nTestWhatIsBlockByOrb = question >> LLMRun() >> SubstringEvaluator(\"opaque resource blocking\", lower=True)\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhatIsBlockByOrb))\n\n",
  "what_is_formatfloat.py": "from evaluator import *\n\nDESCRIPTION = \"This test case checks if models can format f strings with floats.\"\n\nTAGS = ['explain', 'python']\n\nquestion = \"\"\"\nPython format {sdf} to 3 decimal digits\n\"\"\"\n\nTestWhatIsFloatFormat = question >> LLMRun() >> SubstringEvaluator(\":.3f}\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhatIsFloatFormat))\n",
  "what_is_inv.py": "from evaluator import *\n\nDESCRIPTION = \"This test case is designed to check if the model can correctly identify the Python operator used for the tilde (~) symbol.\"\n\nTAGS = ['explain', 'python']\n\nquestion = \"\"\"\nIn python what __thing__ do I use for ~, kind of like how __add__ is for +?\n\"\"\"\n\nTestWhatIsInv = question >> LLMRun() >> (SubstringEvaluator(\"__inv__\") | SubstringEvaluator(\"__invert__\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhatIsInv))\n",
  "what_is_oraw.py": "from evaluator import *\n\nDESCRIPTION = \"This test case checks if the model knows lpr commands.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nI have a .ps file that I want to print from my mac. I'm trying to use lpr. But every time I print it all I get is the actual bytes just printed with like hundreds of pages and this is not what I want. I want the printer to just print the ps directly. What command should I use to make this work? \n\"\"\"\n\nTestWhatIsLPR = question >> LLMRun() >> (SubstringEvaluator(\"-o raw\") | SubstringEvaluator(\"-l \"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhatIsLPR))\n\n",
  "what_is_slice_stop.py": "from evaluator import *\n\nDESCRIPTION = \"This test case checks if the model can say how to properly get the end of a slice.\"\n\nTAGS = ['explain', 'python']\n\nquestion = \"\"\"\nHow do I get sice(0, 5).end?\n\"\"\"\n\nTestWhatIsSlice = question >> LLMRun() >> SubstringEvaluator(\".stop\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhatIsSlice))\n",
  "which_package_sbox.py": "from evaluator import *\n\nDESCRIPTION = \"This test case checks if the model knows what latex package to import for the Sbox environment to work.\"\n\nTAGS = ['explain']\n\nquestion = \"\"\"\nWhat latex package has the Sbox env?\n\"\"\"\n\nTestWhereIsSbox = question >> LLMRun() >> SubstringEvaluator(\"fancybox\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhereIsSbox))\n",
  "whisper_merge.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can implement some string logic given a fuzzy description.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nGiven a parse function like\n\ndef parse_transcript(content):\n    transcript_segments = []\n    pattern = r'\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\]  (.*)'\n    matches = re.findall(pattern, content)\n\n    for start_time, end_time, text in matches:\n        start_seconds = timedelta(hours=int(start_time[:2]), minutes=int(start_time[3:5]), seconds=int(start_time[6:8]), milliseconds=int(start_time[9:])).total_seconds()\n        end_seconds = timedelta(hours=int(end_time[:2]), minutes=int(end_time[3:5]), seconds=int(end_time[6:8]), milliseconds=int(end_time[9:])).total_seconds()\n        transcript_segments.append((start_seconds, end_seconds, text))\n\n    return transcript_segments\n\n\nfor data like\n\n[00:00:14.240 --> 00:00:14.420]   the\n[00:00:14.420 --> 00:00:14.860]   robust\n[00:00:14.860 --> 00:00:15.020]  ness\n[00:00:15.020 --> 00:00:15.140]   of\n[00:00:15.140 --> 00:00:15.500]   neural\n[00:00:15.500 --> 00:00:15.870]   networks\n[00:00:15.870 --> 00:00:16.200]  .\n[00:00:16.200 --> 00:00:16.410]   And\n[00:00:16.410 --> 00:00:16.700]   this\n[00:00:16.700 --> 00:00:16.840]   is\n[00:00:16.840 --> 00:00:17.200]   joint\n\n\nwrite a merge() function that (1) calls parse and (2) merges words into sentences that end in period. Start with the first word and end with the period. Return a list of strings.\n\"\"\"\n\ntest_case, answer = make_python_test([(\"\"\"merge(\"[00:00:14.240 --> 00:00:14.420]   the\\\\n[00:00:14.420 --> 00:00:14.860]   robust\\\\n[00:00:14.860 --> 00:00:15.020]  ness\\\\n[00:00:15.020 --> 00:00:15.140]   of\\\\n[00:00:15.140 --> 00:00:15.500]   neural\\\\n[00:00:15.500 --> 00:00:15.870]   networks\\\\n[00:00:15.870 --> 00:00:16.200]  .\\\\n[00:00:16.200 --> 00:00:16.410]   And\\\\n[00:00:16.410 --> 00:00:16.700]   this\\\\n[00:00:16.700 --> 00:00:16.840]   is\\\\n[00:00:16.840 --> 00:00:17.200]   joint\\\\n\")\"\"\", \"[' the robustness of neural networks.', ' And this is joint']\")], header=\"\"\"\ndef parse_transcript(content):\n    transcript_segments = []\n    pattern = r'\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\]  (.*)'\n    matches = re.findall(pattern, content)\n\n    for start_time, end_time, text in matches:\n        start_seconds = timedelta(hours=int(start_time[:2]), minutes=int(start_time[3:5]), seconds=int(start_time[6:8]), milliseconds=int(start_time[9:])).total_seconds()\n        end_seconds = timedelta(hours=int(end_time[:2]), minutes=int(end_time[3:5]), seconds=int(end_time[6:8]), milliseconds=int(end_time[9:])).total_seconds()\n        transcript_segments.append((start_seconds, end_seconds, text))\n\n    return transcript_segments\n\"\"\")\n\n\nTestWhisperMerge = question >> LLMRun() >> ExtractCode() >> PythonRun(test_case) >> SubstringEvaluator(answer)\n\nif __name__ == \"__main__\":\n    print(run_test(TestWhisperMerge))\n",
  "why_broken_flask_extra_brace.py": "from evaluator import *\n\nDESCRIPTION = \"This test checks is the model can figure out the user has put an accidental extra brace in the request body.\"\n\nTAGS = ['explain', 'python']\n\nquestion = \"\"\"\nWhy isn't this working\n\n```\n@app.route('/getsimilar', methods=['GET'])\ndef getsimilar():\n    print('args',request.args)\n    positive = request.args.get('positive', '')\n    positive = positive.split(\"##\")\n\n    negative = request.args.get('negative', '')\n    negative = negative.split(\"##\")\n\n    keep_yes = [i for i,x in enumerate(titles) if x in positive]\n    set1_yes = emb[keep_yes]\n\n    keep_no = [i for i,x in enumerate(titles) if x in negative]\n    set1_no = emb[keep_no]\n\n    print(\"OK\", len(set1_yes), len(set1_no))\n```\n\nI see\n\n```\nargs ImmutableMultiDict([('positive', 'Title: Towards Evaluating the Robustness of Neural Networks\\nAuthors: Nicholas Carlini; David Wagner'), ('negative', 'Title: On Evaluating Adversarial Robustness Authors: Nicholas Carlini; Anish Athalye; Nicolas Papernot; Wieland Brendel; Jonas Rauber; Dimitris Tsipras; Ian Goodfellow; Aleksander Madry; Alexey Kurakin}')])\nOK 1 0\n```\n\nWhy don't I see OK 1 1? Instead it just says OK 1 0. List exactly one reason explaining what's wrong and what one change would fix it.\n\"\"\"\n\njudge = \"I asked a question about why some code was incorrect. The answer is \\\"because the input contains an extra curly brace }\\\". Does this student answer say that?\\n\\n<A>\\n\\nAnswer either 'The student passes' or 'The student fails'.\"\n\nTestBrokenExtraBrace = question >> LLMRun() >> (LLMRun(judge, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\"))\n\nif __name__ == \"__main__\":\n    print(run_test(TestBrokenExtraBrace))\n\n\n",

  "bash_renamer.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can write a bash script that renames files with a specific pattern.\"\n\nTAGS = ['code', 'bash']\n\nquestion = \"\"\"\nWrite a bash script to rename all files of the form \"merged_*\" to \"finished_*\". I want to call it by running bash rename.sh /path/to/files.\n\"\"\"\n\ndef setup_fn():\n    import os\n    import time\n    os.mkdir(\"./foo\")\n    time.sleep(.5)\n \n    for i in range(10):\n        with open(\"./foo/merged_{}\".format(i), \"w\") as f:\n            f.write(\"a\"*i)\n        with open(\"./foo/{}\".format(i), \"w\") as f:\n            f.write(\"a\"*i)\n\n    time.sleep(.5)\n\ndef check_ok():\n    import os\n    return sorted(os.listdir(\"foo\")) == sorted([\"finished_{}\".format(i) for i in range(10)] + [str(i) for i in range(10)])\n    \n\n\nTestBashRenamer = Setup(setup_fn) >> question >> LLMRun() >> ExtractCode() >> BashRun(args=[\"foo/\"]) >> PyEvaluator(check_ok)\n\nif __name__ == \"__main__\":\n    print(run_test(TestBashRenamer))\n\n",
  "bash_list_files_by_size_mod_ten.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can provide the correct bash command to list files in a directory and sort them by the least significant digit of their size.\"\n\nTAGS = ['bash']\n\nquestion = 'In bash how do I list all files in foo/ but sort the files by the least significant digit of their size so 2820 comes first and then 281771 and finally 2279. I want just the file names e.g., file1 otherfile lastfile.'\n\n\ndef setup_fn():\n    sizes = [921, 714, 120, 637, 366, 662, 305, 403, 49, 158]\n    \n    import os\n    import time\n    os.mkdir(\"foo\")\n    time.sleep(.5)\n \n    for i, size in enumerate(sizes):\n        with open(\"foo/{}\".format(i), \"w\") as f:\n            f.write(\"a\"*size)\n\n    time.sleep(.5)\n\ndef fix_whitespace(x):\n    return \" \".join(x.split())\n\n\nTestBashListSize = Setup(setup_fn) >> question >> LLMRun() >> ExtractCode(keep_main=True, lang=\"bash\") >> BashRun() >> PyFunc(fix_whitespace) >> RegexEvaluator(\"2.*0.*5.*7.*1.*6.*4.*3.*9.*8\")\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestBashListSize))\n\n",
  "bash_find_dont_contain.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can implement (the negation of) a simple bash 1-liner searching for files that don't contain some text.\"\n\nTAGS = ['bash']\n\nquestion = \"In bash list all files in cwd that don't contain asdf in the contents (case-ignore)\"\n\n\ndef setup_fn():\n    open(\"hello\", \"w\").write(\"this is a test\")\n    open(\"there\", \"w\").write(\"test this ASDF\")\n    open(\"this\", \"w\").write(\"whatever thing\")\n    open(\"asdf\", \"w\").write(\"zzzthing\")\n    open(\"lower\", \"w\").write(\"test this asdf\")\n\n    \nTestBashFindDontContain = Setup(setup_fn) >> question >> LLMRun() >> ExtractCode(keep_main=True) >> BashRun() >> (SubstringEvaluator(\"hello\") & ~SubstringEvaluator(\"there\") & SubstringEvaluator(\"this\") & SubstringEvaluator(\"asdf\") & ~SubstringEvaluator(\"lower\"))\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestBashFindDontContain))\n\n",
  "bash_convert_not_overwrite.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can run an incremental update of a bash command without overwriting files that already exist\"\n\nTAGS = ['bash']\n\nquestion = 'Run find . -name \"*.pdf\" -exec python3 process.py {} txt/{}.txt \\;  but don\\'t rewrite files that already exist. Instead of .pdf.txt make it just .txt'\n\n\ndef setup_fn():\n    open(\"process.py\", \"w\").write(\"import sys\\nopen(sys.argv[2], 'w').write(open(sys.argv[1]).read())\\n\")\n    open(\"a.pdf\", \"w\").write(\"hello1\")\n    open(\"b.pdf\", \"w\").write(\"hello2\")\n    open(\"c.pdf\", \"w\").write(\"hello3\")\n    import os\n    os.mkdir(\"txt\")\n    open(\"txt/a.txt\", \"w\").write(\"done1\")\n    open(\"txt/c.txt\", \"w\").write(\"done3\")\n\n\ndef check_ok():\n    import os\n    return open(\"txt/a.txt\").read() == \"done1\" and open(\"txt/b.txt\").read() == \"hello2\" and open(\"txt/c.txt\").read() == \"done3\" and len(os.listdir(\"txt\")) == 3\n\n\n    \nTestBashIncrementalUpdate = Setup(setup_fn) >> question >> LLMRun() >> ExtractCode(keep_main=True) >> BashRun() >> PyEvaluator(check_ok)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestBashIncrementalUpdate))\n\n\n",
  "fix_with_patch.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate a .patch file to fix a bug in a given Python code.\"\n\nTAGS = ['code', 'fix', 'python']\n\ncode = \"\"\"import re\n\nclass Tokenizer:\n    def __init__(self, input_str):\n        '''\n        input_str: a string of digits and operators\n        '''\n        self.position = 0\n        self.tokens = re.findall(r'\\d+|(\\+|\\-|\\*|/|sin|cos|\\(|\\))', input_str)\n\n    def get_next_token(self):\n        if self.position >= len(self.tokens):\n            return None\n        token = self.tokens[self.position]\n        self.position += 1\n        return token\n\n    def peek_next_token(self):\n        if self.position >= len(self.tokens):\n            return None\n        return self.tokens[self.position]\n\"\"\"\n\nquestion = f\"\"\"\nWrite a .patch file to stdout that will fix the bug in this tokenizer. Still keep the new file name the same after patching.\n\nI'll evaluate you based on my ability to do\n- echo [your patch] > fix.patch\n- patch < fix.patch\n- python3 run_tokenizer.py\n\n\n\nHere is the entirety of run_tokenizer.py:\n```python\n{code}\n```\n\"\"\"\n\ndef setup_fn():\n    code = \"\"\"import re\n\nclass Tokenizer:\n    def __init__(self, input_str):\n        '''\n        input_str: a string of digits and operators\n        '''\n        self.position = 0\n        self.tokens = re.findall(r'\\d+|(\\+|\\-|\\*|/|sin|cos|\\(|\\))', input_str)\n\n    def get_next_token(self):\n        if self.position >= len(self.tokens):\n            return None\n        token = self.tokens[self.position]\n        self.position += 1\n        return token\n\n    def peek_next_token(self):\n        if self.position >= len(self.tokens):\n            return None\n        return self.tokens[self.position]\"\"\"\n    import time\n    open('run_tokenizer.py', 'w').write(code)\n\n    time.sleep(.5)\n\n\ndef check():\n    import os\n    import time\n    os.system(\"patch < fix.patch\")\n    time.sleep(.5)\n    import run_tokenizer\n    return run_tokenizer.Tokenizer(\"cos(1)+sin(23)*5\").tokens == ['cos', '(', '1', ')', '+', 'sin', '(', '23', ')', '*', '5']\n\n\nTestFixPatch = Setup(setup_fn) >> question >> LLMRun() >> ExtractCode(manual=\"Take the below answer to my question and return just the complete .patch in a single file so I can copy and paste it into an editor and apply it with the `patch` tool as-is. Include nothing else other than the patch. Here is the code: <A>\") >> MakeFile(\"fix.patch\") >> PyEvaluator(check)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestFixPatch))\n",
  "find_bug_in_paper.py": "from evaluator import *\n\nDESCRIPTION = \"Test if a model can find math errors in the latex source of a paper.\"\n\nTAGS = ['explain']\n\nquestion = r\"\"\"\n# Setup\n\nWhich of the following equations are incorrect according to the specification?\n\n# Notation\n\nA neural network is a function $F(x) = y$ that accepts an input $x \\in \\mathbb{R}^n$\nand produces an output $y \\in \\mathbb{R}^m$.\nThe model $F$ also implicitly depends on some model parameters $\\theta$; in our work\nthe model is fixed, so for convenience we don't show the dependence on $\\theta$.\n\nIn this paper we focus on neural networks used as an $m$-class classifier.\nThe output of the network is computed using the softmax function,\nwhich ensures that the output vector $y$ satisfies\n$0 \\le y_i \\le 1$ and $y_1 + \\dots + y_m = 1$.\nThe output vector $y$ is thus treated as a probability distribution, i.e.,\n$y_i$ is treated as the probability that input $x$ has class $i$.\nThe classifier assigns the label $C(x) = \\arg\\max_i F(x)_i$ to the input $x$.\nLet $C^*(x)$ be the correct label of $x$.\nThe inputs to the softmax function are called \\emph{logits}.\n\nWe use the notation from Papernot et al. \\cite{distillation}: define $F$ to\nbe the full neural network including the softmax function, $Z(x) = z$ to be the output of\nall layers except the softmax (so $z$ are the logits), and\n\\begin{equation*}\nF(x) = \\softmax(Z(x)) = y.\n\\end{equation*}\nA neural network typically \\footnote{Most simple networks have this simple\n  linear structure, however other more sophisticated networks have\n  more complicated structures (e.g., ResNet \\cite{he2016deep} and Inception \\cite{szegedy2015rethinking}).\n  The network architecture does not impact our attacks.}\nconsists of layers\n\\begin{equation*}\nF = \\softmax \\circ F_n \\circ F_{n-1} \\circ \\cdots \\circ F_1\n\\end{equation*}\nwhere\n\\begin{equation*}\nF_i(x)  = \\sigma(\\theta_i \\cdot x) + \\hat\\theta_i\n\\end{equation*}\nfor some non-linear activation function $\\sigma$, some matrix $\\theta_i$ of model\nweights, and some vector $\\hat\\theta_i$ of model biases. Together $\\theta$ and\n$\\hat\\theta$ make up the model parameters.\nCommon choices of $\\sigma$\nare tanh \\cite{mishkin2015all}, sigmoid, ReLU \\cite{maas2013rectifier}, or ELU \\cite{clevert2015fast}.\nIn this paper we focus primarily on networks that use a ReLU activation function,\nas it currently is the most widely used\nactivation function\n\\cite{szegedy2015rethinking,springenberg2014striving,mishkin2015all,distillation}.\n\n\n# Specification:\n\nWe define an objective function $f$ such that\n$C(x+\\delta) = t$ if and only if $f(x+\\delta) \\le 0$.\nThere are many possible choices for $f$:\n\n# The equations\n\n\\begin{align*}\nf_{1}(x') &= -\\loss_{F,t}(x') + 1\\\\\nf_{2}(x') &= (\\max_{i \\ne t}( F(x')_i) - F(x')_t)^+\\\\\nf_{3}(x') &= \\text{softplus}(\\max_{i \\ne t}(F(x')_i) - F(x')_t)-\\log(2)\\\\\nf_{4}(x') &= (0.5 - F(x')_t)^+\\\\\nf_{5}(x') &= -\\log(2 F(x')_t - 2)\\\\\nf_{6}(x') &= (\\max_{i \\ne t}( Z(x')_i) - Z(x')_t)^+\\\\\nf_{7}(x') &= \\text{softplus}(\\max_{i \\ne t}(Z(x')_i) - Z(x')_t)-\\log(2)\n\\end{align*}\nwhere $s$ is the correct classification, $(e)^+$ is short-hand for\n$\\max(e,0)$, $\\text{softplus}(x) = \\log(1+\\exp(x))$, and\n$\\loss_{F,s}(x)$ is the cross entropy loss for $x$.\n\n# Your task\n\nConsider each equation one by one. End your answer with a python list of numbers [1,2,3,4,5,6,7] for those that are wrong.\n\"\"\"\n\ndef check(x):\n    ints = list(map(int,x.split(\"[\")[-1].split(\"]\")[0].split(\",\")))\n    return ints == [1,3,4,5,7], \"\"\n\nTestFindBugPaper = question >> LLMRun() >> Echo() >> PyFunc(check)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestFindBugPaper))\n",
  "fancy_sql_process.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can generate a Python program that retrieves data from a SQL file.\"\n\nTAGS = ['sql']\n\nquestion = \"\"\"\nI have a sqlite3 database name TEXT, favorite_friend TEXT, age INT, profession TEXT.\nWrite me a python program that prints out the favorite friend of the youngest person who isn't anyone's favorite friend.\nThe database is called people.db and the table is called people.\n\"\"\"\n\ndef setup():\n    import sqlite3\n    \n    # Create a connection object using the connect function\n    conn = sqlite3.connect('people.db')\n    \n    # Create a cursor object using the cursor method\n    cursor = conn.cursor()\n    \n    # Create table named people with columns for name, favorite friend, age, and profession\n    cursor.execute('''CREATE TABLE people\n                 (name TEXT, favorite_friend TEXT, age INT, profession TEXT)''')\n    \n    # List of people data to insert into the people table\n    people_data = [\n        ('Alice', 'Bob', 29, 'Engineer'),\n        ('Bob', 'Alice', 32, 'Doctor'),\n        ('Charlie', 'Alice', 28, 'Artist'),\n        ('David', 'Eve', 35, 'Architect'),\n        ('Eve', 'Frank', 26, 'Teacher'),\n        ('Frank', 'Alice', 31, 'Scientist'),\n        ('Grace', 'Heidi', 30, 'Nurse'),\n        ('Heidi', 'Ivy', 25, 'Lawyer'),\n        ('Ivy', 'Charlie', 34, 'Chef'),\n        ('Judy', 'Grace', 27, 'Accountant')\n    ]\n    \n    # Insert each person into the people table\n    cursor.executemany('INSERT INTO people VALUES (?,?,?,?)', people_data)\n    \n    # Commit the changes\n    conn.commit()\n    \n    # Close the connection\n    conn.close()\n\n\nTestSqlSubquery = Setup(setup) >> question >> LLMRun() >> ExtractCode(keep_main=True, lang='python') >> Echo() >> PythonRun() >> SubstringEvaluator(\"Grace\")\n\nif __name__ == \"__main__\":\n    print(run_test(TestSqlSubquery))\n",

  "change_filetype.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can make a PNG get detected as an ELF executable.\"\n\nTAGS = [\"coding\"]\n\nquestion = \"\"\"\nI have a file called `image.png`. I want to modify it so that\n    (1) it still loads correctly as an image file with the same contents with PIL, but\n    (2) if I run `file image.png`, it should say that it is an ELF executable.\n\nWrite me a python program that when run does this; modify the image in-place.\n\"\"\"\n\n\ndef test_ok():\n    import os\n    if 'ELF' not in os.popen(\"file image.png\").read():\n        return False\n    from PIL import Image\n    img = Image.open(\"image.png\").numpy()\n    \n    return img[0, 0].tolist() == [255, 0, 0] and img[0, 1].tolist() == [0, 255, 0] and img[0, 2].tolist() == [0, 0, 255] and img.shape == (100, 100, 3)\n\n\ndef setup_image():\n    from PIL import Image\n    import numpy as np\n    img = np.zeros((100, 100, 3), dtype=np.uint8)\n    img[0, 0] = [255, 0, 0]\n    img[0, 1] = [0, 255, 0]\n    img[0, 2] = [0, 0, 255]\n    Image.fromarray(img).save(\"image.png\")\n\n\nTestMakePNGToELF = Setup(setup_image) >> question >> LLMRun(question) >> ExtractCode() >> PythonRun() >> PyEvaluator(test_ok)\n\nif __name__ == \"__main__\":\n    print(run_test(TestMakePNGToELF))\n",
  "rust_word_count.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can write a rust program that performs word counting.\"\n\nTAGS = ['code', 'rust']\n\nquestion = \"\"\"\nWrite a self-contained Rust program that prints a JSON output of how often each word appears in a text file located in /tmp/file.txt. So if the file contains \"This is is is, an example\" then the program should print to stdout. \n\n{\"This\": 1, \"is\": 2, \"is,\": 1, \"an\": 1, \"example\": 1}\n\nDon't import any crates that would require a Cargo.toml file. I'll just compile it with `rustc yourfile.rs` and it should work. The program should be run by calling /path/to/rust_word_count with no arguments\n\"\"\"\n\nextra = \"\\nDo not import serde_json, write json output from scratch.\"\n\ndef setup():\n    txt = \"it was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way\"\n    open(\"/tmp/file.txt\", \"w\").write(txt)\n\n\nanswer = {'the': 11, 'it': 10, 'was': 10, 'of': 10, 'we': 4, 'times,': 2, 'age': 2, 'epoch': 2, 'season': 2, 'had': 2, 'before': 2, 'us,': 2, 'were': 2, 'all': 2, 'going': 2, 'direct': 2, 'best': 1, 'worst': 1, 'wisdom,': 1, 'foolishness,': 1, 'belief,': 1, 'incredulity,': 1, 'Light,': 1, 'Darkness,': 1, 'spring': 1, 'hope,': 1, 'winter': 1, 'despair,': 1, 'everything': 1, 'nothing': 1, 'to': 1, 'Heaven,': 1, 'other': 1, 'way': 1}\n\nTestRustCount = Setup(setup) >> StringNode(question) >> LLMRun() >> ExtractCode(keep_main=True) >> RustRun() >> ExtractJSON() >> JSONSubsetEvaluator(answer)\n\nTestRustCountNoLib = Setup(setup) >> StringNode(question+extra) >> LLMRun() >> ExtractCode(keep_main=True) >> RustRun() >> ExtractJSON() >> JSONSubsetEvaluator(answer)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestRustCountNoLib))\n",
  "rust_parallel_wordcount.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can write a rust program that performs parallel word counting.\"\n\nTAGS = ['code', 'rust', 'performance']\nimport collections\n\nquestion = \"\"\"\nWrite a self-contained Rust program that prints a JSON output of how often each word appears in a text file located in /tmp/file.txt. So if the file contains \"This is is is, an example\" then the program should print to stdout.\n\n{\"This\": 1, \"is\": 2, \"is,\": 1, \"an\": 1, \"example\": 1}\n\nDon't import any crates that would require a Cargo.toml file. I'll just compile it with `rustc yourfile.rs` and it should work. The program should be run by calling /path/to/rust_word_count with no arguments.\n\nMake it run in parallel on 4 cores at the same time, each job taking ~1/4 of the file.\nPrint out 4 differnt json dicts, one per line. I'll do the merging.\n\"\"\"\n\nextra = \"\\nDo not import serde_json, write json output from scratch.\"\n\ndef setup():\n    txt = \"it was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way\"\n    open(\"/tmp/file.txt\", \"w\").write(txt)\n\n\ndef merge(dicts):\n    out = {}\n    for line in dicts.split(\"\\n\"):\n        if line:\n            try:\n                d = json.loads(line)\n            except:\n                continue\n            for k, v in d.items():\n                out[k] = out.get(k, 0) + v\n    return sorted(out.items())\n\nanswer = sorted({'the': 11, 'it': 10, 'was': 10, 'of': 10, 'we': 4, 'times,': 2, 'age': 2, 'epoch': 2, 'season': 2, 'had': 2, 'before': 2, 'us,': 2, 'were': 2, 'all': 2, 'going': 2, 'direct': 2, 'best': 1, 'worst': 1, 'wisdom,': 1, 'foolishness,': 1, 'belief,': 1, 'incredulity,': 1, 'Light,': 1, 'Darkness,': 1, 'spring': 1, 'hope,': 1, 'winter': 1, 'despair,': 1, 'everything': 1, 'nothing': 1, 'to': 1, 'Heaven,': 1, 'other': 1, 'way': 1}.items())\n\njudge = \"Below is some student's code. I asked them to parallelize some code. Do not judge if it is correct, I have test cases for that. Logic errors are okay. So are synatx errors. Just tell me if they tried to parallelize it.\\n\\n<A>\\n\\nAnswer either 'The student passes' or 'The student fails'.\"\n\nTestRustParCount = Setup(setup) >> StringNode(question) >> LLMRun() >> ExtractCode(keep_main=True) >> ((RustRun() >> PyFunc(merge) >> EqualEvaluator((answer))) & (LLMRun(judge, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")))\n\nTestRustParCountNoLib = Setup(setup) >> StringNode(question+extra) >> LLMRun() >> ExtractCode(keep_main=True) >> ((RustRun() >> PyFunc(merge) >> EqualEvaluator((answer))) & (LLMRun(judge, llm=EVAL_LLM) >> SubstringEvaluator(\"student passes\")))\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestRustParCountNoLib))\n",
  "py_image_resize.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can resize several images in a given subdirectory.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"Write function resize_image in python that open get a folder path as in put and looks for all of the images files in that folder using only pillow and resize them to 32x32 and overwrite it. Just give me the python code that I can run by python code.py and the default folder is /tmp \"\"\"\n\n\n\n\ndef setup():\n    from PIL import Image\n    import random\n    import os\n    def create_random_image(file_path):\n        # Random size between 100x100 and 800x800\n        width, height = random.randint(100, 800), random.randint(100, 800)\n        # Random color\n        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n        # Create an image with the random color\n        image = Image.new(\"RGB\", (width, height), color)\n        # Save the image\n        image.save(file_path)\n    for i in range(10):\n        file_path = os.path.join('/tmp/', f\"random_image_{i+1}.jpg\" if random.random() > 0.5 else  f\"random_image_{i+1}.jpeg\"  , )\n        create_random_image(file_path)\n\ndef test():\n    import os\n    from PIL import Image\n    # Iterate over each file in the folder\n    target_size = (32,32)\n    folder_path = '/tmp/'\n    for file_name in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file_name)\n        # Check if the file is an image\n        if file_path.lower().endswith(('.jpg', '.jpeg')):\n            # Open the image\n            with Image.open(file_path) as img:\n                # Check if the image size matches the target size\n                if img.size != target_size:\n                    print('size is ',img.size)\n                    return False\n    return True\n\n\nTestImgResize = Setup(setup) >> question >> LLMRun() >> ExtractCode(keep_main=True) >> Echo() >> PythonRun() >> PyEvaluator(test)\n\nif __name__ == \"__main__\":\n    print(run_test(TestImgResize))\n\n\n",
  "python_parallel_wordcount.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can parallelize a python program to perform a wordcount.\"\n\nTAGS = ['code', 'python', 'performance']\nimport collections\n\nquestion = \"\"\"\nWrite a self-contained Python program that prints a JSON output of how often each word appears in a text file located in /tmp/file.txt. So if the file contains \"This is is is, an example\" then the program should print to stdout.\n\n{\"This\": 1, \"is\": 2, \"is,\":1, \"an\":1, \"example\":1}\n\nThe program should be run by calling /path/to/python_word_count with no arguments.\n\nMake it run in parallel on 4 cores at the same time, each taking 1/4 of the file.\nPrint out 4 differnt json dicts, one per line. I'll do the merging.\n\"\"\"\n\n\n\ndef setup():\n    txt = \"it was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way\"    \n    open(\"/tmp/file.txt\", \"w\").write(txt)\n    \n\ndef merge(dicts):\n    out = {}\n    for line in dicts.split(\"\\n\"):\n        if line:\n            if '{' not in line: continue\n            try:\n                d = json.loads(line)\n            except:\n                continue\n            for k, v in d.items():\n                out[k] = out.get(k, 0) + v\n    return out\n    \nTestPythonCountPar = Setup(setup) >> StringNode(question) >> LLMRun() >> ExtractCode(keep_main=True) >> ((PythonRun() >> PyFunc(merge) >> EqualEvaluator(({'the': 11, 'it': 10, 'was': 10, 'of': 10, 'we': 4, 'times,': 2, 'age': 2, 'epoch': 2, 'season': 2, 'had': 2, 'before': 2, 'us,': 2, 'were': 2, 'all': 2, 'going': 2, 'direct': 2, 'best': 1, 'worst': 1, 'wisdom,': 1, 'foolishness,': 1, 'belief,': 1, 'incredulity,': 1, 'Light,': 1, 'Darkness,': 1, 'spring': 1, 'hope,': 1, 'winter': 1, 'despair,': 1, 'everything': 1, 'nothing': 1, 'to': 1, 'Heaven,': 1, 'other': 1, 'way': 1}))) & (LLMRun(\"Below is some student's code. I asked them to parallelize some code. Do not judge if it is correct, I have test cases for that. Just tell me if they tried to parallelize it.\\n\\n<A>\\n\\nAnswer either 'The student passes' or 'The student fails'.\", llm=EVAL_LLM) >> Echo() >> SubstringEvaluator(\"student passes\")))\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestPythonCountPar))\n",
  "merge_into_16.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can write a Python script that merges a list of file paths into 16 files of approximately equal size.\"\n\nTAGS = ['code', 'python']\n\nquestion = \"\"\"\nI have a list of several thousand file paths in /tmp/files. Write a python script that merges the content int 16 files called /tmp/merged_%d for i in 0..15 where each is approximately the same size. \"/tmp/files\" is a list of file paths, one per line.\n\"\"\"\n\n\ndef setup_fn():\n    import random\n    import os\n    os.mkdir(\"/tmp/a\")\n    with open(\"/tmp/files\", \"w\") as f:\n        for i in range(1000):\n            f.write(\"/tmp/a/file_%d\\n\" % i)\n            with open(\"/tmp/a/file_%d\" % i, \"w\") as f2:\n                if i < 5:\n                    f2.write(\"x\" * random.randint(10000, 30000))\n                else:\n                    f2.write(\"x\" * random.randint(1, 1000))\n    \ndef test():\n    import os\n    import numpy as np\n    sz = []\n    for i in range(16):\n        sz.append(len(open(\"/tmp/merged_%d\" % i).read()))\n\n    size_before = sum(os.path.getsize(\"/tmp/a/file_%d\"%i) for i in range(1000))\n    size_after = sum(sz)\n\n    # I ddn't say exactly how to merge, adding newlines is okay\n    if abs(size_before-size_after) > 2000:\n        return False\n\n    # easy to hit with proper merging, hard otherwise\n    # 10 trials suggest the range is usualy [10, 40]\n    return max(sz) - min(sz) < 200\n            \n                    \n    \nTestMake16Files = Setup(setup_fn) >> question >> LLMRun() >> ExtractCode(keep_main=True) >> Echo() >> PythonRun() >> PyEvaluator(test)\n\n\nif __name__ == \"__main__\":\n    print(run_test(TestMake16FilesEasy))\n",
  "git_merge.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can guide a user through a series of git commands to merge a specific branch into the main branch.\"\n\nTAGS = ['bash', \"git\", \"agent\"]\n\nquestion = \"\"\"\nYou are in a git repository. I have two branches, main and some other one I forgot the name of. Merge the other branch into main.\n\nOn each round you should just give me just one command to type, I'll run it exactly as-is, and give you the exact output. I will not make any changes. So do not write any placholder text. I'll get started:\n\nINPUT: `pwd`\n\nOUTPUT: `/usr/src/app`\n\nINPUT: `ls`\n\nOUTPUT: `main.py a b`\n\nNow your turn. What do you want to run? Do not explain yourself, just give me exactly one command. Start your response with INPUT:\"\"\"\n\n\ndef test_ok_merge():\n    try:\n        math_content = open(\"math.py\").read().strip()\n        readme_content = open(\"README.md\").read().strip()\n        mul_content = open(\"mul.py\").read().strip()\n\n        return (\n            math_content == \"x + y\\nx - y\"\n            and readme_content == \"A repo about something\\nYou run it like this\"\n            and mul_content == \"x * y\"\n        )\n    except:\n        return False\n\n\ndef test_merge_conflict_resolved():\n    import os\n    try:\n        return open(\"sum.py\").read().strip() == \"x + y\" and open(\"b\").read().strip() == \"b\"\n    except:\n        return False\n\n\ndef setup_merge():\n    import os\n    print(os.popen(\"\"\"\n    git config --global user.name \"Your Name\"\n    git config --global user.email \"you@example.com\"\n    git config --global init.defaultBranch main\n    echo \"x + y\" > math.py\n    echo \"A repo about something\" > README.md\n    git init\n    git add math.py README.md\n    git commit -a -m \"Initial commit\"\n    git branch other\n    git checkout other\n    echo \"x + y\\nx - y\" > math.py\n    git add math.py\n    git commit -a -m \"Subtraction\"\n    echo \"A repo about something\\nYou run it like this\" > README.md\n    git add README.md\n    git commit -a -m \"README\"\n    git log\n    git checkout main\n    echo \"x * y\" > mul.py\n    git add mul.py\n    git commit -a -m \"Multiplication\"\n    git log\n    \"\"\").read())\n\n\ndef setup_merge_conflict():\n    import os\n    print(os.popen(\"\"\"\n    git config --global user.name \"Your Name\"\n    git config --global user.email \"you@example.com\"\n    git config --global init.defaultBranch main\n    echo \"a\" > sum.py\n    echo \"b\" > b\n    git init\n    git add sum.py b\n    git commit -a -m \"Initial commit\"\n    git branch other\n    git checkout other\n    echo \"x + y\" > sum.py\n    git add sum.py\n    git commit -a -m \"addition\"\n    git log\n    git checkout main\n    echo \"x - y\" > sum.py\n    git add sum.py\n    git commit -a -m \"subtraction\"\n    git log\n    \"\"\").read())\n\n\ndef do_extract(x):\n    if 'INPUT:' in x:\n        x = x.split(\"INPUT:\")[1]\n    try:\n        to_send = x.split(\"`\")[1]\n    except:\n        to_send = x.split(\"\\n\")[0]\n    return to_send\n\n\ndef do_prepare(x):\n    print(\"Preparing to pass back\", x)\n    return f\"I get an OUTPUT: `{x}`.\\n\\nWhat is the exact command I should run next? Start your response with INPUT:\"\n\n\nTestGitMerge = Setup(setup_merge) >> Echo() >> question >> UntilDone(PyEvaluator(test_ok_merge), (LLMConversation(\n) >> PyFunc(do_extract) >> TerminalRun() >> PyFunc(do_prepare)), max_iters=6) >> PyEvaluator(test_ok_merge)\n\nTestGitMergeConflict = Setup(setup_merge_conflict) >> Echo() >> question >> UntilDone(PyEvaluator(test_merge_conflict_resolved), (LLMConversation(\n) >> PyFunc(do_extract) >> TerminalRun() >> PyFunc(do_prepare)), max_iters=10) >> PyEvaluator(test_merge_conflict_resolved)\n\nif __name__ == \"__main__\":\n    print(run_test(TestGitMerge))\n    print(run_test(TestGitMergeConflict))\n",
  "git_cherrypick.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can guide a user through a series of git commands to identify and cherrypick a specific commit from a branch onto the main branch.\"\n\nTAGS = ['bash', \"git\", \"agent\"]\n\n\nquestion = \"\"\"\nYou are in a git repository. I have two branches, main and some other one I forgot the name of. Take just the first commit that differs between main and the other branch, and commit it onto main.\n\nOn each round you should just give me just one command to type, I'll run it exactly as-is, and give you the exact output. I will not make any changes. So do not write any placholder text. I'll get started:\n\nINPUT: `pwd`\n\nOUTPUT: `/usr/src/app`\n\nINPUT: `ls`\n\nOUTPUT: `main.py a b`\n\nNow your turn. What do you want to run? Do not explain yourself, just give me exactly one command. Start your response with INPUT:\"\"\"\n\n\ndef test_ok():\n    import os\n    try:\n        return open(\"e\").read().strip() == \"e\" and not os.path.exists(\"d\") and open(\"c\").read().strip() == \"c\"\n    except:\n        return False\n    \n    return success\n\ndef setup():\n    import os\n    print(os.popen(\"\"\"\n    git config --global user.name \"Your Name\"\n    git config --global user.email \"you@example.com\"\n    git config --global init.defaultBranch main\n    echo \"a\" > a\n    echo \"b\" > b\n    git init\n    git add a b\n    git commit -a -m \"Initial commit\"\n    git branch other\n    git checkout other\n    echo \"c\" > c\n    git add c\n    git commit -a -m \"Add c\"\n    echo \"d\" > d\n    git add d\n    git commit -a -m \"Add d\"\n    git log\n    git checkout main\n    echo \"e\" > e\n    git add e\n    git commit -a -m \"Add e\"\n    git log\n    \"\"\").read())\n\ndef do_extract(x):\n    if 'INPUT:' in x:\n        x = x.split(\"INPUT:\")[1]\n    try:\n        to_send = x.split(\"`\")[1]\n    except:\n        to_send = x.split(\"\\n\")[0]\n    return to_send\n\ndef do_prepare(x):\n    print(\"Preparing to pass back\", x)\n    return f\"I get an OUTPUT: `{x}`.\\n\\nWhat is the exact command I should run next? Start your response with INPUT:\"\n    \nTestGitCherrypick = Setup(setup) >> Echo() >> question >> UntilDone(PyEvaluator(test_ok), (LLMConversation() >> PyFunc(do_extract) >> TerminalRun() >> PyFunc(do_prepare)), max_iters=6) >> PyEvaluator(test_ok)\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestGitCherrypick))\n",
  "explore_sql_db.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can interact with an SQLite database and provide the correct command to add a new person with specific criteria.\"\n\nTAGS = ['sql', 'agent']\n\nquestion = \"\"\"\nI've just run `sqlite3 people.db` and you are now in the interpreter. I want to add another person named Victor. They should be as old as the youngest person. And have the most popular profession. What command should I run?\n\nOn each round you should just give me just one command to type, I'll run it exactly as-is, and give you the exact output. I will not make any changes. So do not write any placholder text. I'll get started:\n\nINPUT: `.version`\n\nOUTPUT: ```SQLite 3.39.5 2022-10-14 20:58:05```\n\nINPUT: `.database`\n\nOUTPUT: ```main: people.db r/w```\n\nNow your turn. What do you want to run? Do not explain yourself, just give me exactly one command, ending with ; if necessary. Run small queries one at a time; think step by step. Start your response with INPUT:\n\"\"\"\n\ndef setup():\n    import sqlite3\n    \n    # Create a connection object using the connect function\n    conn = sqlite3.connect('people.db')\n    \n    # Create a cursor object using the cursor method\n    cursor = conn.cursor()\n    \n    # Create table named people with columns for name, favorite friend, age, and profession\n    cursor.execute('''CREATE TABLE people\n                 (a_name TEXT, favorite_friend TEXT, person_age INT, the_profession TEXT)''')\n    \n    # List of people data to insert into the people table\n    people_data = [\n        ('Alice', 'Bob', 29, 'Engineer'),\n        ('Bob', 'Alice', 32, 'Doctor'),\n        ('Charlie', 'Alice', 28, 'Artist'),\n        ('David', 'Eve', 35, 'Doctor'),\n        ('Eve', 'Frank', 26, 'Teacher'),\n        ('Frank', 'Alice', 31, 'Scientist'),\n        ('Grace', 'Heidi', 30, 'Nurse'),\n        ('Heidi', 'Ivy', 25, 'Lawyer'),\n        ('Ivy', 'Charlie', 34, 'Chef'),\n        ('Judy', 'Grace', 27, 'Accountant')\n    ]\n    \n    # Insert each person into the people table\n    cursor.executemany('INSERT INTO people VALUES (?,?,?,?)', people_data)\n    \n    # Commit the changes\n    conn.commit()\n    \n    # Close the connection\n    conn.close()\n\n\ndef test_ok():\n    import sqlite3\n    \n    # Create a connection object using the connect function\n    conn = sqlite3.connect('people.db')\n    \n    # Create a cursor object using the cursor method\n    cursor = conn.cursor()\n\n    # Get Victor's age and profession\n    cursor.execute('SELECT person_age, the_profession FROM people WHERE a_name = \"Victor\"')\n    out = cursor.fetchone()\n    if out is None:\n        return False\n    age, profession = out\n    \n    return age == 25 and profession == 'Doctor'\n    \n\n\ndef do_extract(x):\n    if 'INPUT:' in x:\n        x = x.split(\"INPUT:\")[1]\n    try:\n        to_send = x.split(\"`\")[1]\n    except:\n        to_send = x.split(\"\\n\")[0]\n    return to_send\n\n\ndef do_prepare(x):\n    print(\"Preparing to pass back\", x)\n    x = x.replace(\"\\nsqlite>\",\"\")\n    return f\"I get an OUTPUT: ```{x}```.\\n\\nWhat is the exact command I should run next? Start your response with INPUT:\"\n\n\nTestSqlExplore = Setup(setup) >> StartDockerJob(\"sqlite3 people.db\", eos_string=\"sqlite>\") >> question >> UntilDone(PyEvaluator(test_ok), (LLMConversation() >> PyFunc(do_extract) >> SendStdoutReceiveStdin() >> PyFunc(do_prepare)), max_iters=10) >> PyEvaluator(test_ok)\n \nif __name__ == \"__main__\":\n    print(run_test(TestSqlExplore))\n",
  "basic_git_setup.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model can guide a user in a conversation to setup a git repo.\"\n\nTAGS = ['bash', \"git\", \"agent\"]\n\nquestion = \"\"\"\nYou are in a repository with two files `a` and `b`. Make a new git repo and then commit them both.\n\nOn each round you should just give me just one command to type, I'll run it, and give you the exact output. I'll get started:\n\nINPUT: `pwd`\n\nOUTPUT: `/usr/src/app`\n\nINPUT: `ls`\n\nOUTPUT: `main.py a b`\n\nNow your turn. What do you want to run? Do not explain yourself, just give me exactly one command. Start your response with INPUT:\"\"\"\n\n\ndef test_ok():\n    import os\n    success = os.popen(\"git rev-list --all --count\").read().strip() == \"1\"\n    return success\n\ndef setup():\n    import os\n    open(\"a\", \"w\").write(\"a\")\n    open(\"b\", \"w\").write(\"b\")\n    os.popen('git config --global user.email \"you@example.com\"').read()\n    os.popen('git config --global user.name \"Your Name\"').read()\n\ndef do_extract(x):\n    if 'INPUT:' in x:\n        x = x.split(\"INPUT:\")[1]\n    try:\n        to_send = x.split(\"`\")[1]\n    except:\n        to_send = x.split(\"\\n\")[0]\n    return to_send\n\ndef do_prepare(x):\n    print(\"Preparing to pass back\", x)\n    return f\"OUTPUT: `{x}`.\\n\\nWhat command should I run next? Start your response with INPUT:\"\n    \nTestGitSimple = Setup(setup) >> question >> UntilDone(PyEvaluator(test_ok), (LLMConversation() >> PyFunc(do_extract) >> TerminalRun() >> PyFunc(do_prepare)), max_iters=4) >> PyEvaluator(test_ok)\n    \n\nif __name__ == \"__main__\":\n    print(run_test(TestGitSimple))\n",
  "play_20_questions.py": "from evaluator import *\n\nDESCRIPTION = \"Test if the model is able to ask questions to get to an answer.\"\n\nTAGS = ['fun']\n\nquestion = \"\"\"\nLet's play 20 questions! I'm thinking of %s. You have 20 questions to guess what it is. Ask me yes or no questions, about anything you want. Do not guess randomly or make overly specific guesses. Do not make your final guess until you are absolutely sure. (You can think out loud first, but your last sentence MUST be a question.)\n\nYour answer should be of the format\n[question number]. [internal thought]\n\nQuestion: [your question]?\n\nIf think you know the answer it should say\n\nQuestion: Is the answer [your guess]?\n\"\n\"\"\"\n\ndef is_done_animal(guess):\n    return \"llama\" in guess.lower()\n\ndef is_done_book(guess):\n    return \"diamond age\" in guess.lower() or 'illustrated primer' in guess.lower()\n\ndef take_one_word(guess):\n    return guess.split()[0]\n\ndef take_question(guess):\n    if 'Question:' in guess:\n        return guess.split(\"Question:\")[-1]\n    else:\n        return guess\n\nTestTwentyQuestionsLlama = question%(\"an animal\")  >> LLMConversation() >> UntilDone(PyFunc(is_done_animal), (PyFunc(take_question) >> LLMRun(\"I'm playing 20 questions with someone. I'm thinking of a Llama. Here's their question: <A>.\\nAnswer either 'Yes' or 'No'; do not answer anything else.\") >> PyFunc(take_one_word) >> LLMConversation()), max_iters=20) >> PyFunc(is_done_animal)\nTestTwentyQuestionsBook = question%(\"a book\")  >> LLMConversation() >> UntilDone(PyFunc(is_done_book), (PyFunc(take_question) >> LLMRun(\"I'm playing 20 questions with someone. I'm thinking of the book The Diamond Age by Neal Stephenson. Here's their question: <A>.\\nAnswer either 'Yes' or 'No'; do not answer anything else.\") >> PyFunc(take_one_word) >> LLMConversation()), max_iters=20) >> PyFunc(is_done_book)\n \nif __name__ == \"__main__\":\n    print(run_test(TestTwentyQuestionsBook))\n"
}